{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a9fdb3",
   "metadata": {},
   "source": [
    "# Stochastic Models in Neurocognition\n",
    "\n",
    "## Class 1\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Preliminary Notes**:\n",
    "\n",
    "<u>Class framework:</u>\n",
    "- First 3 classes on independent models\n",
    "- 2 classes on markov chains and poisson processes\n",
    "- 2 courses on point processses and their statistics\n",
    "- 3 classes on PDMP, Brownian Motions, mean-field.\n",
    "\n",
    "<u>On tutorials:</u>\n",
    "- Answer in .pdf and in .R to do analysis to be posted online (on Moodle)\n",
    "- The grading will be about progression and how good we are about correction of other students\n",
    "\n",
    "<u>Final exam:</u>\n",
    "- Done in February on-site (programmation + write-down)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e71d9",
   "metadata": {},
   "source": [
    "# 1 - SOME EXAMPLES OF MODELS IN NEUROCOGNITION\n",
    "\n",
    "## 1.1 - Models with independence\n",
    "\n",
    "### Examples of models for neurocognition with independence\n",
    "\n",
    "<u>A. **Justifying independence matters**</u>\n",
    "\n",
    "**Independence matters** as it helps do a lot of different statistical analysis:\n",
    "- Independence enables easier computation\n",
    "- Indentifying where independence lies is important in terms of modeling\n",
    "\n",
    "<u>Case 1:</u> **Individuals**, e.g. as part of a cognitive experiment with different participants. <span style=\"color:red\">**/!\\** Participants interacting removes independence</span>.\n",
    "\n",
    "<u>Case 2:</u> **Trials**, e.g. when the participants are asked to repeat an experiment. <span style=\"color:red\">**/!\\** The state of the participants matter: tiredness, location, etc.</span>.\n",
    "\n",
    "<u>B. **Parametric vs Non-parametric modelss**</u>\n",
    "\n",
    "| **Parametric** | **Non-Parametric** | \n",
    "| ---: | ---: |\n",
    "| The distribution of the data is parametrized by a finite set of parameters. e.g. $\\mathcal{N}(\\mu,\\sigma^2)$ with $\\theta = (\\mu, \\sigma^2)\\in\\mathbb{R}^2$ | The distribution depends on more than a finite number of parameters. e.g. a cumulative distribution function $f$ characterizing ***iid*** $X_1, X_2, ..., X_3$ |\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b68a21",
   "metadata": {},
   "source": [
    "## 1.2 - Interspikes intervals\n",
    "\n",
    "The main model to be looked into during the class is called an **Interspikes intervals**.\n",
    "\n",
    "### Model of a neuron\n",
    "\n",
    "A neuron is composed of an **axon**, a **soma**, and **dendrites** connected to another axon via a **synapse**. The **voltage** of a neuron is a continuous time series characterized by fluctuations and an activation/spike called an <span style=\"color:red\">**action potential**</span>.\n",
    "\n",
    "> **For a given neuron, the pattern/shape of the spike/action potential is always consistent/the same**.\n",
    "\n",
    "The spike is powerful enough to travel to the synapse (from the **pre-synaptic neuron**), which will consequently affect the voltage of the neuron down the line (pre-synaptic neuron -> axon -> synapse -> dendrite -> neuron). \n",
    "\n",
    "A pre-synaptic neuron is either:\n",
    "- **excitatory**: its spike induces a higher voltage in the neuron N down-the-line, and the higher the voltage, the more likely is N to spike\n",
    "- **inhibitory**: its spike induces a lower voltage in the neuron N down-the-line, the less likely is N to spike\n",
    "\n",
    "### Problem of such a model\n",
    "\n",
    "<u>A. **Stating the problem**</u>\n",
    "\n",
    "> One can rarely access such a data at a large scale (a human brain has c. $10^{12}$ neurons and setting electrodes in a brain is tricky). \n",
    "\n",
    "Instead, **tetrodes** are used. Tetrodes are deep electrodes that can record activity at different areas of the brain. Signals captured by tetrodes differ from electrodes as a tetrodes collects **unsorted spikes** which give no information on the **location of the neurons**.\n",
    "\n",
    "![tetrodes](images/tetrodes.png)\n",
    "\n",
    "**Tetrodes** still allow to find **spike trains** where we know when a neuron has emitted spikes as a neuron always emits the same spikes.\n",
    "\n",
    "<hr>\n",
    "\n",
    "> **An interspike interval is the distance between the spikes of a neuron on a spike train**\n",
    ">\n",
    "> Short-handed as ***ISI***\n",
    ">\n",
    "> <span style=\"color:red\">We can model the ISIs of a given neuron as an IID variable. </span>\n",
    "\n",
    "<u>(Leaky) Integrate-and-Fire:</u> Model that characterize a particular distribution for the ISI without challenging the fact that it is IID\n",
    "\n",
    "<hr>\n",
    "\n",
    "<u>B. **Why are ISI IID?**</u>\n",
    "\n",
    "**Independence:**\n",
    "\n",
    ">  ISI are considered IID as spikes are intersperced by reset points. This is not a mathematical justification but a modeling one: that neurons 'reset' after a spike.\n",
    ">\n",
    "> \"At least the voltagte is reset to the same value/voltage at each spike\" which would **legitimate** independence\n",
    "\n",
    "Note: This is not the case of burst phenomena\n",
    "\n",
    "**Identical distribution:**\n",
    "\n",
    "> As long as the behavior of the animal does not change too much and is recorded during small periods of time\n",
    ">\n",
    "> **Changes in behavior**, **memory effects** (capacitance, STDP) and **iteractions between neurons are not modeled**\n",
    "\n",
    "### Parametric Assumption\n",
    "\n",
    "$Xs$ follow an **exponential distribution**, related to Poisson processes.\n",
    "\n",
    "\\begin{align}\n",
    "X_1, X_2, ..., X_n &\\sim \\text{IID}\\,\\,\\mathcal{E}(\\lambda),\\,\\,\\lambda \\text{ is unknown}\n",
    "\\end{align}\n",
    "\n",
    "<span style=\"color:red\">**Limit of assumption**:</span> This model does not take into account **refractory period**. A refractory period represent the physical delay where a neuron cannot produce a spike anymore right after a spike (due to the low voltage). This cannot be seen with exponential.\n",
    "\n",
    "<u>Solution:</u> **Shifted exponential**, meaning the density is given by \n",
    "\n",
    "\\begin{align}\n",
    "f(x)&=\\lambda.e^{-(x-\\theta)\\lambda}\\mathbb{1}_{x>\\theta}\\\\\n",
    "\\lambda &\\text{ is expressed in } Hz\n",
    "\\end{align}\n",
    "\n",
    "![shiftedexp](images/shiftedexp.png)\n",
    "\n",
    "<u>Note:</u> When $\\lambda$ is small (1 to 3 Hz), since the refractory period is c. 2ms to 5ms, we will not see this effect and the first model is goo0d.\n",
    "\n",
    "### Non-Parametric Assumption\n",
    "\n",
    "\\begin{align}\n",
    "X_1, X_2, ..., X_n &\\sim \\text{IID}\\,\\,{ density }\\, f\\,\\text{, with $f$ unknown} \n",
    "\\end{align}\n",
    "\n",
    "Density in $\\mathbb{R}$ provides an estimator $\\hat{f}$ of $f$.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37f21c",
   "metadata": {},
   "source": [
    "## 1.3 - Neural Rate Coding\n",
    "\n",
    "### Firing Rate\n",
    "\n",
    "**Definition**: The firing rate of a neuron is in average the number of spikes produced per seconds.\n",
    "\n",
    "<u>Adrian and Zottermann's experiment, 1926:</u> \n",
    "\n",
    "They found out that the firing rate of a sensory nerve of a muscle increases with the weight attacked to it. \n",
    "\n",
    "<u>Georgopoulos, Schwarz and Kettner's experiment, 1986:</u> \n",
    "\n",
    "Beyond weights, neurons have a preferred direction (angle of the rotation of the underlying organ, member). Neurons encodes the strength of the underlying phenomenon, but also the mode of such phenomenon.\n",
    "\n",
    "### Modeling the Firing Rate\n",
    "\n",
    "<u>Modeling:</u>\n",
    "\n",
    "We set:\n",
    "\n",
    "\\begin{align}\n",
    "Y&\\text{ is firing rate}\\\\\n",
    "Y&=\\begin{cases}\n",
    "      a+b.W+\\sigma\\epsilon & \\text{a, b, $\\sigma$ unknown, and $\\epsilon\\sim\\mathcal{N}(0,1)$}\\\\\n",
    "      a + b.cos\\theta+\\sigma\\epsilon & \\text{For the angle of the movement}\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "As such the **parametric model** for $Y$, the firing rate, can be stated as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "Y&=\\begin{cases}\n",
    "      f(W)+\\sigma\\epsilon\\quad\\text{  with f unknown}\\\\\n",
    "      f(\\theta)+\\sigma\\epsilon\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "$\\theta$ is the angle between the movement and the preferred direciton of the movement.\n",
    "\n",
    "<u>Measuring variations:</u>\n",
    "\n",
    "- For one cell, one tries several $W$ or $\\theta$. The observations are independent.\n",
    "- For different cells, one 'hope' that this is still independent\n",
    "\n",
    "<u>Is the noise Gaussian?</u>\n",
    "\n",
    "$$Y = firing\\,\\,rate = \\frac{\\text{number of spikes}}{\\text{duration of the experiment}}$$\n",
    "\n",
    "- The number of spikes is usually modeled by Binomial or Poisson\n",
    "- In both cases when the duration of the experiment is long enough, these distribution might be approximated by a Gaussian\n",
    "\n",
    "Sometimes, one need to transform the data to make them look Gaussian. The Anscombe transform $N\\rightarrow2\\sqrt{N+3/8}$ is known to be the best way to make a Poisson look Gaussian.\n",
    "\n",
    "<hr>\n",
    "\n",
    "***TO REMEMBER:***\n",
    "\n",
    "- **Estimation (Law of Large numbers)**\n",
    "- **Asymptotic confidence intervals (Central Limit Theorem)**\n",
    "- **Tests -> Parametric (ln()) or Non-Parametric (ks.test(), wilcox.test(), shapiro.trest(), x^2 test)**\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3a511",
   "metadata": {},
   "source": [
    "# 2 - LIKELIHOOD AND CONTRAST\n",
    "\n",
    "## 2.1 - Likelihood\n",
    "\n",
    "<u>Toy Example:</u>\n",
    "\n",
    "One observes $X\\sim\\mathcal{N}(\\mu,1)$ and two statistics $\\mu_1$ and $\\mu_2$ are available. One would tend to choose the statistics where the observed \\mu (sample mean $X$) is the closest (i.e. comparing $f_{m_1}(X)$ and $f_{m_2}(X)$).\n",
    "\n",
    "The maximum is achieved in $\\theta=m_1$. As such, the $\\hat{\\theta}(MLE)=m_1$.\n",
    "\n",
    "### Definition\n",
    "\n",
    "In general, one has a **parametric** family $f_\\theta$ parametrized by $\\theta\\in\\mathbb{R}^d$, with $f_theta$ a is either:\n",
    "\n",
    "\\begin{align}\n",
    "f_\\theta&=\\begin{cases}\n",
    "      \\text{ a density if the variable is continuous}\\\\\n",
    "      \\text{ the probability distribution function if the variable is discrete}\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "$$\\theta \\rightarrow f_\\theta(X)$$\n",
    "\n",
    "One observes $X\\sim f_\\theta$ for $\\theta$ unknown.\n",
    "\n",
    "### Maximum Likelihood Estimator\n",
    "\n",
    "An **estimator is a function of the data**, and the data is considered **fixed**. The estimator is a **random variable**.\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta\\in\\Theta}{argmax} f_\\theta(X)$$ i.e. the point $\\theta$ which maximizes $f_theta$ if several $\\theta$ are available. The \"best\" one is selected as it maximizes the likelihood of observing $X$.\n",
    "\n",
    "<u>Heuristics:</u> \n",
    "\n",
    "If one observes $X_1, ..., X_n$ ***IID*** with density $g_\\theta(x)$ then the density of $X=(X_1, ..., X_n)$ is:\n",
    "\n",
    "$$f_\\theta(X) = g_\\theta(X_1)\\,\\,\\times\\,\\,...\\,\\,\\times\\,\\,g_\\theta(X_n)$$\n",
    "\n",
    "<u>Notation:</u>\n",
    "\n",
    "- **Likelihood**: $\\mathcal{L}(\\theta) = f_\\theta(X)$ in this case (IID) it is: $\\overset{n}{\\underset{i=1}{\\prod}}g_\\theta(x_i)$\n",
    "- **Log-Likelihood**: $\\mathcal{l}(\\theta) = log(f_\\theta(X))$ in this case (IID) it is: $\\overset{n}{\\underset{i=1}{\\sum}}log(g_\\theta(x_i))$\n",
    "\n",
    "<u>Assumptions of the MLE:</u>\n",
    "\n",
    "With very few assumptions, the MLE is usually:\n",
    "\n",
    "- **Consistent** (convergence to $\\theta_0$\n",
    "- With the **smallest asyumptotic variance**\n",
    "\n",
    "However:\n",
    "\n",
    "- It is computable by hand in very cases\n",
    "- if $\\mathcal{L}(\\theta)$ is computable, its maximization might be tricky\n",
    "- there are cases where even computing $\\mathcal{L}(\\theta)$ is a challenge\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3bc3e",
   "metadata": {},
   "source": [
    "## 2.2 - Example with exponential ISI\n",
    "\n",
    "We have: $X_1, ..., X_n$ with density $\\lambda e^{-\\lambda x}\\mathbb{1}_{x\\ge0}$\n",
    "\n",
    "As such:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta) &= f_\\theta(X) = \\overset{n}{\\underset{i=1}{\\prod}}(\\lambda e^{-\\lambda x_i}\\mathbb{1}_{x_i\\ge0})\\\\\n",
    "&= \\lambda^n e^{-\\lambda \\underset{i=1}{\\overset{n}{\\sum}}X_i}\\mathbb{1}_{min(X_i)\\ge0}\\\\\n",
    "\\mathcal{l}(\\theta)&=n.log(\\lambda) - \\lambda\\underset{i=1}{\\overset{n}{\\sum}}X_i + log(\\mathbb{1}_{min(X_i)\\ge0})\\\\\n",
    "&=n.log(\\lambda) - \\lambda\\underset{i=1}{\\overset{n}{\\sum}}X_i \\quad\\quad(log(\\mathbb{1}_{min(X_i)\\ge0})\\,\\,\\text{is always 0})\n",
    "\\end{align}\n",
    "\n",
    "To find the maximum, we use the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}(\\theta)&=n.log(\\lambda) - \\lambda\\underset{i=1}{\\overset{n}{\\sum}}X_i\\\\\n",
    "\\mathcal{l}'(\\theta)&=\\frac{n}{\\lambda} - \\underset{i=1}{\\overset{n}{\\sum}}X_i \\\\\n",
    "...\\\\\n",
    "\\mathcal{l}'(\\theta)&=0 \\Leftrightarrow \\lambda=\\frac{n}{\\underset{i=1}{\\overset{n}{\\sum}}X_i}\\\\\n",
    "\\end{align}\n",
    "\n",
    "![mle](images/mle.png)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5286e46",
   "metadata": {},
   "source": [
    "## 2.3 - Gaussian Linear Models\n",
    "\n",
    "\\begin{align}\n",
    "Y_i=\\begin{cases}\n",
    "      a + b.W_i + \\sigma\\epsilon \\\\\n",
    "      a + b.cos(\\theta_i)+\\sigma\\epsilon\n",
    "    \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "In general for linear gaussian models (think also to ANOVA, etc.):\n",
    "\n",
    "> $Y = (T_1, ..., Y_n)^T = \\mu+\\sigma\\epsilon$ with $\\epsilon = (\\epsilon_1, ..., \\epsilon_n)^T$ with $\\epsilon_i\\,\\,IID\\,\\,\\sim\\mathcal{N(0,1)}$.\n",
    ">\n",
    "> $\\mu$ and $\\sigma$ are both unknown, e.g., $V=vect((1,...,1)^T, (W_1,...,W_n)^T)$ or $V=vect((1,...,1)^T, (cos(\\theta_1),...,cost(\\theta_n))^T)$\n",
    "\n",
    "### Estimation of $\\mu$ and $\\sigma$ by MLE\n",
    "\n",
    "<u>Likelihood:</u>\n",
    "\n",
    "$$\\Theta = (\\mu, \\sigma)$$\n",
    "$$dim(V)+1\\text{ parameters}$$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta) =f_\\theta(Y) &= \\underset{i=1}{\\overset{n}{\\prod}}\\frac{e^{-\\frac{(Y_i-\\mu_i)^2}{2\\sigma^2}}}{(\\sqrt{2\\pi\\sigma^2})^n}\\\\\n",
    "&= \\frac{e^{-\\underset{i=1}{\\overset{n}{\\sum}}\\frac{(Y_i-\\mu_i)^2}{2\\sigma^2}}}{(\\sqrt{2\\pi\\sigma^2})^n}\n",
    "\\end{align}\n",
    "\n",
    "**We note that $\\mu = (\\mu_1, ..., \\mu_n)^T\\in V \\subset \\mathbb{R}^n$. In the case of $V$, $\\mu_i = a+b.W_i$ so just two parameters.**\n",
    "\n",
    "<u>Log-Likelihood:</u>\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}(\\mu, \\sigma^2) &= -\\underset{i=1}{\\overset{n}{\\sum}}\\frac{(Y_i-\\mu_i)^2}{2\\sigma^2} - \\frac{n}{2}.log(2\\pi\\sigma^2)\\\\\n",
    "&= -\\frac{||(Y-\\mu)||^2}{2\\sigma^2} - n.log(\\sigma) - \\frac{n}{2}.log(2\\pi)\n",
    "\\end{align}\n",
    "\n",
    "#### FOR $\\mu$\n",
    "\n",
    "**Maximizing the log-likelihood corresponds to minimizing the norm $||Y-\\mu||^2,\\,\\,\\forall \\mu \\in V$.**\n",
    "\n",
    "$$\\hat{\\mu}=\\Pi_VY$$\n",
    "\n",
    "![proj](images/projection.png)\n",
    "\n",
    "#### FOR $\\sigma^2$\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{l}(\\Pi_V(Y), \\sigma^2) = -\\frac{||Y-\\Pi_VY||^2}{2\\sigma^2} - \\frac{n}{2}log(\\sigma^2)\\\\\n",
    "\\frac{\\delta\\mathcal{l}}{\\delta\\sigma^2} = \\frac{||Y-\\Pi_VY||^2}{2(\\sigma^2)^2} - \\frac{n}{2\\sigma^2}\n",
    "\\end{align}\n",
    "\n",
    "$\\frac{\\delta\\mathcal{l}}{\\delta\\sigma^2}$ is null  in $$\\hat{\\sigma^2}=\\frac{||Y-\\Pi_VY||^2}{n}$$\n",
    "\n",
    "<u>Remark:</u> \n",
    "$$||Y-\\Pi_VY||^2\\sim \\sigma^2\\mathcal{X}^2(n-dim(V))$$\n",
    "\n",
    "So $\\mathbb{E}(||Y-\\Pi_VY||^2) = \\sigma^2(n-dim(V))$ which means that:\n",
    "\n",
    "$$\\mathbb{E}(\\hat{\\sigma^2}) = \\frac{\\sigma^2(n-dim(V))}{n}$$\n",
    "\n",
    "Hence, the **MLE IS BIASED** as \n",
    "$\\mathbb{E}(\\hat{\\sigma^2}) = \\frac{\\sigma^2(n-dim(V))}{n} \\neq \\sigma^2$ **but there IS CONVERGENCE** when $n\\rightarrow +\\infty$\n",
    "\n",
    "Most of the time, people prefer $$\\hat{\\sigma^2} = \\frac{||Y-\\Pi_VY||^2}{n-dim(V)}\\quad\\text{cf. ln() in $\\mathbb{R}$}$$ as $$\\mathbb{E}(\\hat{\\sigma^2}) = \\frac{\\sigma^2(n-dim(V))}{n-dim(V))}=\\sigma^2$$\n",
    "\n",
    "<u>Classical estimator</u>\n",
    "\n",
    "$$\\hat{\\sigma}^2_{classic}=\\hat{\\sigma}^2_{MLE}*\\frac{n}{n-dim(V)}$$\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95296df6",
   "metadata": {},
   "source": [
    "## 2.4 - Cognitive Model of Categorization\n",
    "\n",
    "A participant is given a list of objects to categorize. The participant has a learning and transfer phase.\n",
    "\n",
    "Models of transfer aim at modeling how a human can categorize given what they have learned (There is no good answer). There is no feedback.\n",
    "\n",
    "> modeling learning -> difficult as there is no independence -> different participants learn differently\n",
    "\n",
    "Modeling transfer is easier because:\n",
    "\n",
    "> one can image that the answer for each object is independent from the other ones (no feedback)\n",
    ">\n",
    "> for the same reason, they are \"identically distributed\" given the object that is presented\n",
    "\n",
    "<u>Nosofsky, 1986:</u> Proposition of the Generalized Context Modeling (GCM)\n",
    "\n",
    "For a given object $x$, one represent it by a **list of attributes** (color, shape, etc.) such that: $$x = (x_1, ..., x_d)$$\n",
    "\n",
    "**Similarity** between an object $x$ and an object $y$ is represented by:\n",
    "\n",
    "$$S(x, y) = e^{-c.d(x, y)}$$ Where $$d(x, y) = \\overset{d}{\\underset{i=1}{\\sum}}|X_i, y_i|$$\n",
    "\n",
    "We say that $\\mathbb{P}(\\text{y is said to be in A}) = \\frac{\\underset{x\\in\\mathbb{L}\\cap A}{\\sum} S(y, x)}{\\underset{x\\in\\mathbb{L}}{\\sum} S(y, x)}$ where $\\mathbb{L}$ is the set of learned object. \n",
    "\n",
    "$$\\underset{x\\in\\mathbb{L}}{\\sum} S(y, x) = \\underset{x\\in\\mathbb{L}\\cap A}{\\sum} S(y, x) + \\underset{x\\in\\mathbb{L}\\cap B}{\\sum} S(y, x)$$\n",
    "\n",
    "In a 2-category situation.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383efe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
