{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a9fdb3",
   "metadata": {},
   "source": [
    "# Stochastic Models in Neurocognition\n",
    "\n",
    "## Class 2\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Preliminary Notes**:\n",
    "\n",
    "- From now on the true parameter is denote $\\theta_0$, unknown.\n",
    "- $\\lambda$ is a reserved letter for the firing rate\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e71d9",
   "metadata": {},
   "source": [
    "# 1 - Contrast\n",
    "\n",
    "## 1.1 - Maximul likelihood\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta\\in\\Theta}{argmax}\\,\\,l_\\theta(X) = \\underset{\\theta\\in\\Theta}{argmin}\\,\\,-l_\\theta(X)$$\n",
    "\n",
    "In the Gaussian linear model: \n",
    "\n",
    "$$\\hat{\\mu} = \\underset{\\\\mu\\in V}{argmin}\\,\\,||Y-\\mu||^2$$\n",
    "\n",
    "## 1.2 - Contrast definition\n",
    "\n",
    "In general, a contrast is a function $C(\\theta, X)$ where $\\theta\\in\\Theta$ and $X$ is the obsservation such that:\n",
    "\n",
    "> $\\mathbb{E}_{\\theta_0}((C(\\theta,X))$ is minimal at $\\theta_0$\n",
    "\n",
    "Then, the estimator defined by the minimum of the contrast is:\n",
    "\n",
    "> $\\hat{\\theta} = \\underset{\\theta\\in\\Theta}{argmin}\\,\\,C(\\theta, X)$\n",
    "\n",
    "<u>Examples of contrast functions:</u> MLE, least squares\n",
    "\n",
    "## 1.3 - Log-Likelihood\n",
    "\n",
    "Let us prove that $C(\\theta, X) = -\\mathcal{l}_\\theta(X)$ is a contrast\n",
    "\n",
    "$$\\mathbb{E}_{\\theta_0}[-\\mathcal{l}_\\theta(X)] = E_{\\theta_0}(-logf_\\theta(X))=\\int-log[f_\\theta(x)]f_{\\theta_0(X)}dx$$ where $f_\\theta$ is the density of the model with parameter $\\theta$\n",
    "\n",
    "### Kullback-Leiber Divergence\n",
    "\n",
    "The KL divergence is defined by $$K(f, g)$$ where $f$ and $g$ are two densities such that:\n",
    "\n",
    "\\begin{align}\n",
    "K(f,g)&=\\int log(\\frac{f}{g})f(x)\\,\\,\\delta x\\\\\n",
    "&=\\mathbb{E}_{X\\sim f}[log(\\frac{f}{g})] \\\\\n",
    "&=\\int [\\frac{g(x)}{f(x)}-log(\\frac{f}{g})-1]f(x)\\,\\,\\delta x\\\\\n",
    "&= \\int\\frac{g}{f}f + \\int -log(\\frac{f}{g})f + \\int(-1)f\\\\\n",
    "\\int f &= 1\\\\\n",
    "\\int g & = \\int \\frac{g}{f}f = 1\n",
    "\\end{align}\n",
    "\n",
    "As such we have:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{g}{f} - log\\frac{g}{f} - 1 &= e^u - u - 1 \\quad\\text{with $u=log\\frac{g}{f}$}\\\\\n",
    "u&\\rightarrow e^u - u - 1 = h(u)\\\\\n",
    "h'(u) &= e^u - 1\n",
    "\\end{align}\n",
    "\n",
    "<span style=\"color:red\">ADD IMAGE DERIVATION TABLE</span>\n",
    "\n",
    "We find that:\n",
    "\n",
    "$$K(f, g) = \\int h(log\\frac{g}{f})f\\,\\,\\delta x$$\n",
    "\n",
    "As such:\n",
    "\n",
    "> K is **always positive or null** and K = 0 *iff* $\\forall x,\\,\\,log\\frac{g}{f}=0$ which means $\\frac{g}{f} = 1$ or $g = f$.\n",
    "\n",
    "#### <u>Summary</u>\n",
    "\n",
    "\\begin{align}\n",
    "K(f, g) &= \\mathbb{E}_{X\\sim f}[log\\frac{f}{g}]\\\\\n",
    "&\\ge 0\\\\\n",
    "&= 0 \\quad\\text{*iff* $f=g$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "**The KL Leibler divergence measures a distance between densities. The same computation applies to PDF.**\n",
    "\n",
    "### Tying back with $C(\\theta,X) = -\\mathcal{l}_\\theta(X)$\n",
    "\n",
    "\\begin{align}\n",
    "0\\rightarrow\\mathbb{E}_{\\theta_0}[C(\\theta, X)] &= \\mathbb{E}_{\\theta_0}[-log(f_\\theta(X))]\\\\\n",
    "&=\\mathbb{E}_{\\theta_0}[log(f_{\\theta_0}(X))] + \\mathbb{E}_{\\theta_0}[-log(f_\\theta(X))] - \\mathbb{E}_{\\theta_0}[log(f_{\\theta_0}(X))]\\\\\n",
    "&=\\mathbb{E}_{\\theta_0}[log(\\frac{f_{\\theta_0}(X)}{f_\\theta(X)})] - \\mathbb{E}_{\\theta_0}[log(f_{\\theta_0}(X))]\\\\\n",
    "&=K(f_{\\theta_0}, f_\\theta)- \\mathbb{E}_{\\theta_0}[log(f_{\\theta_0}(X))]\n",
    "\\end{align}\n",
    " \n",
    "<span style=\"color:red\">CHECK f_\\theta or f_0</span>\n",
    " \n",
    "$\\mathbb{E}_{\\theta_0}[-log(f_{\\theta_0}(X))]$ does not depend on $\\theta$. So $\\mathbb{E}_{\\theta_0}[-log(f_\\theta(X))]$ is minimal when $K(f_{\\theta_0}, f_\\theta)$ is null, hence when $\\forall x,\\,\\,f_{\\theta_0} = f_\\theta$.\n",
    "\n",
    "If there is no problem of identification, then two different parameters $\\theta$ encode two different densities then it implies that $\\theta=\\theta_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae37f21c",
   "metadata": {},
   "source": [
    "## 1.4 - Least Square Contrast\n",
    "\n",
    "\\begin{align}\n",
    "X &= \\theta_0 + \\epsilon\\\\\n",
    "\\theta_0&\\in\\mathbb{R}^d\n",
    "\\epsilon\\sim pdf \\text{ with }\\mathbb{E}[\\epsilon]=0\\\\\n",
    "\\end{align}\n",
    "\n",
    "<span style=\"color:red\">ADD MISSING LINE</span>\n",
    "\n",
    "If we do not specify the distribution of $\\epsilon$, we **cannot compute the MLE** but we can use the constract:\n",
    "\n",
    "$$C(\\theta, X) = ||X-\\theta||^2$$\n",
    "\n",
    "### Verification that it is a contrast\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{\\theta_0}[C(\\theta, X))] &= \\mathbb{E}_{\\theta_0}[||X-\\theta||^2] = \\mathbb{E}_{\\theta_0}[||X||^2] - 2<\\theta, X> + ||\\theta||^2\\\\\n",
    "&=\\mathbb{E}_{\\theta_0}[||X||^2] - 2<\\theta, \\mathbb{E}_{\\theta_0}[X]> + ||\\theta||^2\\\\\n",
    "&=\\mathbb{E}_{\\theta_0}[||X||^2] - 2<\\theta, \\theta_0> + ||\\theta||^2\\quad\\text{by definition of $X$}\\\\\n",
    "&=\\mathbb{E}_{\\theta_0}[||X||^2] - 2<\\theta, \\mathbb{E}_{\\theta_0}[X]> + ||\\theta||^2\\\\\n",
    "&=- 2<\\theta, \\theta_0> + ||\\theta||^2\\\\\n",
    "&=||\\theta-\\theta_0||^2 - ||\\theta_0||^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "So this is minimal when $\\theta = \\theta_0$. As such, $\\theta\\rightarrow||X-\\theta||^2$ is a contrast, also known as a **least square contrast for vectors**.\n",
    "\n",
    "$$\\hat{\\theta} = \\underset{\\theta\\in V}{argmin}||X-\\theta||^2 = \\prod_V X$$\n",
    "\n",
    "With: $\\prod_V$ the project over $V$.\n",
    "\n",
    "### Least-Square contrast for densities\n",
    "\n",
    "We have $X = (X_1, ..., X_n)^T$ $IID$ with density $f_0$. Let $f$ be a candidate density and $C(f, X) = \\frac{2}{n}\\sum_{i=1}f(X_i) - \\int[f(x)]^2\\delta x$. \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{X_i\\sim f_0}[C(f,X)] &= -\\frac{2}{n}\\sum_{i=1}\\int f(x)f_0(x)\\delta x + \\int[f(x)]^2\\delta x\\\\\n",
    "&=-2 \\int f(x)f_0(x)\\delta x + \\int[f(x)]^2\\delta x\\\\\n",
    "&= \\int(f(x)-f_0(x))^2 \\delta x - \\int[f(x)]^2\\delta x\\\\\n",
    "\\end{align}\n",
    "\n",
    "So $C(f, X)$ is minimal when $\\int(f(x)-f_0(x))^2 \\delta x$ is minimal but $\\int(f(x)-f_0(x))^2 \\delta x$ is $\\ge 0$ and $=0$ *iff* $\\forall x,\\,\\,f(x) = f_0(x)$.\n",
    "\n",
    "**So $C(f, X)$ is a contrast, called the least-square contrast for densities**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a984d8",
   "metadata": {},
   "source": [
    "# 2 - Choice of Models\n",
    "\n",
    "### Example from data science\n",
    "\n",
    "Given $Y_i$ the firing rate, we have $Y_i = f_0 = (W_i) + \\epsilon_i$ with $W_i$ the weight and $\\epsilon\\,iid\\,\\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "A possible model would be $f(W) = a + b * W$ with a, b unknown (linear). We can also think about quadratic, cubic, etc. models.\n",
    "\n",
    "Another set of models rely on the *angle* such that: $Y_i = f_0(U_i) + \\epsilon_i$ with $U_i$ the angle of the movement and $\\epsilon_i\\sim\\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "<u>model 1:</u> $f(U_i) = a + b * cos(2\\pi U_i)$\n",
    "\n",
    "<u>model 2:</u> $f(U_i) a_0 + a_1*cos(U_i) + a_{-1}*sin(U_i) + a_d *cos(d*U_i) + a_{-d}*sin(d*U_i)$\n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH FFT</span>\n",
    "\n",
    "In general, we have a bunch of linearly independent functions:\n",
    "\n",
    "$$\\phi_1(X), ..., \\phi_d(X)$$\n",
    "\n",
    "The problem is broached such that: $Y_i = f_o(X_i) + \\epsilon_i$ with $\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "The model of $dim(d)$ is $f(X)\\in Vec(\\phi_1(X), ..., \\phi_d(X)) = V$ such that $f(X_i) = a_1\\phi_1(X_i) + ... + a_d\\phi_d(X_i) \\quad\\forall i$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$d << n$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "For this model, we stop clear of $n$, but when?\n",
    "\n",
    "For this model, we know that $(..., \\hat{f}_d(X_i), ...)^T = \\prod_V(Y_i)$\n",
    "\n",
    "<span style=\"color:red\">ADD IMAGE PROJECTION (slide 17)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f656c2",
   "metadata": {},
   "source": [
    "## 2.1 - Bias Variance Decomposition\n",
    "\n",
    "$$Y_i = f_0(X_i) + \\epsilon_i$$\n",
    "\n",
    "For model $d$, we define $(\\hat{f}_d(X_1), ..., \\hat{f}_d(X_n))^T =\\hat{f}_d(X)$. What is the $d$ for which:\n",
    "\n",
    "$$\\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\hat{f}_d(X)||^2]$$ is the smallest.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\hat{f}_d(X)||^2] &= \\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\prod_VY||^2]\\\\\n",
    "&= \\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\prod_Vf_0(X) + \\prod_Vf_0(X) - \\prod_VY||^2]\\\\\n",
    "&= \\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\prod_Vf_0(X)||^2] + \\mathbb{E}_{X\\sim F_0}[||\\prod_V\\epsilon||^2]\\\\\n",
    "&= \\mathbb{E}_{X\\sim F_0}[||f_0(X) - \\prod_Vf_0(X)||^2] + \\sigma^2d\\quad\\text{since $||\\prod_V\\epsilon||^2\\sim\\sigma^2\\mathcal{X}^2(d)$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "<span style=\"color:red\">CHECK COMPUTATION BIAS TERM</span>\n",
    "\n",
    "$||f_0(X) - \\prod_Vf_0(X)||^2$ is the bias term, which decreases with d increases, and the variance term $\\sigma^2d$ increases with d.\n",
    "\n",
    "### Model choice for d\n",
    "\n",
    "To choose a good $d$ you need a trade-off between:\n",
    "- complexity of the model (to have a small bias)\n",
    "- variance of each of the coefficient (to have a small variance)\n",
    "\n",
    "We define an **oracle** $\\tilde{d}$ which is a benchmark:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{d}&=\\underset{d\\in[1,...,n-1]}{argmin}\\big(||f_0(X) - \\prod_Vf_0(X)||^2 + \\sigma^2d\\big)\n",
    "\\end{align}\n",
    "\n",
    "### Mallow's Cp\n",
    "\n",
    "This consists in choosing $\\hat{d} = \\underset{d}{argmin}||Y-\\prod_VY||^2 + 2\\sigma^2d$, assuming that $\\sigma$ is known. $||Y-\\prod_VY||^2$ is the least square, and $2\\sigma^2d$ is a penalty.\n",
    "\n",
    "If there was no penalty, $\\hat{d}=\\underset{d}{argmin}(||Y-\\prod_VY||^2)$ takes always the largest $d$.\n",
    "\n",
    "**A penalty is always there to avoid overfitting. Mallow's Cp is a particular case which satisfies an oracle inequality.**\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}[||Y-\\prod_VY||^2] &\\le C * \\text{oracle risk}\\\\\n",
    " &\\le C * \\underset{d}{min}(||Y-\\prod_VY||^2 + 2\\sigma^2d)\\\\\n",
    "C&\\rightarrow \\text{ converges to 1}\n",
    "\\end{align}\n",
    "\n",
    "### Akaike Criterion (AIC)\n",
    "\n",
    "Model $M$ parametrized by $\\Theta_M\\rightarrow MLE\\,\\hat{\\theta}_M\\rightarrow f_{\\hat{\\theta}_M}(X)$ the density of X when the parameters are MLE. $f_{\\hat{\\theta}_M}$ is the likelihood on the model $M$ at the parameter $\\hat{\\theta}_M$.\n",
    "\n",
    "AIC is: $$\\hat{M} = \\underset{M\\in\\mathcal{M}}{argmin}\\big(-log(f_{\\hat{\\theta}_m}(X) + dim(M)\\big)$$\n",
    "\n",
    "Oracle inequalities exist for that too, as long as there are few models with the same number of parameters.\n",
    "\n",
    "<span style=\"color:red\">EXPLANATION MODEL DIM</span>\n",
    "\n",
    "### BIC Criterion\n",
    "\n",
    "\\begin{align}\n",
    "Y_i = a_0 + a_1 X_i^1 + ... + a_pX_p^p + \\epsilon_i\\quad\\text{ with }\\epsilon_i\\sim\\mathcal{N}(0,\\sigma^2)\\\\\n",
    "\\end{align}\n",
    "\n",
    "To do variable selection, one could put in competition\n",
    "\\begin{align}\n",
    "V_0 &= Vec(1,...,1)^T\\\\\n",
    "V_{01} &= Vect((1,...,1), X^1)^T\\\\\n",
    "V_{0...d} &= Vect(1,...,1), X^1, ..., X^d)\n",
    "\\end{align}\n",
    "\n",
    "The number of models with dimension 2 is $\\frac{p(p+1)}{2}$. As such:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{M}&= \\underset{M\\in\\mathcal{M}}{argmin} \\big[\\frac{ln(n)dim(M)}{2} - log(f_{\\hat{\\theta}_M}(X))\\big]\n",
    "\\end{align}\n",
    "\n",
    "Be careful. The more models in competition, the larger the penalty, and one may end with a model of small dimensions. Whereas if the number of models was not too big, one could have used AIC and selected it.\n",
    "\n",
    "### Other penalties\n",
    "\n",
    "There are other penalties: \n",
    "\n",
    "- Lasso\n",
    "- Ridge\n",
    "- ElasticNet\n",
    "\n",
    "### Slope heuristic\n",
    "\n",
    "<span style=\"color:red\">ADD PLOT IMAGE </span>\n",
    "\n",
    "**Note**: More mathematics details, <u>Birgie and Massart</u>, \"Gaussian Model Selection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd33d7f",
   "metadata": {},
   "source": [
    "## 2.1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\overset{n}{\\underset{i=}{\\sum}}\n",
    "\\overset{n}{\\underset{i=}{\\prod}}\n",
    "\n",
    "<span style=\"color:red\">ADD </span>\n",
    "\n",
    "\\mathcal{E} \n",
    "\\mathcal{N}\n",
    "\n",
    "\\underset{\\theta\\in\\Theta}{argmax}\\,\\,\n",
    "\n",
    "\\begin{align}\n",
    "&=\\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
