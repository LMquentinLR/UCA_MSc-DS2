---
title: "tutorial_2_model_selection"
output: html_document
author: Quentin Le Roux
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

## Gaussian model selection in the simplified Georgopoulos setting

**In this setting, we measure $n$ times the same cell but each time with a different angle of movement
$\forall i\in \{0,...,n-1\},\,u_i = 2\pi(\frac{i}{n})$. We decompose the regression function on a Fourier basis until size $p$ with $2p + 1 \le n$.**

<hr>

***Question 1. Create a matrix $X$ of size $n\times2p + 1$. For $u_i = 2π(\frac{i}{n})$, the coefficient $X_{i,j}$ is given as follows:***

\begin{align}
X_{i+1,j} &= 1 \text{ if } j = 1\\
X_{i+1,j} &= \cos(ku_i) \text{ if } j = 2k\\
X_{i+1,j} &= \sin(ku_i) \text{ if } j = 2k + 1
\end{align}

```{r generate_fourier_matrix}

u <- function(i, n) {2*pi*i/n}

generate_fourier_matrix <- function(n, p) {
  ### Generates a matrix of size (n, 2p+1) of fourier
  ### coefficients
  # Checks if the parameters are ill-defined
  if (2*p+1>n) {
    print("FAILED to comply with 2p+1 lesser or equal to n")
    return(NULL)
  }
  # Computes the fourier matrix
  fourier = matrix(NA, nrow=n, ncol=2*p+1)
  for (row in 1:n) {
    for (col in 1:(2*p+1)) {
      # Checks for the first column to populate with 1s
      if (col==1) {
        fourier[row, col] = 1
      } else {
        # Checks for pair-numbered columns, then populates
        if (col%%2==0){
          k = col/2
          fourier[row, col] = cos(k*u(row-1,n))
        } else {
          k = (col-1)/2
          fourier[row, col] = sin(k*u(row-1,n))
        }
      }
    }
  }
  return(fourier)
}

```

We print an example matrix $X$ with parameters $n=10$ and $p=2$:

```{r fourier_matrix_example}

X = generate_fourier_matrix(10, 2)
round(X, 5)

```

<hr>

***Question 2. By computing the different scalar products (with R), show that the columns of $X$ are orthogonal but not of norm 1 and renormalize them: this gives you the matrix $X′$.***

```{r orthogonality_check}

check <- function(vec1, vec2) {
  ### two vectors v1 and v2 are orthogonal if their inner 
  ### product is equal to zero
  # Rounding to account for floating point memory mgt
  round(sum(vec1 * vec2),15) == 0
}

check_length <- function(x) {
  # Check the length of unique elems in a list of two elements
  # Used to remove elems of a permutation where the same index
  # is used twice
  length(unique(x))==2
  }

check_orthogonality <- function(matrix_to_check) {
  ### Exhaustively checks for the orthogonality of each columns
  ### of a matrix with each other
  nc = dim(matrix_to_check)[2]
  # Computes all permutations of column indexes
  # not optimized, contains clones (swapped indexes)
  perms = expand.grid(c1=1:nc, c2=1:nc, stringsAsFactors=F)
  perms = perms[apply(perms, 1, check_length),]
  # Interatively checks for orthogonality
  all(apply(perms, 1,function(x) {check(
    matrix_to_check[,x[1]], matrix_to_check[,x[2]])
    }))
}

norm <- function(vec) {sqrt(sum(vec * vec))}

compute_norm_list <- function(matrix_to_check) {
  ### Computes the norm of each column of a matrix and returns
  ### the list of norms
  apply(matrix_to_check, 2, norm)
}

orthonormalize_matrix <- function(matrix_to_cast) {
  ### Divides every column vector of a matrix by its norm
  apply(matrix_to_cast, 2, function(x){x/norm(x)})
}

```

We check whether our previous example matrix $X$ has its vectors orthogonal with each other.

```{r example_orthogonality_check}

check_orthogonality(X)

```

We then check whether the column vectors of the example matrix $X$ have a norm different from 1. The list of norms of the $5$ columns is:

```{r norm_check}

compute_norm_list(X)

```

We then normalize each column vector of the example matrix $X$ by its norm, and check that each column vector's norm of the resulting matrix $X'$ is 1.

```{r orthonormalize}

Xprime = orthonormalize_matrix(X)

compute_norm_list(Xprime)

```

We can also check whether $X'$ is orthonormal:

```{r orthonormal_check}

# Rounding to take into account floating point memory mgt
round(t(Xprime)%*%Xprime, 15)

```
<hr>

***Question 3. Give, for a given $d<p$, the projection estimator of the regression function composed of the first $2d + 1$ Fourier coefficients. Transform this into a function in R.***

```{r projection_matrix}

construct_projection_matrix <- function(X, d) {
  ### Retrieves the first 2*d+1 columns of the generated fourier
  ### projection matrix
  # Checks if the parameters are ill-defined
  if (2*d+1>dim(X)[2]) {
    print("FAILED to comply with 2d+1 <= ncols(X)")
    return(NULL)
  }
  # Returns the truncated projection matrix
  X[,1:(2*d+1)]
}

```

<hr>

***Question 4. Simulate two different experiments:***
\begin{align}
Y_i &= 16 + 14 \cos(u_i) + 5\epsilon_i\\
Y_i &= 10\exp(−\frac{(u_i-\pi)^2}{0.2}) + 1 * \epsilon_i\\
\text{with }\epsilon_i&\sim\mathcal{N}(0,1)\text{ (IID)}
\end{align}
***Plot the data, the true function to estimate and 4 or 5 different projection estimators in each cases. Explain the problem of overfitting and the problem of taking a model of too low dimension. NB: you can try other regression function if you want***

We decide to proceed with the example simulation with $1000$ simulation, i.e. $n=1000$. We arbitrarily set the factor $p$ to 10, leaving us enough space to produce 5 random draws for a variable $d$ such that $d\in\{1,...,p\}$.

```{r simulations}

# Sets preliminary variables

n = 1000
p = 10
d_sequence = sort(unique(round(runif(5, 1, p))))

# Prints parameters of simulation
cat("Given", n, "observations for each simulation and a maximum",
    "of", p, "Fourier coefficients, we select the following",
    "amounts of coefficients to perform projection/regressions:",
    d_sequence)

# Generates the simulations

generate_experiment_1 <- function(n) {
  ### Generates the simulation 1: 16+14*cos(u_i)+5*\epsilon_i
  f <- function(x){16+14*cos(u(x,n))}
  truth = apply(as.matrix(seq(1,n,1)),1,f)
  noise = 5*rnorm(n,0,1)
  return(list("truth"=truth,"noisy"=truth+noise))
}

generate_experiment_2 <- function(n) {
  ### Generates the simulation 2: 10*exp(-\frac{(u_i-\pi)^2}{0.2})
  ###                             + \epsilon_i
  f <- function(x){10*exp(-(x-pi)^2/0.2)}
  truth = apply(as.matrix(seq(1,n,1)),1,f)
  noise = rnorm(n,0,1)
  return(list("truth"=truth, "noisy"=truth+noise))
}

Y1 = generate_experiment_1(n)
Y2 = generate_experiment_2(n)

```
Once we generated the two simulations (truth and noise-added truth), we compute the resulting projection estimator for both cases:

```{r projection_estimators}

# Generates the orthonormal matrix Xprime
X = orthonormalize_matrix(generate_fourier_matrix(n, 2))

```

Once the data generated, we can plot them:

```{r data_plots}

plot_simulation <- function(Y, simulation_num) {
  plot(Y$noisy, type="l", col=3, 
       main=paste("Experiment",simulation_num,"simulations"),
       xlab="Angle parameter i",
       ylab=paste("Y",simulation_num))
  lines(Y$truth, type="l", col=4)
}

plot_simulation(Y1, 1)
plot_simulation(Y2, 2)

```

<hr>

***Question 5. Make a function in R which, for a given $p$, computes the Mallow's Cp criterion for all the models ($d\le p$) and gives the estimator which minimizes the criterion. NB: the behavior would be similar for other models with - log-likelihood and AIC criterion***

<hr>

***Question 6. Let us now fix $p$ and look at all the subspaces $V$ that can be written on the basis up to $2p+1$. For instance, we could have a subspace generated by $1,\cos(u),\sin(2u)$. Let us look at the BIC criterion and assume $\sigma^2$ is known. Simplify the formulas to show that this minimization problem is solved by taking as non-zeros coordinates the ones for which $|<Y |e_i>|$ is larger than $\sqrt{\ln(n)\sigma^2}. Implement this method and show the resulting estimator on both previous cases.***
