{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059224fb",
   "metadata": {},
   "source": [
    "# Advanced Learning Class 1: An Introduction to Explainable AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed8a6d",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Machine Learning algorithms are used for **critical** decisions (e.g. medical, credit scoring, etc.). However, ML solutions like DNN are often referred as \"black boxes\". It happens that *in many applications, performance matters and not interpretability*.\n",
    "\n",
    "### What is an interpretable model?\n",
    "\n",
    "Older, linear models were interpretable given their sparse number of features (e.g. credit scoring, historical features). Small decision trees (shallow depth and/or breadth) are also an interpretable case. The problem of those old models however is that they do not achieve high enough accuracy (in complex tasks). \n",
    "\n",
    "### Explainability\n",
    "\n",
    "<u>Goal:</u> Explain how a ML algorithm takes a particular decision.\n",
    "\n",
    "<u>Local explanation:</u> \n",
    "Explaining a black-box model \n",
    "- $f:\\mathbb{R}^D\\rightarrow\\mathbb{R}$ at an example $\\zeta\\in\\mathbb{R}^D$\n",
    "\n",
    "In many cases, explanation corresponds to an interpreatable model that looks like $f$ around $\\zeta$. But on the surface, ***visual clues help us understand the model*** (e.g. only parts of a given image triggers some network neurons).\n",
    "\n",
    "<u>Notables sources:</u>\n",
    "\n",
    "- Brown et al., \"Language Models are few-shot learning,\" arxiv, 2020\n",
    "- Dastile et al., \"Statistical and machine learning models in credit scoring: A Systematic literature survey,\" Applied Soft Computing, 2020\n",
    "- Ribeiro et al., \"'Why should I trust you?' Explaning the Predictions of Any Classifier,\" SGKDD, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe014a",
   "metadata": {},
   "source": [
    "## 2. Game-Theory Perspective\n",
    "\n",
    "### Shapley Values\n",
    "\n",
    "<u>Setting:</u> D-player Game (Shapley, A value for n-person game, Contributions to the theory of games, 1953)\n",
    "- characteristic function $v:2^D\\rightarrow\\mathbb{R}$ gives the value of a coalition $S$\n",
    "- total sum of gains the members of $S$ can obtain by cooperation\n",
    "\n",
    "<u>Idea:</u> distribute fairly the total gains to the players, assuming that they all contribute\n",
    "\n",
    "<u>Shapley value of player $j$:</u>\n",
    "$$\\phi_j(v)= \\underset{S\\in\\{1,...,D\\}/\\{j\\}}{\\sum} \\frac{|S|!(D-|S|-1)!}{D!}(v(S\\cup\\{j\\})-v(S))$$\n",
    "\n",
    "<u>Intuition:</u> if player $j$ plays much better than the others, then $v(S\\cup\\{j\\})$ consistently higher than $v(S)$, and $\\phi_j(v)>>0$.\n",
    "\n",
    "<u>Properties:</u>\n",
    "\n",
    "- **efficiency**: sum o Shapley values = gain of the whole coalition: $\\sum_j\\phi_j(v)=v(\\{1,...,D\\})$\n",
    "- **symmetry**: players with the same skills are rewarded equally\n",
    "- **linearity**: $v$ and $w$ two characteristic function then $\\forall J\\in\\{1,...,D\\},\\phi_j(v+w) = \\phi_j(v)+\\phi_j(w)$\n",
    "- **null player**: a player that does not bring anything is not rewarded\n",
    "\n",
    "<u>Shapley Theorem</u>\n",
    "\n",
    "We can consider features as players in the context of ML algorithms. The **Shapley regression value** associated to feature $j$ is given by: $$\\phi_j(v)= \\underset{S\\in\\{1,...,D\\}/\\{j\\}}{\\sum} \\frac{|S|!(D-|S|-1)!}{D!}(f_{S\\cup\\{j\\})}(\\zeta_{S\\cup\\{j\\}})-f_S(\\zeta_S))$$\n",
    "\n",
    "<u>Issues</u>\n",
    "\n",
    "- computationally expensive: $\\mathcal{O}(2^D)$\n",
    "- retraining the model each time\n",
    "\n",
    "One can subsample in the sum over all subsets. Instead of retraining the model, mimic the removal of a variable via random sampling, i.e., replace $f_S(\\zeta_S)$ with $\\mathbb{E}[f(x)|x_S=\\zeta_S]$. $f$ can ow be any model, provided that we can query efficiently.\n",
    "\n",
    "### SHAP\n",
    "\n",
    "<u>Idea:</u> Linear regression on the presence/absence of features. We defice interpretable features $z\\in\\{0,1\\}^d$, with $d\\le D$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3700403",
   "metadata": {},
   "source": [
    "## 3. Local approximations\n",
    "\n",
    "### Introduction\n",
    "\n",
    "**Theorem (Taylor, order one)**: let $f$ be a smooth-enough function in the neighborhood of $\\zeta\\in\\mathbb{R}^D$. Then: $f(x)=f(\\zeta)+\\nabla f(\\zeta)^T(x-\\zeta) + o(||x-\\zeta||)$ where $\\nabla f(\\zeta)$ is the *gradient* of $f$ at $\\zeta$.\n",
    "\n",
    "A ML model is a complicated function of the inputs. It approximate this function by a first order approximation.\n",
    "\n",
    "### Gradient-Based Explanations\n",
    "\n",
    "<u>Idea:</u> Take the gradient of the function to explain at the point of interest.\n",
    "\n",
    "<u>Intuition:</u> Tell us how much a change in each input dimension would change the prediction *in a small neighborhood around $\\zeta$*. \n",
    "\n",
    "<u>Notable sources:</u>\n",
    "\n",
    "- Erhan et al., \"Visualizing higher-layer features of a deep network,\" tech. report, 2009"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743576b5",
   "metadata": {},
   "source": [
    "## 4. LIME\n",
    "\n",
    "LIME: Local Interpretable Model-Agnostic Explanations. It has a complicated operating procedure but is very popular. $\\zeta\\in\\mathbb{R}^D$, an image to explain, and $f:\\mathbb{R}^D\\rightarrow[0,1]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
