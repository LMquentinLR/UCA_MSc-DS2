---
title: "latent_space_model"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Statistical models for network visualization

## The MDS

```{r}

library(sna)
library(VBLPCM)
data(sampson)

A = as.matrix.network.adjacency(samplike)

```

Once we have the network loaded, we first need to compute the shortest-path distance on the graph.

```{r}

g = as.network(A)
D = dist(g)

```

Here the `dist` function recognizes the object `g` is a network and the shortest-path distance is computed.

The MDS algorithm is implemented in R by the "cmdscale" function:

```{r}
?comdscale
```

A call to MDS will be:

```{r}
Z = cmdscale(D,2)
plot(Z,pch=19)
```

If we want to also plot the edges, we can use the gplot function of the same package:

```{r}

library(sna)
gplot(A, coord=Z, edge.col="lightgray")

```

It is also interesting to come back to a very simple network.

```{r}

A = cbind(
  c(0, 1, 1, 1, 0),
  c(1, 0, 0, 0, 1),
  c(1, 0, 0, 0, 1),
  c(1, 0, 0, 0, 1),
  c(0, 1, 1, 1, 0)
)

g = as.network(A)
D = dist(g)
Z = cmdscale(D, 2)
gplot(A, coord=Z, edge.col="lightgray")

```

```{r}

A = cbind(
  c(0, 1, 1, 1),
  c(1, 0, 0, 0),
  c(1, 0, 0, 0),
  c(1, 0, 0, 0)
)

g = as.network(A)
D = dist(g)
Z = cmdscale(D, 2)
gplot(A, coord=Z, edge.col="lightgray", label=1:5)

```

As we see on this quite simple example, MDS can be trapped in some difficult numerical situations and the resulting visualization can become very poor.

## The Latent Space Model

Let's first try to code ourselves an implementation of the LSM.

```{r}

negloglik <- function(theta, A, d) {
  # Definition of the log-likelihood to optimize
  ll=0
  n = nrow(A)
  alpha = theta[1]
  Z = matrix(theta[-1], ncol=d)
  D = as.matrix(dist(Z))
  for (i in 1:(n-1)) {
    for (j in (i+1):n) {
      ll = ll + A[i,j] * (alpha - D[i,j]) - log(1+exp(alpha - D[i,j]))
    }
  }
  ll = -2*ll # to account all the symmetric values
  ll
}

LSM <- function(A, d, ...) {
  # A: Adjacency Matrix, denoted by X in the math
  # Y: covariate (could be null)
  # d: latent space (2 by default)
  # initializes values for theta
  n = nrow(A)
  alpha_0 = 0.5
  Z_0 = rnorm(n*d, 0, 1)
  theta = c(alpha_0, Z_0)
  # numerical optimization TO MODIFY SO IT MAXIMUZES THE LOG-LIKELIHOOD
  theta_opt = optim(theta, negloglik, A=A, d=d,...)
  print(matrix(theta_opt$par, ncol=d))
  print(theta_opt[-1])
  print(theta_opt[1])
  alpha = theta_opt$par[1]
  Z = matrix(theta_opt$par[-1], ncol=d)
  return(list(alpha=alpha, Z=Z))
}

```

<u>Professor's code:</u>

```{r}

library(sna)
LSM <- function(A,dim=2,...){
  # A is the adjacency matrix and is denoted by X in the maths,
  # dim is the dimensionality of the latent space (2 by default)
  n = nrow(A)
  
  # definition of the log-likelihood to optimize
  negloglik <- function(theta,A,dim){
    ll = 0
    alpha = theta[1]
    Z = matrix(theta[-1],ncol=dim)
    D = as.matrix(dist(Z))
    for (i in 1:(n-1)){
      for (j in (i+1):n){
        ll = ll + A[i,j]*(alpha - D[i,j]) - log(1 + exp(alpha - D[i,j]))
      }
    }
    negll = - 2 * ll
  }
  
  # Initial values for theta
  alpha_0 = 0.5
  Z_0 = rnorm(n*dim,0,1)
  theta = c(alpha_0,Z_0)
  
  
  # Numerical optimization
  theta_opt = optim(theta,negloglik,A=A,dim=dim,...) 
  
  # Plot and return the results
  alpha = theta_opt$par[1]
  Z = matrix(theta_opt$par[-1],ncol=dim)
  return(list(alpha=alpha,Z=Z))
}

```

Test:

```{r}

test = LSM(A, 2)

```

## Exercise

This code won't work as it is because it contains an error. Modify it such that the function maximizes the log-likelihood and then test it on both the simple network and the sampson monks.


```{r}

library(VBLPCM)
data(sampson)
A = as.matrix.network.adjacency(samplike)

n = nrow(A)
dim = 2
Z.rand = matrix(rnorm(n*dim, 0, 1), ncol=dim)
out = LSM(A, 2, method = "SANN")
gplot(A, coord=Z.rand, edge.col="lightgray", label = rownames(A))

```

Just to quantify the importance of the work done by LSM in finding relevant positions for plotting the network, we can compare it to a random positionning of the nodes:

```{r}

n = nrow(A)
dim = 2
Z.rand = matrix(rnorm(n*dim, 0, 1), ncol=dim)
gplot(A, coord=Z.rand, edge.col="lightgray", label=rownames(A))

```

One of the main interest of the LSM model is its ability to model the uncertainty of the edges we observe or not. We can indeed, once the model parameters estimated, compute the posterior probabilities of the edges to be observed:

$$P(Y_{ij}=1|\hat{\alpha}, \hat{Z})$$

Remember that the model assumes $\mathrm{logit}(P(Y_{ij}=1|\alpha}, Z)) = \alpha - d(z_i, z_j)$ where $\mathrm{logit}(p)=\log(\frac{p}{1-p})$. So, it is quite easy to get the expression of $P(Y_{ij}=1|\alpha,Z)$.

```{r}

LSM.posterior <- function(out) {
  n = nrow(out$Z)
  P = matrix(0, n, n)
  D = as.matrix(dist(out$Z))
  for (i in 1:(n-1)){
    for (j in (i+1):n){
      P[i,j] = P[j,i] = exp(out$a - D[i,j]) / (1+exp(out$a - D[i,j]))
    }
  }
  return(P)
}

A = cbind(
  c(0, 1, 1, 1),
  c(1, 0, 0, 0),
  c(1, 0, 0, 0),
  c(1, 0, 0, 0)
)

out = LSM(A, dim=2, method="SANN")
P = LSM.posterior(out)
par(mfrow=c(1,2))
image(A, main="Observed edges")
image(P, main="Posterior probabilities")

```

```{r}

data(sampson)
A = as.matrix.network.adjacency(samplike)
out = LSM(A, dim=2, method="SANN")
P = LSM.posterior(out)

par(mfrow=c(1,2))
image(A, main="Observed edges")
image(P, main="Posterior probabilities")

par(mfrow=c(1,2))
gplot(A, coord=out$Z, edge.col="lightgray")
gplot(A, coord=out$Z, edge.col="lightgray", edge.lwd=20*P)

```

The LSM model is also implemented in the `latentnet` package with a MCMC algorithm for inference (of a Bayesian version of the model):

```{r}

#install.packages("latentnet")
library(latentnet)
g = as.network(A)
out = ergmm(g ~ euclidean(d=2))
plot(out)

```

# Clustering of the nodes of a network

## Latent Position Cluster Model (LPCM)

Let's first use the `latentnet` package. It is almost the same call as above, except now $K=3$ groups.

```{r}

library(latentnet)
g = as.network(A)
out = ergmm(g ~ euclidean(d=2, G=3))
plot(out)

```

The output of this call is both a different visualization and the clustering of the nodes. All the estimated parameters are returned in the `mkl` field, among which the `out$mkl$mbc` provides the posterior cluster means, the variances, the posterior cluster membership probabilities, and the cluster proportions

```{r}

out$mkl$mbc

```

Alternatively, it is also possible to infer the LPCM model with a VBEM algorithm.


```{r}

library(VBLPCM)
v.start = vblpcmstart(samplike, G=3, model = "plain", LSTEPS=1e3)
v.fit = vblpcmfit(v.start, STEPS=30)
plot(v.fit)

```

### Exercise

Estimate the performance/computing time with both methods.
