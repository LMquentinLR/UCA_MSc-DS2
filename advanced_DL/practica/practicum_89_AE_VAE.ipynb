{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "practicum_8_AE_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python [conda env:ML_base]",
      "language": "python",
      "name": "conda-env-ML_base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tyYDKlcqkBE"
      },
      "source": [
        "# AE/VAE on binarised MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B30ONxejRn-"
      },
      "source": [
        "In this tutorial, we'll play a bit with a binarised version of MNIST. We'll train an autoencoder on it and a variational autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sguCcpg2Yw"
      },
      "source": [
        "# Loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwsirHNgywB"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPYV_R2ghyx"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h58v-MfihGen"
      },
      "source": [
        "# Loading MNIST and binarising it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sTryqQpguSj"
      },
      "source": [
        "(train_images_nonbinary, y_train), (test_images_nonbinary,  y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images_nonbinary = train_images_nonbinary.reshape(train_images_nonbinary.shape[0], 28*28)\n",
        "test_images_nonbinary = test_images_nonbinary.reshape(test_images_nonbinary.shape[0], 28*28)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int32)\n",
        "y_test =tf.cast(y_test, tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "xP-RnOcwjIUK",
        "outputId": "7ba9dffc-eac3-4f66-c272-8b4516d168ca"
      },
      "source": [
        "plt.imshow(train_images_nonbinary[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JaPwhRdhI8s"
      },
      "source": [
        "Then we binarise the data. There are many ways to do that. Here, we simply round the numbers, following the [TF tutorial on convolutional VAEs](https://www.tensorflow.org/tutorials/generative/cvae)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIpmbV8g4d3"
      },
      "source": [
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images = train_images_nonbinary/255.\n",
        "test_images = test_images_nonbinary/255.\n",
        "\n",
        "# Binarization\n",
        "train_images[train_images >= .5] = 1.\n",
        "train_images[train_images < .5] = 0.\n",
        "test_images[test_images >= .5] = 1.\n",
        "test_images[test_images < .5] = 0.\n",
        "\n",
        "train_images = tf.cast(train_images, tf.float32)\n",
        "test_images = tf.cast(test_images, tf.float32)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "s8pOg8LNg7p2",
        "outputId": "dfe44f13-b6c9-444b-b65f-b997050eeaf3"
      },
      "source": [
        "plt.imshow(train_images[0, :].numpy().reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADd0lEQVR4nO3dwU7bQBhG0UzF+7/ydNUNikKVwfEd+5wliEIXV7/EJ+Mx53wAPX/O/gGA58QJUeKEKHFClDgh6uvVJ8cYfpULB5tzjmcfdzkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVFfZ/8A7GPOufT1Y4xD//2V713kckKUOCFKnBAlTogSJ0SJE6LECVF2zhMcueeV3fX//S6XE6LECVHihChxQpQ4IUqcEGVKecKv/N+z42NZZS4nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDlec4nVl9Vd9VX3XnO9bNcTogSJ0SJE6LECVHihChxQpQ4IcrO+YYz/z7rXb/3HbmcECVOiBInRIkTosQJUeKEKHFClJ1zM6vPkrIPlxOixAlR4oQocUKUOCFKnBBlSjnBq7nDn5/kH5cTosQJUeKEKHFClDghSpwQJU6IsnPGrL4+cHUn9chZh8sJUeKEKHFClDghSpwQJU6IEidE2Tk3s7qD/uTV19tAP8vlhChxQpQ4IUqcECVOiBInRIkTouycF3PkDupZ0c9yOSFKnBAlTogSJ0SJE6LECVHihCg7582sbI1HPiv6eNhBv3M5IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRHhm7mdXHvvgclxOixAlR4oQocUKUOCFKnBAlToiyc27GTnkfLidEiROixAlR4oQocUKUOCFKnBBl54zZecf0Cr/f5XJClDghSpwQJU6IEidEiROiTCkHMIfwG1xOiBInRIkTosQJUeKEKHFClDgh6pY758475E/slNfhckKUOCFKnBAlTogSJ0SJE6LECVHb7pxX3ipfsWPeh8sJUeKEKHFClDghSpwQJU6IEidEZXfOK++Ytkr+h8sJUeKEKHFClDghSpwQJU6IEidEZXdOWyB353JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosaVX7UHO3M5IUqcECVOiBInRIkTosQJUX8BqMZW74BKFnQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lglEtUoJ5bIX"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(32) # TF creates the batches for us"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukx-euwihWT0"
      },
      "source": [
        "## Regular Auto-Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqCIj_dXjv-o"
      },
      "source": [
        "Let us start with a regular auto-encoder. We want to encode (or compress) the data into a low-dimensional embedding.\n",
        "\n",
        "\n",
        "*   The low-dimensional subspace $\\mathbb{R}^d$ is the **code space**.\n",
        "\n",
        "*   The decoder is a function $$\\text{Decoder}_{\\theta_{decoder}} : \\mathbb{R}^d \\longrightarrow [0,1]^{28 \\times 28}$$ that will transform low-dimensional codes into distributions over images.\n",
        "\n",
        "*   Conversely, the encoder is a function $$\\text{Encoder}_{\\theta_{encoder}} : \\{0,1\\}^{28 \\times 28} \\longrightarrow \\mathbb{R}^d$$ that will encode the images in low-dimensional space.\n",
        "\n",
        "* Both functions are parametrised by (deep) neural nets whose weigths are stored in $\\theta_{decoder}$ and $\\theta_{encoder}$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5cOzLSCOtOy"
      },
      "source": [
        " The loss function is the **average reconstruction error**:\n",
        "\n",
        "$$\\ell_{AE}(\\theta_{encoder},\\theta_{decoder}) = \\sum_{i=1}^n \\text{Xentropy}(x_i,\\text{Decoder}_{\\theta_{decoder}}(\\text{Encoder}_{\\theta_{encoder}}(x_i)).$$\n",
        "\n",
        "Indeed, we want to be able to **reconstruct (approximately) the original data using only the low-dimensional embeddings**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO7T-0Uijvbq"
      },
      "source": [
        "h = 200 # number of hidden units\n",
        "sigma = \"relu\" # activation function\n",
        "d = 2\n",
        "\n",
        "encoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(d),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4FsRuTOVNSS"
      },
      "source": [
        "decoder = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[d,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(28*28), # I don't use a sigmoid here because TF will use it inside tf.nn.sigmoid_cross_entropy_with_logits\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8i_fYDMrftq"
      },
      "source": [
        "@tf.function\n",
        "def reconstruction_error(data):\n",
        "  reconstruction_parameters = decoder(encoder(data))\n",
        "  reconstruction_errors = tf.nn.sigmoid_cross_entropy_with_logits(labels = data, logits = reconstruction_parameters)\n",
        "  return tf.reduce_mean(reconstruction_errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDEryxresksx"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "theta_ae = encoder.trainable_variables + decoder.trainable_variables"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trlwRBjU5DTo"
      },
      "source": [
        "@tf.function\n",
        "def train_step_ae(data):\n",
        "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
        "    loss = reconstruction_error(data)  # the loss is the average reconstruction error (cross-entropy)\n",
        "  gradients = tape.gradient(loss, theta_ae)  # here, the gradient is automatically computed\n",
        "  optimizer.apply_gradients(zip(gradients, theta_ae))  # Adam iteration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrir2CzgsXLY",
        "outputId": "b09fe3f2-3ab0-49d8-a33f-8a2c38c005e2"
      },
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, labels in train_dataset:\n",
        "    train_step_ae(images) # Adam iteration\n",
        "  train_error = reconstruction_error(train_images)\n",
        "  test_error = reconstruction_error(test_images)\n",
        "  if (epoch % 5) == 1:\n",
        "    print('Epoch  %g' %epoch)\n",
        "    print('Train reconstruction error  %g' %train_error.numpy())\n",
        "    print('Test reconstruction error  %g' %test_error.numpy())\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1\n",
            "Train reconstruction error  0.187175\n",
            "Test reconstruction error  0.187019\n",
            "-----------\n",
            "Epoch  6\n",
            "Train reconstruction error  0.167717\n",
            "Test reconstruction error  0.169014\n",
            "-----------\n",
            "Epoch  11\n",
            "Train reconstruction error  0.162598\n",
            "Test reconstruction error  0.164343\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwmwVInJThdu"
      },
      "source": [
        "Let's look at the faitfulness of the reconstructions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "qWlprhE8sqtS",
        "outputId": "5bbee7ac-fb4c-4faf-bc5c-345d8265e1a1"
      },
      "source": [
        "plt.imshow(train_images[1:2, :].numpy().reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADhUlEQVR4nO3dQW7bQBQFwTDI/a88WRugRcPUiE1O1dJeSJvGB/RAaRtj/AF6/l79BoB94oQocUKUOCFKnBD179U/t23zUS5MNsbY9v7uckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6Je/gQg6xlj3q8+btvuL93xDZcTosQJUeKEKHFClDghSpwQJU6IsnMuZuaOefa17aBfuZwQJU6IEidEiROixAlR4oQocUKUnfNhrtwxeS+XE6LECVHihChxQpQ4IUqcEGVKuZnyVHL0yNfRe3/1/xUfJ3M5IUqcECVOiBInRIkTosQJUeKEKDvnBcpbZdWKX6vpckKUOCFKnBAlTogSJ0SJE6LECVF2zsWcfebyrq99Ry4nRIkTosQJUeKEKHFClDghSpwQZeec4Mq97uxzjU98LvKuXE6IEidEiROixAlR4oQocUKUOCHKzvkLd94xn+qJ32vrckKUOCFKnBAlTogSJ0SJE6JMKTtmTyV3/Fifz3M5IUqcECVOiBInRIkTosQJUeKEKDvnBHZM3sHlhChxQpQ4IUqcECVOiBInRIkTopbcOa/8akv4KZcTosQJUeKEKHFClDghSpwQJU6IWnLnPMvzmnyCywlR4oQocUKUOCFKnBAlTogSJ0TZOfmYmc/RPnF7djkhSpwQJU6IEidEiROixAlRphTexlTyXi4nRIkTosQJUeKEKHFClDghSpwQZefkx2b/dOKKW+YrLidEiROixAlR4oQocUKUOCFKnBBl5+QLz2R2uJwQJU6IEidEiROixAlR4oQocUKUnfMXjrbAmXve7Gcqz7BjvpfLCVHihChxQpQ4IUqcECVOiDKlTFCeO84wlXyWywlR4oQocUKUOCFKnBAlTogSJ0TZORdjq7wPlxOixAlR4oQocUKUOCFKnBAlTohacuc82vrKz2PaKdfhckKUOCFKnBAlTogSJ0SJE6LECVFL7pxHbIkUuJwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oSobYxx9XsAdricECVOiBInRIkTosQJUeKEqP/RSlP2Pu1yYQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "gfL1Ua8JARrh",
        "outputId": "da5fa2ec-b09e-44a3-883b-e4190258eec0"
      },
      "source": [
        "plt.imshow((decoder(encoder(train_images[1:2, :])).numpy().reshape((28,28))>0.5), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADkElEQVR4nO3dUU7jQBRFQYzY/5abDYQYEuw+bld9MhqNiXT0JK7MbGOMD6Dnc/YDAI+JE6LECVHihChxQtTXsz/cts2PcuFgY4zt0dddTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC1NfsB+BcY4zZj/CjbdtmP0KKywlR4oQocUKUOCFKnBAlTogSJ0TZOS+mvFO+69n3dscN1OWEKHFClDghSpwQJU6IEidEmVJiVp5K+BuXE6LECVHihChxQpQ4IUqcECVOiLJzTnDVLfPd17be+b73/u6Kr5S5nBAlTogSJ0SJE6LECVHihChxQpSd8wDlHXPmHrj3b5c/txlcTogSJ0SJE6LECVHihChxQpQ4IcrOuZgV32u8K5cTosQJUeKEKHFClDghSpwQJU6IsnO+YOZ7h3bM+3A5IUqcECVOiBInRIkTosQJUaaUB0wlFLicECVOiBInRIkTosQJUeKEKHFClJ1zAlvm/9vbpq/4mbucECVOiBInRIkTosQJUeKEKHFC1C13zqPf17zipnZ1K37mLidEiROixAlR4oQocUKUOCFKnBB1y52TOWb+PuArcjkhSpwQJU6IEidEiROixAlR4oQoO+cLVnx3kB6XE6LECVHihChxQpQ4IUqcELXslOL1pPMd+Znfcb5yOSFKnBAlTogSJ0SJE6LECVHihKhld07+n/868VwuJ0SJE6LECVHihChxQpQ4IUqcEGXnvBnvuV6HywlR4oQocUKUOCFKnBAlTogSJ0TZOV+wtxXOfC+xvGN6X/NvXE6IEidEiROixAlR4oQocULUslPKsx/bHz03lOeMd5hCzuVyQpQ4IUqcECVOiBInRIkTosQJUcvunM/s7XWr7pS/YcvscDkhSpwQJU6IEidEiROixAlR4oSoW+6ce668g9op1+FyQpQ4IUqcECVOiBInRIkTosQJUXbOF9gSOYPLCVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiNrGGLOfAXjA5YQocUKUOCFKnBAlTogSJ0R9A7eFU/T8PaOlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDdheQApPeec"
      },
      "source": [
        "Questions:\n",
        "\n",
        "\n",
        "1. Write down the mathematical formulas for the encoder and decoder defined via Keras.\n",
        "2.   Does the decoder output binary or continuous images? Would it be possible to change that?\n",
        "3.   Try to plot the low-dimensional representation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYoHDDwWP19L"
      },
      "source": [
        "**This simple auto-encoder is not a probabilistic model! A VAE is a probabilistic version of it that allows to also sample new \"fake\" images (among many other applications).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1pPoLI6jlKI"
      },
      "source": [
        "# Simple generative modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfHdx_pdQocD"
      },
      "source": [
        "Before training a VAE, we start with a very dumb generative model as an appetizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKb773pujpvx"
      },
      "source": [
        "We build here a **super simple** generative model for the data set. Specifically, the model is simply a product of Bernoulli distributions:\n",
        "$$p (\\textbf{x}) = \\prod_{j = 1}^{28 \\times 28} \\mathcal{B}(x_j|\\pi_j).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTLh2xtFkqzA"
      },
      "source": [
        "Training can simply be done by computing the means of all the pixels independently (and add a small regularization error):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUtIAt3nlhgC"
      },
      "source": [
        "pis_bern = train_images.numpy().mean(0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKeaKSYIltiB"
      },
      "source": [
        "prod_of_bernoullis =  tfd.Independent(distribution  = tfd.Bernoulli(probs = pis_bern), reinterpreted_batch_ndims = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "iJILKmwriDNG",
        "outputId": "96812715-7dad-4cd3-bfaa-573ccdb01ac4"
      },
      "source": [
        "sample_pofbern = tf.reshape(prod_of_bernoullis.sample(1),(28,28))\n",
        "plt.imshow(sample_pofbern, cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEE0lEQVR4nO3dUWrcMABF0bpk/1t2F1DPGCI0uiOf89mSZhK4CPqQfZzn+Qfo+bv6AwDXxAlR4oQocUKUOCHq591fHsfhv3JhsvM8j6s/d3JClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROi3r4CkDnO8/WbFY/j8m1wW3j3c9/Z+ffyipMTosQJUeKEKHFClDghSpwQJU6IsnMusOtmd7dj7vpzz+LkhChxQpQ4IUqcECVOiBInRIkTouycmxm5M1n2xA3VyQlR4oQocUKUOCFKnBAlTogypcSMTgYrJ4XRGeepjwx9xckJUeKEKHFClDghSpwQJU6IEidE2TkvlK8nzb4S9u5nm/17Wfm9i5ycECVOiBInRIkTosQJUeKEKHFClJ3zwuhmNrJFrvzed+4+W3mLLH+2V5ycECVOiBInRIkTosQJUeKEKHFClJ1zgpmb2ey9buZGu3L/Le6Yd5ycECVOiBInRIkTosQJUeKEKHFClJ3zy8y+Uzny7NhR37hFzuTkhChxQpQ4IUqcECVOiBInRJlSJig/hnHmHDJ7ankaJydEiROixAlR4oQocUKUOCFKnBBl5/yF0R1z5SMiR7jS9VlOTogSJ0SJE6LECVHihChxQpQ4IcrOeaH8CMiVG+udmfdYy3dkZ3FyQpQ4IUqcECVOiBInRIkTosQJUY/cOUe3vtFNbeV9zm+9a7rjjnnHyQlR4oQocUKUOCFKnBAlTogSJ0Rtu3PO3OtWvody9n3OJ+6JVU5OiBInRIkTosQJUeKEKHFC1LZTSvlVeSsfTzny9WaWz3JyQpQ4IUqcECVOiBInRIkTosQJUdvunDPNfjzlOysf67nyutkTr7o5OSFKnBAlTogSJ0SJE6LECVHihKhtd86Re4mzHz858m+Pfu/qK/74n5MTosQJUeKEKHFClDghSpwQJU6I2nbnHNnkZj93duV9zjvvPtvKn/uJG6uTE6LECVHihChxQpQ4IUqcELXtlLLSysdPrnyEZPmzfSMnJ0SJE6LECVHihChxQpQ4IUqcEGXnvPDNe9zKx3bO/vqncXJClDghSpwQJU6IEidEiROixAlRj9w5yzvm7DuRtsbv4eSEKHFClDghSpwQJU6IEidEiROiHrlzjm59XoXHJzg5IUqcECVOiBInRIkTosQJUeKEqEfunKNsjXyCkxOixAlR4oQocUKUOCFKnBBlSvmF8qM12YeTE6LECVHihChxQpQ4IUqcECVOiLJz/oIdk09wckKUOCFKnBAlTogSJ0SJE6LECVHH6OvsgDmcnBAlTogSJ0SJE6LECVHihKh/eOvDNQgnafcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVFrfB03RKcd"
      },
      "source": [
        "Why is this clearly a pretty bad model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyDKnZgGpbVZ"
      },
      "source": [
        "We usually assess the quality of such generative models by computing the **test log-likelihood**. Here, we can do that easily because the model is simple:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLYwDGrUl77G",
        "outputId": "b899a958-17fd-4852-a2b5-e95a16193d3e"
      },
      "source": [
        "tf.reduce_mean(prod_of_bernoullis.log_prob(test_images)).numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-inf"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnj_Px1kpu8n"
      },
      "source": [
        "The state of the art for binarised versions of MNIST is around -75, which is much higher than that! Using a VAE (or an IWAE), we'll try to close the gap a bit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXYPIjFTQeT"
      },
      "source": [
        "# VAE: defining a deep latent variable model and its inference network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YMTygyEqKek"
      },
      "source": [
        "We will use a **deep latent variable model with a Gaussian prior and a Bernoulli observation model**. This can be written:\n",
        "\n",
        "$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}_1,...,\\mathbf{x}_n) = \\prod_{i=1}^n p(\\mathbf{x}_i|\\mathbf{z}_i)p(\\mathbf{z}_i),$$\n",
        "$$p(\\mathbf{z}_i) = \\mathcal{N}(\\mathbf{z}_i|\\mathbf{0}_d,\\mathbf{I}_d), $$\n",
        "$$p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i|\\mathbf{z}_i) = \\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i)),$$\n",
        "\n",
        "where $\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}: \\mathbb{R}^d \\rightarrow [0,1]^p$ is a function (called the **decoder**) parametrised by a deep neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGQPPSexqQ7E"
      },
      "source": [
        "d = 2 # dimension of the latent space\n",
        "\n",
        "p_z = tfd.Independent(tfd.Normal(loc = tf.zeros(d, tf.float32), scale = tf.ones(d, tf.float32)),reinterpreted_batch_ndims=1)  # that's the prior"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Tv17Q_U883"
      },
      "source": [
        "h = 200 # number of hidden units\n",
        "sigma = \"relu\" # activation function\n",
        "\n",
        "decoder_vae = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[d,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(28*28), # No need to use the logistic to ensure that the output is in [0,1] here, it will be done later\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnYPGF-qf3t"
      },
      "source": [
        "To be able to train our model, we will need an **encoder** (aka **inference network**), that will allow us to approximate the intractable posterior $p(\\mathbf{z}|\\mathbf{x})$.\n",
        "The approximate posterior is defined as follows\n",
        "$$ q(\\mathbf{z}|\\mathbf{x}) = \\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x} ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x} )),\n",
        "$$ \n",
        "where $\\mathbf{x} \\mapsto (\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x} ), \\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x} ))$ is a function from the data space (i.e. $\\{0,1\\}^p$) to $\\mathbb{R}^d \\times [0, \\infty[^d$ parametrised by a deep neural network. In other words, the encoder outputs the mean and the diagonal of the covariance matrix of the approximate posterior $q(\\mathbf{z}|\\mathbf{x})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6mC83FtU8_j"
      },
      "source": [
        "encoder_vae = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(h, activation=sigma),\n",
        "  tfkl.Dense(d)# , What should we put here ?\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrfR3gnpuaYT"
      },
      "source": [
        "# Sampling from the deep generative model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgh2YuNpudlB"
      },
      "source": [
        "Before we train it, let's look at some samples from the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V9ts94uuhaj"
      },
      "source": [
        "@tf.function\n",
        "def vae_sample(num_samples):\n",
        "  codes = p_z.sample(num_samples)\n",
        "  out_decoder = decoder_vae(codes)\n",
        "  p_xgivenz = tfd.Independent(tfd.Bernoulli(out_decoder), reinterpreted_batch_ndims = 1)\n",
        "  return p_xgivenz.sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "v4pzrcv0ukhP",
        "outputId": "9cf4962e-4d6b-4669-b9cc-c443f05fe10d"
      },
      "source": [
        "plt.imshow(vae_sample(1).numpy().reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S845ndpbuq7V"
      },
      "source": [
        "They look really bad... But we haven't started training yet!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTUCa8GsrjGL"
      },
      "source": [
        "# Training a VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIk8z8YXrniP"
      },
      "source": [
        "The **VAE objective** is defined as\n",
        "$$\n",
        "\\mathcal{L}_1 (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z} \\sim q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)} \\left[ \\log\\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}_i|\\mathbf{z})p(\\mathbf{z})}{q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)} \\right].\n",
        "$$\n",
        "\n",
        "It is a lower bound of the likelihood of the deep latent variable model. Rather than the intractable likelihood, we'll **maximise this bound**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QXxdx59rviy"
      },
      "source": [
        "To see more clearly the neural nets inside the objective, we may rewrite it as:\n",
        "$$\n",
        "\\mathcal{L}_1 (\\boldsymbol{\\theta,\\gamma}) = \\sum_{i=1}^n \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))} \\left[ \\log\\frac{\\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}))\\mathcal{N}(\\mathbf{z}|\\mathbf{0}_d,\\mathbf{I}_d)}{\\mathcal{N}(\\mathbf{z} | \\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))} \\right].\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gbLKHGUryWT"
      },
      "source": [
        "**Create a TF function that computes an unbiased estimate of it!** (using a single sample from the approximate posterior)\n",
        "\n",
        "A good way to go is to follow these steps, for all $\\mathbf{x}_i$ in your batch:\n",
        "\n",
        "\n",
        "*   Encode the batch fo data points, to get the $\\mathbf{m}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ), \\text{Diag}(\\mathbf{s}_\\boldsymbol{\\gamma} ( \\mathbf{x}_i ))$ \n",
        "*   Use these encoder outputs to define $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)$ as a TF probability distribution\n",
        "*   Take one sample $\\mathbf{z}_i$ from $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}|\\mathbf{x}_i)$, and use it to:\n",
        "  *   Compute $\\mathcal{B} (\\mathbf{x}_i|\\boldsymbol{\\pi}_{\\boldsymbol{\\theta}}(\\mathbf{z}_i))$ by decoding $\\mathbf{z}_i$\n",
        "  *   Compute the rest of the log of $p(\\mathbf{z}_i)$ and $q_{\\boldsymbol{\\gamma}}(\\mathbf{z}_i|\\mathbf{x}_i)$ using the $\\texttt{.logprob}$ method from TF probability.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF-ewKECuTsm"
      },
      "source": [
        "EPOCHS = 15\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, labels in train_dataset:\n",
        "    train_step_ae(images) # Adam iteration\n",
        "  train_error = reconstruction_error(train_images)\n",
        "  test_error = reconstruction_error(test_images)\n",
        "  if (epoch % 5) == 1:\n",
        "    print('Epoch  %g' %epoch)\n",
        "    print('Train reconstruction error  %g' %train_error.numpy())\n",
        "    print('Test reconstruction error  %g' %test_error.numpy())\n",
        "    print('-----------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H-wt885wVZj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}