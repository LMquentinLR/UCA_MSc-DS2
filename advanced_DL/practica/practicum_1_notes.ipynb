{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cea60c",
   "metadata": {},
   "source": [
    "# Practicum 1 - Notes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575e35f",
   "metadata": {},
   "source": [
    "<u>Types of AI identified by Alan Turing in 1948 (\"*What is AI?*\" article on alanturing.net):</u>\n",
    "\n",
    "- **knowledge-driven** (top-down), hypothetical-deductive machines (inputs help make assumption and develop a program\n",
    "    - e.g. evolutionary algorithms (genetic algorithms, ant colony algorithms), knowledge representation, reasoning, logic, automata, intelligent agent systems, etc...\n",
    "    - independence of low-level details of implementation mechanisms\n",
    "    - modeling is based on observations of supposed-intelligent patterns/processes\n",
    "    - ***hypothetically deductive***\n",
    "    \n",
    "- **data-driven** (bottom-up), building of incremental and mathematical mechanisms in order to take decisions\n",
    "    - MLP, decision trees, backprop., random forests, SVM, Boosting, DL, etc.\n",
    "    - ***inductive***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08b451",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "<u>Rough definition:</u>\n",
    "\n",
    "> Finding a function given a set of parameters $\\alpha$ that links data $X$ to output $y$.\n",
    "\n",
    "$$\\{X\\}, f(X, \\alpha) \\Rightarrow y$$\n",
    "\n",
    "*Doubtful* that ML is AI as there is no mimicking of intelligent patterns, processes.\n",
    "\n",
    "<u>Link to statistics:</u> \n",
    "\n",
    "- $S$: Training set\n",
    "- $P(X,y) = P(X)P(y|X)$: Joint distribution\n",
    "\n",
    "Statistics tries to approximate laws of the data.\n",
    "\n",
    "ML tries to learn a function/approximation of what is best (terms of minimizing a specific error measure/cost/loss related to the problem: $L((x, y), \\alpha)\\in [a, b]$. Given the data, we are looking to estimate the **empirical error/risk**.\n",
    "\n",
    "**Key assumption of ML:** Elements of training set $S=\\{x_i, y_i\\}_{i=1,...,m}$ are considered IID according to joint probability $P(X,y)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e07959",
   "metadata": {},
   "source": [
    "> *The Vapnik Theory validates the bottom-up approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e7a2b",
   "metadata": {},
   "source": [
    "***/!\\ Cover's theorem***\n",
    "\n",
    "Cover's theorem is a statement in computational learning theory and is one of the primary theoretical motivations for the use of non-linear kernel methods in machine learning applications. The theorem states that given a set of training data that is not linearly separable, one can with high probability transform it into a training set that is linearly separable by projecting it into a higher-dimensional space via some non-linear transformation. The theorem is named after the information theorist Thomas M. Cover who stated it in 1965. Roughly, the theorem may be stated as:\n",
    "\n",
    "A complex pattern-classification problem, cast in a high-dimensional space nonlinearly, is more likely to be linearly separable than in a low-dimensional space, provided that the space is not densely populated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc6a65e",
   "metadata": {},
   "source": [
    "***ML/DL is the problem of learning representation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f4050",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<u>MLE, binomial:</u>\n",
    "\n",
    "\\begin{align}\n",
    "\\sum ln\\,p(t_n|\\pi) &= \\sum ln(\\pi_n^t(1-\\pi_n)^{1-t})\\\\\n",
    "&= \\sum t_n\\,ln(\\pi_n) + \\sum (1-t_n)ln(1-\\pi_n)\\\\\n",
    "&= N_1ln(\\pi) + (n-N_1)ln(1-\\pi)\\\\\n",
    "MLE &\\overset{\\delta}{\\rightarrow} \\frac{N_1}{\\pi} - \\frac{N-N_1}{1-\\pi} = 0\\\\\n",
    "\\frac{N_1}{\\pi}&=\\frac{N-N_1}{1-\\pi} \\\\\n",
    "\\frac{1-\\pi}{\\pi}&=\\frac{N-N_1}{N} \\\\\n",
    "\\frac{1}{\\pi}-1 &=\\frac{N-N_1}{N} \\\\\n",
    "\\frac{1}{\\pi}&=\\frac{N}{N_1}\\\\\n",
    "\\pi_{MLE}&=\\frac{N_1}{N}\n",
    "\\end{align}\n",
    "where\n",
    "\\begin{align}\n",
    "N_1&=\\sum t_n \\\\\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
