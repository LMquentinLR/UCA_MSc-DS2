{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practicum_3_deepgenerativemodels.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jarvIzm6TMq_"
      },
      "source": [
        "# MSc Data Science: (deep) discriminative models for **MNIST**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3sguCcpg2Yw"
      },
      "source": [
        "# Loading useful stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMPYV_R2ghyx"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "tfd = tfp.distributions\n",
        "tfpl = tfp.layers"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zTFbzrTaie"
      },
      "source": [
        "#Loading and normalising MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo-tSpBxTMVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3de74c-4999-47f4-c22e-39bb0dfc5aac"
      },
      "source": [
        "(train_images, y_train), (test_images,  y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape(train_images.shape[0], 28*28)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28*28)\n",
        "\n",
        "y_train = tf.cast(y_train, tf.int32)\n",
        "y_test =tf.cast(y_test, tf.int32)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "dokv8mqDTMYD",
        "outputId": "cabd918c-b920-4dde-8c66-548b51213d6b"
      },
      "source": [
        "plt.imshow(train_images[0, :].reshape((28,28)), cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oshsOP18ToYt"
      },
      "source": [
        "# Normalizing the images to the range of [0., 1.]\n",
        "train_images = train_images/255.\n",
        "test_images = test_images/255."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32sDnj6Txzb"
      },
      "source": [
        "# Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJwgLwUnT23s"
      },
      "source": [
        "Our goal is to build a classifier on MNIST. A first simple example of classifier is **logistic regression**, a particular case of **discriminative model**. The model for (multiclass) logistic regression is \n",
        "$$ p (y | \\mathbf{x} ) = \\text{Cat} (y |\\text{Softmax}(\\mathbf{W}\\mathbf{x}+\\mathbf{b})),$$\n",
        "where the unknown parameters are $\\mathbf{W}$ and $\\mathbf{b}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-2YQtR9Uiqk"
      },
      "source": [
        "**Question 1.** What are the dimensions of $\\mathbf{W}$ and $\\mathbf{b}$? What is the total number of parameters in the model?\n",
        "\n",
        "$$W\\in\\mathbb{R}^{10\\times784}$$\n",
        "$$b\\in\\mathbb{R}^{10}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffD2B-BUYwnl",
        "outputId": "15da1db1-6f68-4af7-dbe1-09fd57b27081"
      },
      "source": [
        "train_images[0].shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1UhiLHDU4da"
      },
      "source": [
        "We will build our logistic regression model using [**keras**](https://keras.io/), a nice deep learning API. In particular, keras's [sequential model](https://keras.io/guides/sequential_model/) is simple way of building compositions of parametric functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpTTmZ36T2T9"
      },
      "source": [
        "logistic_regression = tfk.Sequential([\n",
        "  tfkl.InputLayer(input_shape=[28*28,]),\n",
        "  tfkl.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=1)) # because we have 10 classes\n",
        "])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj4QfIp0vKff"
      },
      "source": [
        "Here, $\\texttt{logistic_regression}$ represents the function $ \\mathbf{x} \\mapsto \\mathbf{W}\\mathbf{x}+\\mathbf{b}$, that takes vectors as inputs, and returns probabilities for each class. We can try with the first MNIST image. The model is initialised by sampling each coefficient of $\\mathbf{W}$ from a standard Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY-1z5qbUuKc",
        "outputId": "5ab00684-62c4-4339-91a7-05a03ab46019"
      },
      "source": [
        "logistic_regression(train_images[0:1,])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
              "array([[ 14.398406  ,   9.759026  ,   7.282996  ,   6.4654703 ,\n",
              "        -14.880961  ,   3.152071  ,  -4.856968  ,   4.6309824 ,\n",
              "          0.36624956,   9.990465  ]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijfw4WfpvsP_"
      },
      "source": [
        "Note that the output is a Tensorflow tensor. One can easily get a Numpy array instead this way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHqYNRStUuMz",
        "outputId": "bdd99532-c61d-456d-b8cf-31abc1a1d8c3"
      },
      "source": [
        "logistic_regression(train_images[0:1,]).numpy()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 14.398406  ,   9.759026  ,   7.282996  ,   6.4654703 ,\n",
              "        -14.880961  ,   3.152071  ,  -4.856968  ,   4.6309824 ,\n",
              "          0.36624956,   9.990465  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umvYlfGBv2TX"
      },
      "source": [
        "This $\\texttt{logistic_regression}$ conveniently can also handle **batches** of inputs. Here we look at the predictions of the 10 first digits of MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OC4ZgXVv1hc",
        "outputId": "32e2a92e-55bc-4eca-b5a4-f43a728fd6db"
      },
      "source": [
        "tf.nn.softmax(logistic_regression(train_images[0:10,]).numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
              "array([[9.7743452e-01, 9.4455909e-03, 7.9415645e-04, 3.5063879e-04,\n",
              "        1.8802726e-13, 1.2760463e-05, 4.2421360e-09, 5.5995031e-05,\n",
              "        7.8704608e-07, 1.1905461e-02],\n",
              "       [2.1088786e-01, 7.8894293e-01, 9.7266957e-07, 7.2862369e-09,\n",
              "        2.2465319e-07, 6.8607675e-10, 3.5139720e-16, 1.3819966e-08,\n",
              "        2.0155348e-16, 1.6794112e-04],\n",
              "       [8.0089545e-04, 1.3203209e-08, 9.5567942e-01, 3.8552933e-10,\n",
              "        6.1706291e-04, 1.4933558e-04, 4.1724567e-04, 1.3743354e-06,\n",
              "        1.5852065e-09, 4.2334657e-02],\n",
              "       [8.2788712e-05, 1.5258001e-01, 1.7019792e-07, 8.1430542e-01,\n",
              "        1.2785073e-07, 6.7236812e-07, 1.8456838e-06, 5.5242361e-10,\n",
              "        3.3028960e-02, 2.7414859e-09],\n",
              "       [6.4391261e-01, 3.5545176e-01, 7.6010975e-11, 2.5693338e-05,\n",
              "        7.6150404e-06, 2.4467399e-09, 9.7722821e-14, 1.0665828e-11,\n",
              "        2.1802928e-09, 6.0232927e-04],\n",
              "       [7.0909003e-12, 9.9995232e-01, 6.7542173e-20, 3.9691757e-05,\n",
              "        1.4104939e-09, 1.7625810e-13, 4.3123009e-11, 2.0791464e-17,\n",
              "        3.5947417e-06, 4.3629489e-06],\n",
              "       [9.7340512e-01, 3.6391754e-08, 1.9576550e-05, 3.3075479e-04,\n",
              "        5.9441995e-06, 2.5265984e-02, 5.0435639e-10, 7.7995471e-11,\n",
              "        5.1381867e-06, 9.6749584e-04],\n",
              "       [3.4215921e-01, 1.9215415e-03, 4.3972421e-05, 1.0076515e-01,\n",
              "        1.2143981e-07, 2.1434621e-12, 9.3009313e-14, 1.8234303e-03,\n",
              "        3.4607689e-05, 5.5325198e-01],\n",
              "       [7.5434709e-01, 2.2610251e-04, 3.5927442e-03, 1.0224036e-01,\n",
              "        1.0694367e-01, 3.0056769e-02, 2.4388801e-06, 1.1233832e-06,\n",
              "        1.7188780e-05, 2.5725698e-03],\n",
              "       [3.4200589e-08, 1.3086624e-06, 1.2755226e-05, 6.4385415e-05,\n",
              "        6.2553930e-01, 6.2800026e-10, 6.0287234e-04, 2.2201247e-04,\n",
              "        8.2283151e-07, 3.7355652e-01]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65KRekywEtx"
      },
      "source": [
        "One can check that each row of these predictions sums to one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9dvwvD5UuPQ",
        "outputId": "a70a84b7-2758-4621-b1c5-d02c58ca9983"
      },
      "source": [
        "np.sum(tf.nn.softmax(logistic_regression(train_images[0:10,]), axis=1).numpy(),1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9999999 , 0.99999994, 1.        , 1.        , 1.        ,\n",
              "       0.99999994, 1.0000001 , 1.        , 1.        , 1.        ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEUC7LCz_wG"
      },
      "source": [
        "One can us Tensorflow Probability to create the distribution  $p (y | \\mathbf{x} )$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tGJ3oYdT2WE"
      },
      "source": [
        "p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(train_images[0:10,]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHLCouzqwQBW",
        "outputId": "e615d5f0-d46c-45d4-c192-7c6a4b011d7d"
      },
      "source": [
        "p_ygivenx_logistic_regression.sample() # sampling the predicted labels"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 0, 1, 0, 9, 0, 4], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCkZytXzwQDl",
        "outputId": "98a59b7b-9c92-4ef7-df19-74326560627f"
      },
      "source": [
        "p_ygivenx_logistic_regression.mode() # looking at the most probable labels"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 0, 1, 0, 9, 0, 4], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_RnKqm1W1N"
      },
      "source": [
        "# Training the logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3E0_KzB1dRE"
      },
      "source": [
        "To train the classifier, we define a function that performs a gradient descent step. First, we choose the flavour of SGD that we want (in this case, the [fairly famous Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgMTlL-2wQFf"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5wus2n2Tubq"
      },
      "source": [
        "@tf.function\n",
        "def train_step_logistic_regression(data, labels):\n",
        "  # step 1: explain how to compute gradients\n",
        "  with tf.GradientTape() as tape: \n",
        "    # the gradient tape saves all the step that needs to be saved for automatic differentiation\n",
        "    # One could also use logits rather than probs and remove the softmax layer...\n",
        "    p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data))\n",
        "    # compute log(p(labels|data))\n",
        "    logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
        "    # the loss is the average negative log likelihood\n",
        "    loss = -tf.reduce_mean(logp_ygivenx_logistic_regression) \n",
        "  #step 2: computation of gradients\n",
        "  gradients = tape.gradient(loss, logistic_regression.trainable_variables)  # here, the gradient is automatically computed\n",
        "  # step 3: do one iteration of optimizer\n",
        "  optimizer.apply_gradients(zip(gradients, logistic_regression.trainable_variables))  # Adam iteration"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EmfozUiTueC"
      },
      "source": [
        "@tf.function\n",
        "def evaluate_logistic_regression(data, labels):\n",
        "  p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data))\n",
        "  logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
        "  log_likelihood = tf.reduce_mean(logp_ygivenx_logistic_regression)\n",
        "  y_pred = p_ygivenx_logistic_regression.mode()\n",
        "  acc = tf.reduce_mean(tf.cast(y_pred == labels,tf.float32))\n",
        "  return acc, log_likelihood"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sRf2jgK4W9s",
        "outputId": "abacc38d-a2a1-4d38-a966-26c568e642c1"
      },
      "source": [
        " evaluate_logistic_regression(train_images,y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=0.096883334>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=-14.342852>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfcpUtie4_MC"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(32) # TF creates the batches for us"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_XF8G4kTugJ",
        "outputId": "ec170428-d1fa-45ab-f213-62dfb6f49f9c"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  for images, labels in train_dataset:\n",
        "    train_step_logistic_regression(images, labels) # Adam iteration\n",
        "  acc, log_likelihood = evaluate_logistic_regression(train_images,y_train)\n",
        "  acc_test, log_likelihood_test = evaluate_logistic_regression(test_images,y_test)\n",
        "  print('Epoch  %g' %epoch)\n",
        "  print('Train accuracy  %g' %acc.numpy())\n",
        "  print('Test accuracy  %g' %acc_test.numpy())\n",
        "  print('Train log-likelihood  %g' %log_likelihood.numpy())\n",
        "  print('Test log-likelihood  %g' %log_likelihood_test.numpy())\n",
        "  print('-----------')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1\n",
            "Train accuracy  0.693783\n",
            "Test accuracy  0.7007\n",
            "Train log-likelihood  -1.63976\n",
            "Test log-likelihood  -1.57222\n",
            "-----------\n",
            "Epoch  2\n",
            "Train accuracy  0.712617\n",
            "Test accuracy  0.7197\n",
            "Train log-likelihood  -1.52026\n",
            "Test log-likelihood  -1.45722\n",
            "-----------\n",
            "Epoch  3\n",
            "Train accuracy  0.728783\n",
            "Test accuracy  0.7343\n",
            "Train log-likelihood  -1.42138\n",
            "Test log-likelihood  -1.36277\n",
            "-----------\n",
            "Epoch  4\n",
            "Train accuracy  0.7421\n",
            "Test accuracy  0.7477\n",
            "Train log-likelihood  -1.33791\n",
            "Test log-likelihood  -1.28341\n",
            "-----------\n",
            "Epoch  5\n",
            "Train accuracy  0.753683\n",
            "Test accuracy  0.7588\n",
            "Train log-likelihood  -1.26654\n",
            "Test log-likelihood  -1.21599\n",
            "-----------\n",
            "Epoch  6\n",
            "Train accuracy  0.763417\n",
            "Test accuracy  0.7688\n",
            "Train log-likelihood  -1.20525\n",
            "Test log-likelihood  -1.15848\n",
            "-----------\n",
            "Epoch  7\n",
            "Train accuracy  0.772483\n",
            "Test accuracy  0.7778\n",
            "Train log-likelihood  -1.15181\n",
            "Test log-likelihood  -1.10903\n",
            "-----------\n",
            "Epoch  8\n",
            "Train accuracy  0.780133\n",
            "Test accuracy  0.784\n",
            "Train log-likelihood  -1.10524\n",
            "Test log-likelihood  -1.06569\n",
            "-----------\n",
            "Epoch  9\n",
            "Train accuracy  0.78765\n",
            "Test accuracy  0.7921\n",
            "Train log-likelihood  -1.06389\n",
            "Test log-likelihood  -1.02755\n",
            "-----------\n",
            "Epoch  10\n",
            "Train accuracy  0.793983\n",
            "Test accuracy  0.7988\n",
            "Train log-likelihood  -1.02701\n",
            "Test log-likelihood  -0.993728\n",
            "-----------\n",
            "Epoch  11\n",
            "Train accuracy  0.7994\n",
            "Test accuracy  0.804\n",
            "Train log-likelihood  -0.993861\n",
            "Test log-likelihood  -0.963129\n",
            "-----------\n",
            "Epoch  12\n",
            "Train accuracy  0.805183\n",
            "Test accuracy  0.8088\n",
            "Train log-likelihood  -0.963976\n",
            "Test log-likelihood  -0.935363\n",
            "-----------\n",
            "Epoch  13\n",
            "Train accuracy  0.809467\n",
            "Test accuracy  0.8135\n",
            "Train log-likelihood  -0.937075\n",
            "Test log-likelihood  -0.910991\n",
            "-----------\n",
            "Epoch  14\n",
            "Train accuracy  0.814067\n",
            "Test accuracy  0.819\n",
            "Train log-likelihood  -0.912383\n",
            "Test log-likelihood  -0.888631\n",
            "-----------\n",
            "Epoch  15\n",
            "Train accuracy  0.8177\n",
            "Test accuracy  0.8228\n",
            "Train log-likelihood  -0.889698\n",
            "Test log-likelihood  -0.868356\n",
            "-----------\n",
            "Epoch  16\n",
            "Train accuracy  0.82145\n",
            "Test accuracy  0.8258\n",
            "Train log-likelihood  -0.868969\n",
            "Test log-likelihood  -0.849681\n",
            "-----------\n",
            "Epoch  17\n",
            "Train accuracy  0.824333\n",
            "Test accuracy  0.8296\n",
            "Train log-likelihood  -0.849861\n",
            "Test log-likelihood  -0.83278\n",
            "-----------\n",
            "Epoch  18\n",
            "Train accuracy  0.82825\n",
            "Test accuracy  0.8332\n",
            "Train log-likelihood  -0.831912\n",
            "Test log-likelihood  -0.815931\n",
            "-----------\n",
            "Epoch  19\n",
            "Train accuracy  0.831133\n",
            "Test accuracy  0.8371\n",
            "Train log-likelihood  -0.815381\n",
            "Test log-likelihood  -0.801001\n",
            "-----------\n",
            "Epoch  20\n",
            "Train accuracy  0.833917\n",
            "Test accuracy  0.8394\n",
            "Train log-likelihood  -0.799898\n",
            "Test log-likelihood  -0.787663\n",
            "-----------\n",
            "Epoch  21\n",
            "Train accuracy  0.8363\n",
            "Test accuracy  0.8418\n",
            "Train log-likelihood  -0.785408\n",
            "Test log-likelihood  -0.774731\n",
            "-----------\n",
            "Epoch  22\n",
            "Train accuracy  0.83855\n",
            "Test accuracy  0.843\n",
            "Train log-likelihood  -0.771899\n",
            "Test log-likelihood  -0.762077\n",
            "-----------\n",
            "Epoch  23\n",
            "Train accuracy  0.840867\n",
            "Test accuracy  0.8456\n",
            "Train log-likelihood  -0.759111\n",
            "Test log-likelihood  -0.750658\n",
            "-----------\n",
            "Epoch  24\n",
            "Train accuracy  0.842967\n",
            "Test accuracy  0.8479\n",
            "Train log-likelihood  -0.747022\n",
            "Test log-likelihood  -0.74005\n",
            "-----------\n",
            "Epoch  25\n",
            "Train accuracy  0.84505\n",
            "Test accuracy  0.8487\n",
            "Train log-likelihood  -0.735717\n",
            "Test log-likelihood  -0.729539\n",
            "-----------\n",
            "Epoch  26\n",
            "Train accuracy  0.846967\n",
            "Test accuracy  0.8509\n",
            "Train log-likelihood  -0.724861\n",
            "Test log-likelihood  -0.720285\n",
            "-----------\n",
            "Epoch  27\n",
            "Train accuracy  0.8484\n",
            "Test accuracy  0.8523\n",
            "Train log-likelihood  -0.714659\n",
            "Test log-likelihood  -0.711077\n",
            "-----------\n",
            "Epoch  28\n",
            "Train accuracy  0.85035\n",
            "Test accuracy  0.8531\n",
            "Train log-likelihood  -0.704999\n",
            "Test log-likelihood  -0.702403\n",
            "-----------\n",
            "Epoch  29\n",
            "Train accuracy  0.851917\n",
            "Test accuracy  0.8544\n",
            "Train log-likelihood  -0.695764\n",
            "Test log-likelihood  -0.694366\n",
            "-----------\n",
            "Epoch  30\n",
            "Train accuracy  0.853317\n",
            "Test accuracy  0.8559\n",
            "Train log-likelihood  -0.686831\n",
            "Test log-likelihood  -0.686205\n",
            "-----------\n",
            "Epoch  31\n",
            "Train accuracy  0.8546\n",
            "Test accuracy  0.8564\n",
            "Train log-likelihood  -0.678444\n",
            "Test log-likelihood  -0.678692\n",
            "-----------\n",
            "Epoch  32\n",
            "Train accuracy  0.856033\n",
            "Test accuracy  0.8578\n",
            "Train log-likelihood  -0.67028\n",
            "Test log-likelihood  -0.671305\n",
            "-----------\n",
            "Epoch  33\n",
            "Train accuracy  0.857767\n",
            "Test accuracy  0.8589\n",
            "Train log-likelihood  -0.662536\n",
            "Test log-likelihood  -0.664331\n",
            "-----------\n",
            "Epoch  34\n",
            "Train accuracy  0.859067\n",
            "Test accuracy  0.8603\n",
            "Train log-likelihood  -0.655191\n",
            "Test log-likelihood  -0.658078\n",
            "-----------\n",
            "Epoch  35\n",
            "Train accuracy  0.860583\n",
            "Test accuracy  0.8611\n",
            "Train log-likelihood  -0.648064\n",
            "Test log-likelihood  -0.651755\n",
            "-----------\n",
            "Epoch  36\n",
            "Train accuracy  0.861317\n",
            "Test accuracy  0.8612\n",
            "Train log-likelihood  -0.641211\n",
            "Test log-likelihood  -0.645354\n",
            "-----------\n",
            "Epoch  37\n",
            "Train accuracy  0.86265\n",
            "Test accuracy  0.8628\n",
            "Train log-likelihood  -0.634603\n",
            "Test log-likelihood  -0.639262\n",
            "-----------\n",
            "Epoch  38\n",
            "Train accuracy  0.863633\n",
            "Test accuracy  0.863\n",
            "Train log-likelihood  -0.628308\n",
            "Test log-likelihood  -0.633924\n",
            "-----------\n",
            "Epoch  39\n",
            "Train accuracy  0.864583\n",
            "Test accuracy  0.8645\n",
            "Train log-likelihood  -0.622146\n",
            "Test log-likelihood  -0.628345\n",
            "-----------\n",
            "Epoch  40\n",
            "Train accuracy  0.865333\n",
            "Test accuracy  0.8651\n",
            "Train log-likelihood  -0.616263\n",
            "Test log-likelihood  -0.623467\n",
            "-----------\n",
            "Epoch  41\n",
            "Train accuracy  0.8663\n",
            "Test accuracy  0.8665\n",
            "Train log-likelihood  -0.61057\n",
            "Test log-likelihood  -0.618593\n",
            "-----------\n",
            "Epoch  42\n",
            "Train accuracy  0.867083\n",
            "Test accuracy  0.8672\n",
            "Train log-likelihood  -0.605129\n",
            "Test log-likelihood  -0.61312\n",
            "-----------\n",
            "Epoch  43\n",
            "Train accuracy  0.8679\n",
            "Test accuracy  0.8683\n",
            "Train log-likelihood  -0.599753\n",
            "Test log-likelihood  -0.608523\n",
            "-----------\n",
            "Epoch  44\n",
            "Train accuracy  0.868833\n",
            "Test accuracy  0.8696\n",
            "Train log-likelihood  -0.59458\n",
            "Test log-likelihood  -0.604044\n",
            "-----------\n",
            "Epoch  45\n",
            "Train accuracy  0.869067\n",
            "Test accuracy  0.8704\n",
            "Train log-likelihood  -0.589592\n",
            "Test log-likelihood  -0.599488\n",
            "-----------\n",
            "Epoch  46\n",
            "Train accuracy  0.870033\n",
            "Test accuracy  0.8713\n",
            "Train log-likelihood  -0.58479\n",
            "Test log-likelihood  -0.595196\n",
            "-----------\n",
            "Epoch  47\n",
            "Train accuracy  0.870767\n",
            "Test accuracy  0.8716\n",
            "Train log-likelihood  -0.58012\n",
            "Test log-likelihood  -0.591032\n",
            "-----------\n",
            "Epoch  48\n",
            "Train accuracy  0.871617\n",
            "Test accuracy  0.8728\n",
            "Train log-likelihood  -0.575514\n",
            "Test log-likelihood  -0.586979\n",
            "-----------\n",
            "Epoch  49\n",
            "Train accuracy  0.872583\n",
            "Test accuracy  0.8728\n",
            "Train log-likelihood  -0.571079\n",
            "Test log-likelihood  -0.583078\n",
            "-----------\n",
            "Epoch  50\n",
            "Train accuracy  0.873083\n",
            "Test accuracy  0.8745\n",
            "Train log-likelihood  -0.566844\n",
            "Test log-likelihood  -0.579303\n",
            "-----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLOwSKOic8D-",
        "outputId": "35ad73e3-406d-46ef-fb8c-d77b138998f7"
      },
      "source": [
        "print(logistic_regression(train_images[0:1,]), y_train[0], \"\", sep=\"\\n\")\n",
        "evaluate_logistic_regression(train_images[0:1,:],y_train[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  3.8855178  -9.357399    2.5331233  12.701228  -15.107371   13.599654\n",
            "   -0.6550052   6.639809    3.9878988   0.5572613]], shape=(1, 10), dtype=float32)\n",
            "tf.Tensor(5, shape=(), dtype=int32)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
              " <tf.Tensor: shape=(), dtype=float32, numpy=-0.34238696>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOI9P5zElw60"
      },
      "source": [
        "## Alternative way of defining and training the model\n",
        "\n",
        "We can actually put the distribution inside the kera sequential model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSUkuCvDl1Fg"
      },
      "source": [
        "logistic_regression = tfk.Sequential(\n",
        "    [tfkl.InputLayer(input_shape=[28*28,]),\n",
        "     tfkl.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=1)),\n",
        "     #tpf.layers. <- instead of tfpl\n",
        "     tfpl.DistributionLambda(make_distribution_fn=lambda t: tfd.Categorical(logits = t), \n",
        "                             convert_to_tensor_fn=lambda s: s.mode())\n",
        "     ]\n",
        ")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5JQVpp2plzS"
      },
      "source": [
        "We now need to define a loss function than can handle probabilistic predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWRbmBo-oF9L",
        "outputId": "b4632580-0506-42f5-bd1c-2b2b5510ce17"
      },
      "source": [
        "logistic_regression(train_images[:10,:]).sample(2)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
              "array([[2, 1, 6, 2, 6, 2, 0, 2, 6, 2],\n",
              "       [2, 1, 6, 2, 6, 2, 0, 2, 4, 2]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_K1SERGntT6",
        "outputId": "917b9d4d-044a-4652-d8c2-0ace389b9c10"
      },
      "source": [
        "logistic_regression(train_images[:10,:]).mode()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([2, 1, 6, 2, 6, 2, 0, 2, 6, 2], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deI63P7rpITN"
      },
      "source": [
        "neg_loglikelihood = lambda y, pred: -pred.log_prob(y)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYUQdhl7qhSR"
      },
      "source": [
        "We now need to compile our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nL9uo54pZJZ"
      },
      "source": [
        "logistic_regression.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0005), \n",
        "                            loss = neg_loglikelihood,\n",
        "                            metrics = [tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNujrLuWrA2m",
        "outputId": "45bf1191-e4e9-45ee-f3de-7daeef3b3b3e"
      },
      "source": [
        "logistic_regression.fit(train_images, y_train, validation_data=(test_images, y_test), verbose=2, epochs=20)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 - 4s - loss: 8.0858 - categorical_accuracy: 0.0907 - val_loss: 6.1329 - val_categorical_accuracy: 0.0895\n",
            "Epoch 2/20\n",
            "1875/1875 - 3s - loss: 5.3605 - categorical_accuracy: 0.0779 - val_loss: 4.7647 - val_categorical_accuracy: 0.0607\n",
            "Epoch 3/20\n",
            "1875/1875 - 3s - loss: 4.3977 - categorical_accuracy: 0.0517 - val_loss: 4.0814 - val_categorical_accuracy: 0.0639\n",
            "Epoch 4/20\n",
            "1875/1875 - 3s - loss: 3.8886 - categorical_accuracy: 0.0747 - val_loss: 3.7035 - val_categorical_accuracy: 0.0863\n",
            "Epoch 5/20\n",
            "1875/1875 - 3s - loss: 3.5772 - categorical_accuracy: 0.0704 - val_loss: 3.4490 - val_categorical_accuracy: 0.0799\n",
            "Epoch 6/20\n",
            "1875/1875 - 3s - loss: 3.3622 - categorical_accuracy: 0.0736 - val_loss: 3.2664 - val_categorical_accuracy: 0.0703\n",
            "Epoch 7/20\n",
            "1875/1875 - 3s - loss: 3.2016 - categorical_accuracy: 0.0709 - val_loss: 3.1276 - val_categorical_accuracy: 0.0927\n",
            "Epoch 8/20\n",
            "1875/1875 - 3s - loss: 3.0802 - categorical_accuracy: 0.0624 - val_loss: 3.0231 - val_categorical_accuracy: 0.0927\n",
            "Epoch 9/20\n",
            "1875/1875 - 3s - loss: 2.9793 - categorical_accuracy: 0.0720 - val_loss: 2.9445 - val_categorical_accuracy: 0.0607\n",
            "Epoch 10/20\n",
            "1875/1875 - 3s - loss: 2.8999 - categorical_accuracy: 0.0699 - val_loss: 2.8599 - val_categorical_accuracy: 0.0735\n",
            "Epoch 11/20\n",
            "1875/1875 - 3s - loss: 2.8315 - categorical_accuracy: 0.0736 - val_loss: 2.8030 - val_categorical_accuracy: 0.0575\n",
            "Epoch 12/20\n",
            "1875/1875 - 3s - loss: 2.7725 - categorical_accuracy: 0.0731 - val_loss: 2.7455 - val_categorical_accuracy: 0.0895\n",
            "Epoch 13/20\n",
            "1875/1875 - 3s - loss: 2.7221 - categorical_accuracy: 0.0667 - val_loss: 2.7051 - val_categorical_accuracy: 0.0671\n",
            "Epoch 14/20\n",
            "1875/1875 - 3s - loss: 2.6804 - categorical_accuracy: 0.0789 - val_loss: 2.6630 - val_categorical_accuracy: 0.0543\n",
            "Epoch 15/20\n",
            "1875/1875 - 3s - loss: 2.6422 - categorical_accuracy: 0.0885 - val_loss: 2.6293 - val_categorical_accuracy: 0.0607\n",
            "Epoch 16/20\n",
            "1875/1875 - 3s - loss: 2.6083 - categorical_accuracy: 0.0843 - val_loss: 2.6000 - val_categorical_accuracy: 0.0799\n",
            "Epoch 17/20\n",
            "1875/1875 - 3s - loss: 2.5801 - categorical_accuracy: 0.0859 - val_loss: 2.5713 - val_categorical_accuracy: 0.0735\n",
            "Epoch 18/20\n",
            "1875/1875 - 3s - loss: 2.5551 - categorical_accuracy: 0.0752 - val_loss: 2.5476 - val_categorical_accuracy: 0.0863\n",
            "Epoch 19/20\n",
            "1875/1875 - 3s - loss: 2.5314 - categorical_accuracy: 0.0805 - val_loss: 2.5233 - val_categorical_accuracy: 0.0639\n",
            "Epoch 20/20\n",
            "1875/1875 - 3s - loss: 2.5113 - categorical_accuracy: 0.0800 - val_loss: 2.5099 - val_categorical_accuracy: 0.0735\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd94ceed590>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVstkS5asjc3"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWZZZwYgz3BK"
      },
      "source": [
        "**Question 2.** Compare the results of your logistic regression classifier with the ones given by scikit-learn's logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Ju1xtK0Cr8"
      },
      "source": [
        "**Question 3.** Replace the logistic regression model by a deep classifier of your choice (e.g. a MLP or a CNN). Try to beat logistic regression!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ51QqEavWL7",
        "outputId": "86c32e13-b434-484a-ad78-fac6a251cd25"
      },
      "source": [
        "deep_regression = tfk.Sequential(\n",
        "    [tfkl.InputLayer(input_shape=[28*28,]),\n",
        "     tfkl.Dense(100, kernel_initializer=initializers.RandomNormal(stddev=1), \n",
        "                activation=\"tanh\"),\n",
        "     tfkl.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=1)),\n",
        "     #tpf.layers. <- instead of tfpl\n",
        "     tfpl.DistributionLambda(make_distribution_fn=lambda t: tfd.Categorical(logits = t), \n",
        "                             convert_to_tensor_fn=lambda s: s.mode())\n",
        "     ]\n",
        ")\n",
        "\n",
        "deep_regression.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0005), \n",
        "                            loss = neg_loglikelihood,\n",
        "                            metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "deep_regression.fit(train_images, y_train, validation_data=(test_images, y_test), verbose=2, epochs=20)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 - 4s - loss: 7.7960 - categorical_accuracy: 0.0757 - val_loss: 5.1336 - val_categorical_accuracy: 0.0767\n",
            "Epoch 2/20\n",
            "1875/1875 - 4s - loss: 3.9364 - categorical_accuracy: 0.1019 - val_loss: 2.9891 - val_categorical_accuracy: 0.1214\n",
            "Epoch 3/20\n",
            "1875/1875 - 4s - loss: 2.6570 - categorical_accuracy: 0.1099 - val_loss: 2.4394 - val_categorical_accuracy: 0.0479\n",
            "Epoch 4/20\n",
            "1875/1875 - 4s - loss: 2.3772 - categorical_accuracy: 0.0811 - val_loss: 2.3326 - val_categorical_accuracy: 0.0543\n",
            "Epoch 5/20\n",
            "1875/1875 - 4s - loss: 2.3255 - categorical_accuracy: 0.0555 - val_loss: 2.3182 - val_categorical_accuracy: 0.0351\n",
            "Epoch 6/20\n",
            "1875/1875 - 4s - loss: 2.3145 - categorical_accuracy: 0.0827 - val_loss: 2.3092 - val_categorical_accuracy: 0.1278\n",
            "Epoch 7/20\n",
            "1875/1875 - 4s - loss: 2.3106 - categorical_accuracy: 0.0832 - val_loss: 2.3122 - val_categorical_accuracy: 0.0192\n",
            "Epoch 8/20\n",
            "1875/1875 - 4s - loss: 2.3094 - categorical_accuracy: 0.0885 - val_loss: 2.3115 - val_categorical_accuracy: 0.0990\n",
            "Epoch 9/20\n",
            "1875/1875 - 4s - loss: 2.3081 - categorical_accuracy: 0.0992 - val_loss: 2.3100 - val_categorical_accuracy: 0.1086\n",
            "Epoch 10/20\n",
            "1875/1875 - 4s - loss: 2.3079 - categorical_accuracy: 0.1077 - val_loss: 2.3101 - val_categorical_accuracy: 0.1470\n",
            "Epoch 11/20\n",
            "1875/1875 - 4s - loss: 2.3078 - categorical_accuracy: 0.1035 - val_loss: 2.3077 - val_categorical_accuracy: 0.1182\n",
            "Epoch 12/20\n",
            "1875/1875 - 4s - loss: 2.3076 - categorical_accuracy: 0.1093 - val_loss: 2.3074 - val_categorical_accuracy: 0.1150\n",
            "Epoch 13/20\n",
            "1875/1875 - 4s - loss: 2.3069 - categorical_accuracy: 0.1040 - val_loss: 2.3054 - val_categorical_accuracy: 0.2300\n",
            "Epoch 14/20\n",
            "1875/1875 - 4s - loss: 2.3066 - categorical_accuracy: 0.1088 - val_loss: 2.3093 - val_categorical_accuracy: 0.0703\n",
            "Epoch 15/20\n",
            "1875/1875 - 4s - loss: 2.3071 - categorical_accuracy: 0.1003 - val_loss: 2.3069 - val_categorical_accuracy: 0.1310\n",
            "Epoch 16/20\n",
            "1875/1875 - 4s - loss: 2.3076 - categorical_accuracy: 0.1104 - val_loss: 2.3039 - val_categorical_accuracy: 0.1150\n",
            "Epoch 17/20\n",
            "1875/1875 - 4s - loss: 2.3071 - categorical_accuracy: 0.1216 - val_loss: 2.3055 - val_categorical_accuracy: 0.2748\n",
            "Epoch 18/20\n",
            "1875/1875 - 4s - loss: 2.3064 - categorical_accuracy: 0.1173 - val_loss: 2.3156 - val_categorical_accuracy: 0.1246\n",
            "Epoch 19/20\n",
            "1875/1875 - 4s - loss: 2.3072 - categorical_accuracy: 0.1104 - val_loss: 2.3031 - val_categorical_accuracy: 0.2812\n",
            "Epoch 20/20\n",
            "1875/1875 - 4s - loss: 2.3066 - categorical_accuracy: 0.1205 - val_loss: 2.3091 - val_categorical_accuracy: 0.1150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd94c4bc810>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wNW2GcRvo4q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}