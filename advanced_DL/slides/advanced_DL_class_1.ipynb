{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7de2b8c",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning - Class 1\n",
    "## From Artificial Intelligence to a first neuron model\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8eafaa",
   "metadata": {},
   "source": [
    "## Context & Vocabulary\n",
    "\n",
    "<u></u>\n",
    "\n",
    "### What is Artificial Intelligence? \n",
    "\n",
    "<u>AI as a research field:</u>\n",
    "\n",
    "- The term **Intelligent Machinery** originated from an unpublished report by Alan Turing in **1948** where two approaches are distinguished:\n",
    "    - **top-down**, knowledge-driven AI\n",
    "    - **bottom-up**, data-driven AI\n",
    "- The term **Artificial Intelligence** was coined at a Dartmouth College conference in the **summer of 1956**\n",
    "- The concept has existed since Antiquity (e.g. Hephaestus' metal automatons, Jewish folklore's Golem, etc.)\n",
    "\n",
    "<u>**top-down**, knowledge-driven AI:</u>\n",
    "\n",
    "- Based on **cognition**, i.e. high-level phenomena independent of low-level details (e.g. implementaiton mechanism)\n",
    "    - examples: evolutionary algorithms (50s), knowledge representation, reasoning (50s-70s), expert systems (70s), logic, automata, intelligent agent systems (1990)\n",
    "- **hypothetical-deductive machines**\n",
    "![hd](images/hd.png)\n",
    "\n",
    "<u>**bottom-up**, data-driven AI</u>\n",
    "- Opposite approach that starts from the data to build incremental and mathematical decision-making mechanisms\n",
    "    - examples: First neuron (1943), first neural network machine (1950), neucognitron (1975), Decision Trees (1983), Backpropagation (1984-1986), Random Forest (1995), Support Vector Machine (1995), Boosting (1995), Deep Learning (1998/2006)\n",
    "- **inductive machines**\n",
    "![inductive](images/inductive.png)\n",
    "\n",
    "<u>Larry Tesler's Theorem, or the \"AI effect\":</u> \"AI is whatever hasnÂ´t been done yet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f02560",
   "metadata": {},
   "source": [
    "### Machine Learning without Math\n",
    "\n",
    "> Given inputs $X$ and a function $f$ with parameter(s) $\\alpha$, the goal is to predict outputs $y$ such that:\n",
    ">\n",
    "> $$(X)\\overset{f(x, \\alpha)}{\\rightarrow}(y)$$\n",
    "\n",
    "**$f$ is part of a family of function $\\{f(x, \\alpha)\\}$ where $\\alpha$ is a set of parameters**.\n",
    "\n",
    "$f$ can also be called an **oracle**. An oracle assigna a value $y$ to a vector $x$ following a probability distribution $\\mathbb{P}(y|x)$ which is also fixed, but unknown.\n",
    "\n",
    "$S$ is called a training set such that:\n",
    "> $S=\\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\\}$ \n",
    ">\n",
    "> $m$ training samples ***IID*** that follow the joint probability $\\mathbb{P}(x, y) = P(x)*P(y|x)$\n",
    "\n",
    "![mle](images/mle.png)\n",
    "\n",
    "<u>Problem of machine learning:</u>\n",
    "- The problem of ML consists in finding the right function $f$ among the family fo function $\\{f(x, \\alpha)\\}$ that provides the best approximation $\\hat{y}$ of the true label $y$ (in a classification case) given by the oracle\n",
    "- **best** is defined in terms of minimizing a specific *error, measure, cost, loss* that is **related to the problem/objective**\n",
    "\n",
    "<u>Minimizing Risk</u>\n",
    "- The objective of ML is to minimize the **(real) Risk**, i.e. the **expectation of the error cost**:\n",
    "$$\\mathcal{R}(\\alpha) = \\int L((x, y), \\alpha)\\,\\, d\\mathbb{P}(x, y)$$\n",
    "where $\\mathbb{P}(x, y)$ is unknown.\n",
    "\n",
    "The training set $S=\\{(x_m, y_m)\\}_{i=1,...,m}$ is built through an IID sampling according to $\\mathbb{P}(x, y)$. Since we cannot compute $\\mathcal{R}(\\alpha)$ realistically, we look to **minimize the empirical risk** instead:\n",
    "$$\\mathcal{R}_{empirical}(\\alpha) = \\frac{1}{m}\\underset{k=1}{\\overset{m}{\\sum}}L((x_i, y_i), \\alpha)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5eb52",
   "metadata": {},
   "source": [
    "### Machine Learning & Statistics\n",
    "\n",
    "<u>Take-away:</u>\n",
    "\n",
    "> $S=\\{(x_m, y_m)\\}_{i=1,...,m}$ is built through an IID sampling according to $\\mathbb{P}(x, y)$\n",
    ">\n",
    "> **ML $\\Leftrightarrow$ Statistics**:\n",
    "> - training through cross-validation\n",
    "> - Training and test sets have to be distributed according to the same law\n",
    "\n",
    "<u>Vapnik Learning Theory (1995):</u>\n",
    "\n",
    "\\begin{align}\n",
    "\\text{real risk} &= \\text{training error} + \\text{generalization error} \\\\\n",
    "R(\\alpha_m) &\\le R_{empirical}(\\alpha_m) + (b-a) * \\large\\textstyle\\sqrt{\\frac{d_{VC}(ln(2m/d_{VC})+1)-ln(\\eta/4)}{m}}\n",
    "\\end{align}\n",
    "\n",
    "Minimizing the Risk depends on **minimzing the Empirical Risk and the Generalization Error** of the model which depends on $m$ (the number of training samples), and $d_{VC}$ (the complexity of the model family chosen, also called Vapnik-Chervonenkis Dimension)\n",
    "\n",
    "<u>Vapnik-Chervonenkis dimension:</u>\n",
    "Also called **model capacity** in a classification case measures the number of realisable functions by the model when varying the parameters in all possible configurations.\n",
    "\n",
    ">  It is defined as the cardinality of the largest set of points that the algorithm can shatter [wiki](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186223e",
   "metadata": {},
   "source": [
    "## Mathematical Basics\n",
    "\n",
    "### Data representation\n",
    "\n",
    "Samples are represented by **vectors** and points in a **n-dimensional space** ($\\mathbb{R}^n$).\n",
    "\n",
    "### Operations on vectors in $\\mathbb{R}^n$\n",
    "\n",
    "1. **Multiplication by a scalar**:\n",
    "> given the vector $v = (v_1, ..., v_n)$ and scalar $c$\n",
    ">\n",
    "> *multiplication*: $c*v = (c*v_1, ..., c*v_n)$\n",
    "\n",
    "2. **Addition**:\n",
    "> given the vectors $v = (v_1, ..., v_n)$ and $u = (u_1, ..., u_n)$\n",
    ">\n",
    "> *addition*: $u + v = (u_1 + v_1, ..., u_n + v_n)$\n",
    "\n",
    "3. **Subtraction**:\n",
    "> given the vectors $v = (v_1, ..., v_n)$ and $u = (u_1, ..., u_n)$\n",
    ">\n",
    "> *subtraction*: $u - v = (u_1 - v_1, ..., u_n - v_n)$\n",
    "\n",
    "4. **Euclidian length or L2-norm**:\n",
    "\n",
    "The L2-norm is a typical way to measure the length of a vector.\n",
    "\n",
    "> given the vector $v = (v_1, ..., v_n)$\n",
    ">\n",
    "> *L2-norm*: $||v||_2 = \\textstyle\\sqrt{v_1^2 + ... + v_n^2}$\n",
    "\n",
    "5. **Dot Product**:\n",
    "> given the vectors $v = (v_1, ..., v_n)$ and $u = (u_1, ..., u_n)$\n",
    ">\n",
    "> *dot product*: $u . v = \\underset{i=1}{\\overset{n}{\\sum}}u_i * v_i = ||u||_2||v||_2cos\\theta$\n",
    "\n",
    "if $u$ and $v$ are perpendicular, then $u.v=0$. $u . u = ||y||_2^2$.\n",
    "\n",
    "**example**: classical regression equation $y = w . x + b$\n",
    "\n",
    "### Hyperplane\n",
    "\n",
    "A hyperplace is a $n-1$-dimensional linear decision surface that splits a $n$-dimensional space into two parts, e.g. binary classifier.\n",
    "\n",
    "### Derivative rules\n",
    "\n",
    "**Univariate case**\n",
    "\n",
    "> $(a.x)' = a$\n",
    ">\n",
    "> $(a.x + b)' = a$\n",
    ">\n",
    "> $(g(f(x)))' = g'(f(x)).f'(x)$ (chain rule)\n",
    "\n",
    "**Multivariate case**\n",
    "\n",
    "> $\\nabla_X(a.X) = a$\n",
    ">\n",
    "> $\\nabla_X(a.X + b) = a$\n",
    ">\n",
    "> $\\nabla_X(g(f(x))) = \\nabla_Xg(f(x)).\\nabla_Xf(x)$ (chain rule)\n",
    ">\n",
    ">$\\nabla_{x_i}(V.X) = \\nabla_{x_i}(v_1.x_1 + ... + v_n.x_n) = v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438eeb1d",
   "metadata": {},
   "source": [
    "## Simple Models\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "If the **biological neuron** is **spike-based** (i.e. performing gradient descent is exceptionally hard), the **artificial neuron** is based on an activation/decision function (**rate-based description/steady regime**) that allows gradient descent.\n",
    "\n",
    "$$y = s(\\sum w_i x_i)$$\n",
    "\n",
    "### A single artificial neuron\n",
    "\n",
    "![neuron](images/neuron.png)\n",
    "\n",
    "- each input $x$ has an associated weight $w$ that can be modified\n",
    "- inputs $x$ corresponds to signals from other neuron axons\n",
    "- $x_0$ are biases (special inputs) with weight $w_0$\n",
    "\n",
    "The weights corresponds to **synaptic modulation**, the summation to **cell body**, the activation function to **axon hillock**, and the output to **axon signal**.\n",
    "\n",
    "<u>Perceptron algorithm:</u>\n",
    "\n",
    "1. Pick initial weight vector $w$ (including $w_0$)\n",
    "2. Repeat **until all points are correctly classified**. Repeat for each point:\n",
    "    - compute $y_i\\,.\\,w\\,.\\,x_i$ for point $i$\n",
    "    - if $y_i\\,.\\,w\\,.\\,x_i\\gt0$:\n",
    "        - the point is correctly classified\n",
    "    - else:\n",
    "        - update the weights to increase the value of $y_i\\,.\\,w\\,.\\,x_i$, proportionally to $y_i\\,.\\,x_i$\n",
    "        \n",
    "Each change of $w$ decreases the error on a specific point. However, changes for several points are correlated, that is different points could change the weights in opposite directions. Thus, this iterative algorithm requires several loops to converge. \n",
    "\n",
    "**It is mathematically guaranteed to find a separating hyperplane if one exists** (if data is linearly separable). If data are not linearly separable, then this algorithm loops indefinitely.\n",
    "       \n",
    "<u>Gradient Ascent:</u>\n",
    "\n",
    "**Question**: *Why pick $y_i\\,.\\,x_i$ as increment to weights?*\n",
    "\n",
    "**Univariate case**:\n",
    "\n",
    "The goal is to **maximize the scalar function of one variable $f(w)$**.\n",
    "\n",
    "- pick initial $w$\n",
    "- change $w$ to $w+\\eta \\frac{df}{dw}$ with ($\\eta\\gt0$, small)\n",
    "- until $f$ stops changing (i.e. $\\frac{df}{dw}\\approx0$)\n",
    "\n",
    "**Multivariate case**:\n",
    "\n",
    "- pick initial $w$\n",
    "- change $w$ to $w+\\eta \\nabla f_W$ with ($\\eta\\gt0$, small)\n",
    "- until $f$ stops changing (i.e. $ \\nabla f_W\\approx0$)\n",
    "\n",
    "We find the local maximum, unless the function is globally convex such that:\n",
    "\n",
    "$$\\nabla f_w = [\\frac{\\delta f}{\\delta w_1}, ..., \\frac{\\delta f}{\\delta w_n}]$$\n",
    "\n",
    "If **f is non-linear**, the learning rate $\\eta$ has to be chosen very carefully as:\n",
    "- too small: slow convergence\n",
    "- too big: overshoot and oscillation\n",
    "\n",
    "**In general**\n",
    "\n",
    "The goal is to maximize the margin of misclassified points. \n",
    "\n",
    "- **Off-line training**: Compute, at each iteration, the gradient as sum over all training points\n",
    "- **On-line training**: Approximate gradient by one of the terms in the sum: $y_i\\,.\\,x_i$ (principlpe of the Stochastic Gradient Descent, SGD)\n",
    "\n",
    "**XOR is an example of non-linearly separable function (by one perceptron)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d08a6",
   "metadata": {},
   "source": [
    "## From simple to complex models\n",
    "\n",
    "Multi-layer perceptrons offer **manifold disentanglement** capabilities.\n",
    "\n",
    "### Deep Learning Principles\n",
    "\n",
    "1. <span style=\"color:red\"><u>**Cybenko (1989), Hornik, Stinchcombe & White (1989) Theorem**:</u></span>\n",
    "    \n",
    "<span style=\"color:red\">A neural network with one single hidden layer is a **universal approximator**. It can represent any continuous function on compact subsets of $\\mathbb{R}^2$. 2 layers are enough but hidden layer size may be exponential for error $\\epsilon$ (or even infinite for error $0$). There is no efficient learning rule known.</span>\n",
    "\n",
    "2. <span style=\"color:red\"><u>**Hastad (1986), Bengio (2007) Theorem**:</u></span>\n",
    "    \n",
    "<span style=\"color:red\">Functions representable compactly with $k$ layers may require exponential size with $k-1$ layers.</span>\n",
    "\n",
    "3. <span style=\"color:red\"><u>**Cover's (1965) Theorem**:</u></span>\n",
    "    \n",
    "<span style=\"color:red\">A complex pattern-classification problem cast in a high-dimensional space non-linearly is more likely to be linearly sperable than in a low-dimensional space</span> (repeated sequence of Bernouilli trials). \n",
    "\n",
    "The number of groupings that can be formed by $(l-1)$-dimensional hyperplanes to separate $N$ points in two classes is:\n",
    "$$O(N, l) = 2\\underset{i=0}{\\overset{l}{\\sum}}\\frac{(N-1)!}{(N-l-i)!i!}$$\n",
    "\n",
    "4. <span style=\"color:red\"><u>**Curse of dimensionality** (**Bellman's Theorem**, 1956):</u></span>\n",
    "    \n",
    "<span style=\"color:red\">Euclidian distance looses relevant in high dimension</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfefa5c",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "- Trial and error as well as intuition often inform the choices in layer and neuron numbers\n",
    "- Specific network structures can be designed\n",
    "- Structure can be automatically determined (area of research: AutoML, AdaNet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
