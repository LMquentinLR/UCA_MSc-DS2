{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7de2b8c",
   "metadata": {},
   "source": [
    "# Advanced Deep Learning - Class 2\n",
    "## Convolutional Neural Networks and Optimization Tricks\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac35eb3",
   "metadata": {},
   "source": [
    "**Driving question**: can a network structure reduce the exploration space while also providing useful properties: invariance, robustness, etc.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79be2dc",
   "metadata": {},
   "source": [
    "## Invariance to spatial changes\n",
    "> convolutional neural network aka CNN, ConvNet\n",
    "\n",
    "In a convolutional neural networks, filters (which emulate simple or complex biological cortex functions) are learnable neurons. Computing via filter is intensively used in image processing and games.\n",
    "\n",
    "### Convolution in nature\n",
    "\n",
    "Based on **Hubel and Wiesel**'s work in 1962, the cortex process follows:\n",
    "\n",
    "- Convolution: resulting in the feature map\n",
    "- Activation\n",
    "- Pooling\n",
    "\n",
    "### Deep representation by CNN\n",
    "\n",
    "**Yann LeCun** (1998) showed that:\n",
    "\n",
    "- subpart of the field of vision are translation invariant\n",
    "- S-cells: convolution with filters\n",
    "- C-cells: max-pooling\n",
    "\n",
    "A **feature map** is the result of a convolution (but before the application of the activation function). Convolution is a **process with a filter that extracts parallelized characteristics at each layer** (e.g. edges)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2830bd",
   "metadata": {},
   "source": [
    "## Optimization, tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a41664",
   "metadata": {},
   "source": [
    "### Pre-processing steps\n",
    "\n",
    "Pre-processing steps are important in order to prep a machine learning process:\n",
    "\n",
    "1. **Train/Validation/Test splits** (Validation is used to tune hyperparameters) are mandatory.\n",
    "\n",
    "2. **Features must be normalized** ($W\\approx\\mathcal{N}(0,1)$) in order to have features at the same scale.\n",
    "\n",
    "3. If the data is highly dimensional, consider using a **dimensionality reduction** technique such as PCA\n",
    "\n",
    "4. **Data augmentation** ca be performed if the dataset is small (e.g. CV, horizontal flipping, random crops and color jitteriing, NLP: synonym substitution)\n",
    "\n",
    "### Softmax layer\n",
    "\n",
    "Multi-class classification models rely on a softmax activation function at the last layer. \n",
    "\n",
    "> Softmax is **monotonical** and **non-local**\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "1. <u>Sigmoid</u>\n",
    "    - **Pros**: \n",
    "        - $\\alpha(x)\\in[0,1]$, which is useful at the output layer\n",
    "        - derivation is easily computable: $\\alpha(x)(1-\\alpha(x))$\n",
    "    - **Cons**:\n",
    "        - Saturates when receiving strong signals\n",
    "        - Derivatives at 0 on both ends, causing gradients in previous layers to go towards 0 (diminishing gradient)\n",
    "        - exploding gradient problem (can be solved with gradient clipping)\n",
    "        \n",
    "2. <u>tanh</u>\n",
    "    - **Pros**: \n",
    "        - $\\alpha(x)$ is centered around 0\n",
    "        - derivation is easily computable: $1-\\alpha^2(x)$\n",
    "        - converges faster than Sigmoid\n",
    "    - **Cons**:\n",
    "        - Saturating, vanishing, or exploding gradient problems\n",
    "        \n",
    "### Loss Function $L$\n",
    "\n",
    "**Total Cost**: $C(W) = \\underset{r=1}{\\overset{R}{\\sum}}L^r(W) \\Leftarrow$ *Find the network parameters $W^*$ that minimizes this value*\n",
    "\n",
    "1. <u>Classification:</u>\n",
    "    - **Cross-Entropy**: $$L(x) = -y\\,.\\,ln(f(x) + (1+y)\\,.\\,ln(1-f(x))$$\n",
    "    - **Hinge-Loss** (max-magin loss, 0-1 loss): $$L(x) = max(0, 1-y\\,.\\,f(x))$$\n",
    "\n",
    "2. <u>Regression:</u>\n",
    "    - **Mean square loss** (or Quadratic Loss): $$L(x) = (f(x)-y)^2$$\n",
    "    - **Mean Absolute Loss**\n",
    "\n",
    "3. <u>Retrieval:</u>\n",
    "    - Triplet loss\n",
    "    - Cosine similarity\n",
    "    \n",
    "> If the loss is minimized but accuracy is low: **check the loss function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b985473",
   "metadata": {},
   "source": [
    "### Overfitting and underfitting\n",
    "\n",
    "Y. Bengio: \"*Check if the model is powerful enough to overfit, if not then change model structure or make model larget.*\"\n",
    "\n",
    "![baby](images/babysit.png)\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "1. Randomly pick a starting point $W^0$\n",
    "2. Compute the negative gradient at $W^0$: $-\\nabla C(W^0)$ s.t.:\n",
    "![error](images/error_surface.png)\n",
    "3. Update the weights with a learning rate $\\eta$\n",
    "4. Repeat until a minima is reached\n",
    "\n",
    "**Gradient descent risks**: plateau, saddle points, local minima.\n",
    "\n",
    "![vanilla_sgd](images/vanilla_SGD.png)\n",
    "\n",
    "![momentum](images/momentum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4945751",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "1. **All zero initialization**: Not good when the network is deep, every neuron computes the same output, have the same gradients during back-propagation\n",
    "\n",
    "2. **Glorot normal/uniform**: zero-mean gaussian intialization with a variance scaled by number of input neurons and number of output neurons. The goal is to keep the signal in a resonable range of values through many layers $$W\\approx\\mathcal{N}(0, 2/(input\\,\\,neurons+output\\,\\,neurons))$$\n",
    "\n",
    "3. **He normal/uniform**: zero-mean gaussian intializaiton scaled by number of input neurons $$W\\approx\\mathcal{N}(0, 1/input\\,\\,neurons)$$\n",
    "\n",
    "### Train by Mini-Batches of training Samples\n",
    "\n",
    "*Usually works faster and better than standard SGD*.\n",
    "\n",
    "1. randomly intialize $W^0$\n",
    "2. Pick the first batch and update the weights: $W^1 \\leftarrow W^0 - \\eta\\nabla C(W^0)$\n",
    "3. Pick the second batch and update the weights: $W^2 \\leftarrow W^1 - \\eta\\nabla C(W^1)$\n",
    "4. once all batches are done, an epoch is done, and the process is repeated\n",
    "\n",
    "### Dealing with internal covariate shift\n",
    "\n",
    "**Internal covariate shift** corresponds to the change in the distribution of activations owing to parameter updates that might slow learning.\n",
    "\n",
    "This can be dealt with via **batch-normalization** layers. It helps provide:\n",
    "\n",
    "- Faster learning\n",
    "- Increased accuracy \n",
    "- the possibility for a higher learning rate\n",
    "- some preventive measure against bad initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283067e",
   "metadata": {},
   "source": [
    "### Recipes when poor performance on training data\n",
    "\n",
    "1. **Modify the network**: new activation functions (ReLU, LeakyReLU)\n",
    "\n",
    "2. **Better optimization strategy**: Adaptive learning rates\n",
    "\n",
    "### Recipes when poor performance on validation data\n",
    "\n",
    "3. **preventing overfitting**: dropout\n",
    "\n",
    "4. **Regularization**\n",
    "\n",
    "5. **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb3213",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem\n",
    "\n",
    "Early layers have smaller gradients, learn very slowly, and are almost random. Meanwhile, deeper layers have larger gradients, learn quickly and converges. There is an amount of randomness. \n",
    "\n",
    "<u>Solutions:</u>\n",
    "- Rectified Linear Unit (ReLU): fast to compute, biological reason, infinite signoid with different biases, vanishing gradient problem. \n",
    "    - setting nodes to 0 when a result is negative is equivalent to having a thinner linear network.\n",
    "- Leaky ReLU\n",
    "- Parametric ReLU\n",
    "\n",
    "### Learning Rates\n",
    "\n",
    "**Idea**: Reducing the learning rate by some factor at each epoch. E.g. $\\frac{1}{t},\\,\\,\\eta^t=\\eta/\\sqrt{t+1}$\n",
    "\n",
    "0. <u>Original Gradient Descent:</u> $W^t \\leftarrow W^{t-1}-\\eta\\,.\\,\\nabla C(W^{t-1})$\n",
    "\n",
    "1. <u>AdaGrad:</u> $W^t \\leftarrow W^{t-1}-\\eta_W\\,.\\,g^t$, with $g^t=\\frac{\\delta C(W^{t})}{\\delta W}$ and $\\eta_W = \\frac{\\eta}{\\sqrt{\\sum^t_{i=0}(g^i)^2}}$\n",
    "- parameter dependent learning rate ($\\eta_W$)\n",
    "- summation of the square of the previous derivatives\n",
    "\n",
    "2. <u>RMSProp:</u> Root Mean Square of the gradients with previous gradients being decayed\n",
    "\\begin{align}\n",
    "W^1 &\\leftarrow W^0 - \\frac{\\eta}{\\sigma^0}g^0,\\text{ $\\sigma^0=g^0$}\\\\\n",
    "W^{t+1} &\\leftarrow W^t - \\frac{\\eta}{\\sigma^t}g^t,\\text{ $\\sigma^t=\\sqrt{\\alpha(\\sigma^{t-1})^2 + (1-\\alpha)(g^t)^2}$}\\\\\n",
    "\\end{align}\n",
    "\n",
    "3. <u>Adam</u>\n",
    "![adam](images/adam.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030e7a9e",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is an add-on to the loss function. The new loss function must not only minimize the original cost but also the regularization term.\n",
    "\n",
    "1. <u>L2 Regularization:</u> $$L'(W) = L(W) + \\lambda\\frac{1}{2}||W||_2$$\n",
    "$$\\frac{\\delta L'}{\\delta W} = \\frac{\\delta L}{\\delta W} + \\lambda W$$\n",
    "$$W^{t+1}\\rightarrow (1-\\eta\\lambda) W^t - \\eta\\frac{\\delta L}{\\delta W}$$\n",
    "Also known as **ridge**, or weight decay, it is the most widely used regularization method.\n",
    "\n",
    "2. <u>L1 Regularization:</u>\n",
    "Also known as **lasso**, produces sparse results: $$L'(W) = L(W) + \\lambda\\frac{1}{2}|W|$$\n",
    "$$\\frac{\\delta L'}{\\delta W} = \\frac{\\delta L}{\\delta W} + \\lambda sgn(W)$$\n",
    "$$W^{t+1}\\rightarrow W^t - \\eta\\frac{\\delta L}{\\delta W}-\\eta\\lambda sgn(W^T)$$\n",
    "\n",
    "3. <u>Elastic Net:</u> combines L1 and L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d37ce4",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Each neuron has a dropout percentage. Dropout changes the network shape.\n",
    "\n",
    "During validation/testing, the weights are multiplied by the dropout rate.\n",
    "\n",
    "**Dropout ends up being a kind of ensemble technique where parallel networks are 'averaged'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1027712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
