{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5466cf",
   "metadata": {},
   "source": [
    "# Machine Learning Tools in Action\n",
    "\n",
    "<hr>\n",
    "\n",
    "Talk given by Gennady Pekhimenko, UoT.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769e1b9",
   "metadata": {},
   "source": [
    "## 1 - Preliminary\n",
    "\n",
    "There is a dependency circle in machine learning that drives analysis and optimization.\n",
    "- Performance bottlenecks in DNN training\n",
    "- Diverse benchmark suite with state-of-the-art models\n",
    "- Key performance metrics\n",
    "- Tools\n",
    "\n",
    "The talk focuses on the latter: **tooling**.\n",
    "\n",
    "### Feature maps\n",
    "\n",
    "Feature maps remain still more important than weights for memory consumption. To assess the memory consumption of a model, new tools have been created:\n",
    "\n",
    "### Skyline\n",
    "\n",
    "Skyline is an interactive in-editor performance Visualization and Debugging for DNN training. It is motivated by the exploratory need to find why a model runs slow\n",
    "\n",
    "**Key features**:\n",
    "- key performance metrics (throughput, memory usage)\n",
    "- Iteration run time and memory footprint breakdowns\n",
    "- Interactive visualizations linked to batch size predictions\n",
    "- Live and proactive performance debugging during development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4532e41",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2 - The Problem\n",
    "\n",
    "Many GPUs are available for deep neural networks training. Each has a different cost and performance. AS such, a user should ponder what to choose for training.\n",
    "\n",
    "**Key observations**:\n",
    "\n",
    "- DNN training computation is highly repetitive\n",
    "- Predicting a GPU's training performance by predicting the execution time of a single iteration.\n",
    "\n",
    "\n",
    "**The work presented**:\n",
    "\n",
    "- Use an existing GPU to predict execution times on a different GPU using wave scaling and pre-trained MLPs\n",
    "- Implement ideas in a new tool called Habitat\n",
    "- Show two case studies where HAbitat leads users to the correct GPU choice..\n",
    "\n",
    "### Key observations\n",
    "\n",
    "- Deep learning users may already have an existing GPU\n",
    "- DNN training is a repetitive process\n",
    "- Use existing GPU to make iteration execution time predictions for other GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad501ed3",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3 - Habitat: A runtine-base performance predictor\n",
    "\n",
    "### Process\n",
    "\n",
    "1. Profile all operations in a training iteartion on an existing GPU\n",
    "2. Predict each oepration using wave scaling or a MLP\n",
    "\n",
    "![hw](images/habitatworkflow.png)\n",
    "\n",
    "### Behind the hood\n",
    "\n",
    "GPU kernels are entites to which the programmer divide work into thread blocks (Same code, different data). Streaming Multiprocessors run a finite number of blocks concurrently. And blocks round-robin scheduled onto the SMs. As such GPU kernels execute in \"**wave**\" of thread blocks.\n",
    "\n",
    "### Wave Scaling\n",
    "\n",
    "Wave scaling predicts a kernel execution of a GPU on another GPU.\n",
    "\n",
    "GPU can enjoy different types of scaling factors:\n",
    "- Memory bandwidth\n",
    "- Wave size\n",
    "- Clock frequency\n",
    "\n",
    "### One wrinkle\n",
    "\n",
    "- Wave scaling assumes the same kernel is used across GPUs\n",
    "- A few DNN operations use architecture-specific kernels (kernel variation): Convolutions, linear (dense) layers, LSTMS\n",
    "- Habitat uses pre-trained MLPs for these operations\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "How accurate are Habitat's predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd6cd6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4 - Habitat's performance\n",
    "\n",
    "### How accurate is Habitat\n",
    "\n",
    "This is performed by predicting iteration execution time on a GPU. Habitat makes accurate predictions with an average error of **11.8%** across all configurations (30GPU pairs x 5 models x 3 batch sizes).\n",
    "\n",
    "### Scenarios answered\n",
    "\n",
    "- **case 1:** A person wants to train a GNMET and have access to a P4000. Which cloud GPU to use, if any?\n",
    "    - Habitat correctly predicts that the V100 is the best choice for performance\n",
    "    - Habitat correctly predicts that the T4 is the best choice for cost\n",
    "\n",
    "### Why we should not always use the best GPUs\n",
    "\n",
    "- **case 1:** A person wants to train a DCGAN and have access to a 2080Ti. HAbitat correctly predicts that the V100 only offer a marginal improvement on the 2080Ti.\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- DNN computation is special (repetitive), enabling new analysis opportunities\n",
    "- Use runtime-based information to make iteration execution time predictions\n",
    "- Habitat leads to the correct decision in the case studies\n",
    "- The hardware landscape is growing, users need help choosing effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278854d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5 - Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training\n",
    "\n",
    "Benefits of proposed DNN optimizations are note fully exploited because:\n",
    "- efficacy varies for different HW/SW configurations\n",
    "- It is onerous to implement optimizations\n",
    "\n",
    "Daydream efficiently explores the efficacy of various DNN optimization using **Dependency graph analysis**:\n",
    "- Tracking dependencies at the abstraction of GPU kernels\n",
    "- Kernel-to-layer mapping\n",
    "- Transformation rules to model a diverse set of optimizations\n",
    "\n",
    "The evaluation relies on the low estimation error on 5 optimizations, 5 DNN modesl across 3 applications.\n",
    "\n",
    "### Advances in ML full stack research\n",
    "\n",
    "- DNN compute requirements are growing exponentially\n",
    "- Rapid advances in algorithms, systems optimizations & hardware architectures\n",
    "\n",
    "> It is hard for a ML programmer to identify the efficacy of new algorithms, optimizations, and hardware improvements in their deployments\n",
    "\n",
    "### Why dependency analysis\n",
    "\n",
    "A DNN computational graph is usually quite efficient to represent a model. But it leads to some challenges.\n",
    "\n",
    "![daydream](images/daydream.png)\n",
    "\n",
    "#### Challenges for Dependency Graph Analysis in the ML context\n",
    "\n",
    "1. Thousands of tasks, and dependency needs to be tracked across CPU threads, GPU streams and interconnects\n",
    "\n",
    "2. Some optimizaitons operate on the kernel-level granularity. Others operate on layer-level granularity. How should one correlate low-level traces with DNN topology?\n",
    "\n",
    "3. Ability to easily model diverse DNN optimizations\n",
    "\n",
    "### Daydream Methodology\n",
    "\n",
    "Daydream is evaluated on image classification (VGG16, DenseNet-121, ResNet50), machine translation (GNMT [seq2seq]) and language modeling (BERT). Each model is evaluated for each optimization X on a benchmark Y.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Daydream is apparently the first system that aims at estimating efficacy of optimizations for DNN training.\n",
    "\n",
    "Daydream uses:\n",
    "- Dependency graph analysis based on the kernel-level granularity\n",
    "- Sync-Free kernel-to-layer mapping\n",
    "- Graph transformation rules\n",
    "\n",
    "Daydream is able to accurately estimate the efficacy of optimizations acrss a wide range of DNN optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56a98a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6 - RL-Scope: cross-stack profiling for reinforcement learning\n",
    "\n",
    "### Deep reinforcement learning progress\n",
    "\n",
    "#### Existing problems, limitations\n",
    "\n",
    "- Training time limits progress (it is unsupervised learning). \n",
    "- RL workloads is different from supervised learning workloads\n",
    "- Profiling tools are designed for GPU-bound workloads (reinformcement learning uses a lot of bandwidth between CPU and GPU).\n",
    "\n",
    "#### Different between supervised and reinforcement learning\n",
    "\n",
    "![diffslrl](images/diffslrl.png)\n",
    "\n",
    "### RL-Scope: Cross-stack RL profiling\n",
    "\n",
    "![rlalgo](images/rlalgo.png)\n",
    "\n",
    "![rlswstack](images/rlswstack.png)\n",
    "\n",
    "#### RL-scope profiler features\n",
    "\n",
    "- Cross-stack scoping: corss stack view of where CPU and GPU time is spent\n",
    "- Cross-framework: Works with TF, Torch, etc.\n",
    "- Corrects for profiling overhead: Correct CPU overhead for accurate insights\n",
    "\n",
    "#### Contributions\n",
    "\n",
    "RL-scope profiler\n",
    "RL workload survey\n",
    "\n",
    "#### From the user perspective\n",
    "\n",
    "Apply the annotations\n",
    "\n",
    "```python\n",
    "# ML scripting training loop\n",
    "from t in range(num_timesteps):\n",
    "    with rls.operation(\"simulation\"):\n",
    "        # simulation code\n",
    "    with rls.operation(\"inference\"):\n",
    "        # inference code\n",
    "    with rls.operation(\"backprop\"):\n",
    "        # backprop code\n",
    "```\n",
    "\n",
    "Developer annotations help users tie profiler output to high-level code.\n",
    "\n",
    "![rltakeaway](images/rltakeaway.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
