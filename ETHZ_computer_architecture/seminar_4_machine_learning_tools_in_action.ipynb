{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d5466cf",
   "metadata": {},
   "source": [
    "# Machine Learning Tools in Action\n",
    "\n",
    "<hr>\n",
    "\n",
    "Talk given by Gennady Pekhimenko, UoT.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769e1b9",
   "metadata": {},
   "source": [
    "## 1 - Preliminary\n",
    "\n",
    "There is a dependency circle in machine learning that drives analysis and optimization.\n",
    "- Performance bottlenecks in DNN training\n",
    "- Diverse benchmark suite with state-of-the-art models\n",
    "- Key performance metrics\n",
    "- Tools\n",
    "\n",
    "The talk focuses on the latter: **tooling**.\n",
    "\n",
    "### Feature maps\n",
    "\n",
    "Feature maps remain still more important than weights for memory consumption. To assess the memory consumption of a model, new tools have been created:\n",
    "\n",
    "### Skyline\n",
    "\n",
    "Skyline is an interactive in-editor performance Visualization and Debugging for DNN training. It is motivated by the exploratory need to find why a model runs slow\n",
    "\n",
    "**Key features**:\n",
    "- key performance metrics (throughput, memory usage)\n",
    "- Iteration run time and memory footprint breakdowns\n",
    "- Interactive visualizations linked to batch size predictions\n",
    "- Live and proactive performance debugging during development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4532e41",
   "metadata": {},
   "source": [
    "## 2 - The Problem\n",
    "\n",
    "Many GPUs are available for deep neural networks training. Each has a different cost and performance. AS such, a user should ponder what to choose for training.\n",
    "\n",
    "**Key observations**:\n",
    "\n",
    "- DNN training computation is highly repetitive\n",
    "- Predicting a GPU's training performance by predicting the execution time of a single iteration.\n",
    "\n",
    "\n",
    "**The work presented**:\n",
    "\n",
    "- Use an existing GPU to predict execution times on a different GPU using wave scaling and pre-trained MLPs\n",
    "- Implement ideas in a new tool called Habitat\n",
    "- Show two case studies where HAbitat leads users to the correct GPU choice..\n",
    "\n",
    "### Key observations\n",
    "\n",
    "- Deep learning users may already have an existing GPU\n",
    "- DNN training is a repetitive process\n",
    "- Use existing GPU to make iteration execution time predictions for other GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad501ed3",
   "metadata": {},
   "source": [
    "## 3 - Habitat: A runtine-base performance predictor\n",
    "\n",
    "### Process\n",
    "\n",
    "1. Profile all operations in a training iteartion on an existing GPU\n",
    "2. Predict each oepration using wave scaling or a MLP\n",
    "\n",
    "![hw](images/habitatworkflow.png)\n",
    "\n",
    "### Behind the hood\n",
    "\n",
    "GPU kernels are entites to which the programmer divide work into thread blocks (Same code, different data). Streaming Multiprocessors run a finite number of blocks concurrently. And blocks round-robin scheduled onto the SMs. As such GPU kernels execute in \"**wave**\" of thread blocks.\n",
    "\n",
    "### Wave Scaling\n",
    "\n",
    "start at 19:00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd6cd6",
   "metadata": {},
   "source": [
    "## 4 - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7278854d",
   "metadata": {},
   "source": [
    "## 5 - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
