{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b3d235",
   "metadata": {},
   "source": [
    "# Lecture 9: Real PIM Systems: UPMEM\n",
    "*Notes*\n",
    "\n",
    "**PIM**: Processing in Memory using PIM-enabled instructions.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbb548",
   "metadata": {},
   "source": [
    "## 1 - Main Idea\n",
    "\n",
    "Data movement between memory and storage units and compute units is a major contributor to execution time and energy consumption.\n",
    "\n",
    "Processing-in-Memory is a paradigm that can tackle the datamovement bottleneck (Though explored for 50 years, technology challegnes prevented the successful materialization).\n",
    "\n",
    "UPMEM has designed and fabricated the first publicly-available real-world PIM architecture (DDR4 chips embedding in-order multi-hreaded DRAM Processing Units, DPU).\n",
    "\n",
    "**Takeaway:**\n",
    "\n",
    "UPMEM-based PIM systems outperform state-fo-the-art CPUs in terms of performance and energy efficiency on most of PrIM benchmarks. They also outperforms state-of-the-art GPU on a majority of PrIM benchmarks. PIM systems are more energy efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06a687",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2 - UPMEM PIM Programming\n",
    "\n",
    "![obs](images/obs.png)\n",
    "\n",
    "UPMEM DIMMs coexist with conventional DIMMs. Integration of UPMEM DIMMs in a system follows an **accelerator model**. Il resembles GPU computing (explicit data movement between the main processor and the accelerator. Explicit kernel launch onto the UPMEM processors). \n",
    "\n",
    "### Vector Addition\n",
    "\n",
    "Array programming to allow high parallelism without requesting data throughput to the CPU.\n",
    "\n",
    "### CPU-DPU Data Transfers\n",
    "\n",
    "Serial, parallel and broadcast transfers (single DPU, multiple DPU, multiple DPU with a single buffer). \n",
    "\n",
    "### Inter-DPU COmmunication\n",
    "\n",
    "There is no direct communication channel between DPUs (they have to go through the host CPU). \n",
    "\n",
    "### CPU-DPU/DPU-CPU Transfer Bandwidth\n",
    "\n",
    "Data transfer size varies between 8 bytes and 32 MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653f44c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3 - DRAM Processing Unit\n",
    "\n",
    "### Arithmetic Throughput\n",
    "\n",
    "Arithmetic Throughput vs. Operational Intensity (goal is to characterize memory-bound regions and compute-bound regions). \n",
    "\n",
    "Microbenchmark (1. load one chunk of an MRAM array into WRAM, 2. Perform a variable number of operations on the data, 3. write back to MRAM). \n",
    "\n",
    "**Operational Intensity** is defined as ther number of arithmetic operations performed per byte accessed from MRAM.\n",
    "\n",
    "> In a memory-bound region, the arithmetic throughput increases with the operational intensity. In the compute-bound region, the arithmetic throughput is flat at its maximum. The throughput saturation point is the operational intensity where the transition between the memory-bound region and the compute-bound region happens.\n",
    ">\n",
    "> The thorughput saturation point is as low as $\\frac{1}{4}$ Operation/byte, i.e. 1 integer addition per every 32-bit element fetched.\n",
    "\n",
    "### WRAM and MRAM bandwidth\n",
    "\n",
    "#### WRAM Copy\n",
    "\n",
    "$$\\text{WRAM badwidth in }\\frac{B}{S}=\\frac{\\text{Bytes} * \\text{frequency}_{DPU}}{\\text{# instructions}}$$\n",
    "\n",
    "**Copy** executes 2 instructions (Load and store) with 11x16 bytes in 22 cycles -> $2800\\frac{MB}{s}$ at 350Mhz.\n",
    "\n",
    "#### MRAM Read and Write\n",
    "\n",
    "$$\\text{MRAM Bandwidth in }\\frac{B}{S}=\\frac{\\text{size} * \\text{frequency}_{DPU}}{\\text{MRAM latency}}$$\n",
    "\n",
    "We can model the MRAM latency with a linear expression: MRAM LAtency (in cycles) = $\\alpha + \\beta*\\text{size}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85b3d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4 - PrIM Benchmarks\n",
    "\n",
    "A common set of workloads that can be used to evaluate the UPMEM PIM architecture, compare software improvements and compilers, compare future PIM architectures and hardware.\n",
    "\n",
    "Two key selection criteria: 1. selected workloads from different application domains, 2. memory-bound workloads on processor-centric architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
