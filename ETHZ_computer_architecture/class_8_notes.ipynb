{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b3d235",
   "metadata": {},
   "source": [
    "# Lecture 8: Processing Near Memory\n",
    "\n",
    "*Notes*\n",
    "\n",
    "**PIM**: Processing in Memory\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbb548",
   "metadata": {},
   "source": [
    "## 1 - Main Idea\n",
    "\n",
    "We want to see memory as an **accelerator** (e.g. specialzied compute-capability in-memory). \n",
    "\n",
    "The goal is to place compute unit *near* memory. \n",
    "\n",
    "<u>Example:</u> 3D-stacked logic+memory with a logic layer at the bottom of a series of memory layers as part of the construction of memory (driven by the Hybrid Memory Cube consortium). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e06a687",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 2 - 3D-stacked PIM\n",
    "\n",
    "Using 3D memory as a coarse-grained accelerator allows to change the entire system by performing simple function offloading. This requires the minimal processing-in-memory support.\n",
    "\n",
    "This can be used in large graph processing (e.g. rank computing requires frequent random memory accesses but little amounts of computation). Solution: interconnected set of 3d stacked memory + logic chips with simple cores (tesseract systems for graph processing).\n",
    "\n",
    "![tess](images/tesseract.png)\n",
    "\n",
    "### Tesseract (2015)\n",
    "\n",
    "<u>Advantages:</u>\n",
    "- Specialized graph processing accelerator using PIM\n",
    "- Large system performance and energy benefits\n",
    "- Takes advantage of 3D stacking for an important workload\n",
    "- More general than just graph processing\n",
    "\n",
    "<u>Disadvantages:</u>\n",
    "\n",
    "- Changes a lot in the system (new programming model, specialized tesseract cores for graph processing)\n",
    "- Cost\n",
    "- Scalability limited by off-chip links or graph partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653f44c",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3 - 3D stacked PIM in mobile device\n",
    "\n",
    "### Energy cost of data movement\n",
    "\n",
    "<u>Observations:</u>\n",
    "\n",
    "1. 62.7% of the total system energy is spent on data movement between the different parts of a computer (**on Tensorflow-Mobile, 57.3%**)\n",
    "\n",
    "2. A significant fraction of the data movement often comes from simple functions\n",
    "\n",
    "### Operation: PAcking\n",
    "\n",
    "A reordering of the elements of matrices to minimize cache misses during matrix multiplication.\n",
    "\n",
    "$$Matrix \\Rightarrow Packing \\Rightarrow Packed\\,\\,Matrix$$\n",
    "\n",
    "Up to 40% of the inference energy and 31% of the inference execution time correspond to cache misses. Packing's data movement accounts for up to 35.3% of the inference energy.\n",
    "\n",
    "> A simple data reorganization process that reuqires simple arithmetic\n",
    "\n",
    "### Operation: Quantization\n",
    "\n",
    "Corresponds to converting 32-bit floating points to 8-bit integers to improve inference execution time and energy consumption.\n",
    "\n",
    "**PIM cores cand accelerator reduce energy consumption by 40-50%**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85b3d7",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4 - PIM-Enabled Instructions (PEI)\n",
    "\n",
    "Goal: develop mechanisms to get the most out of near-data processing with **minimal cost, minimal changes to the system, no changes to the programming model**.\n",
    "\n",
    "1. Expose each PIM operation as a cache-coherent, virtually-addressed host processor instruction (called PEI) that operates on only a single data block.\n",
    "\n",
    "2. Dynamically decide where to execute a PEI (host processor or PIM accelerator) based on simple locality characteristics\n",
    "\n",
    "<u>Benefits:</u>\n",
    "\n",
    "- Localization: each PEI is bounded to one memory module\n",
    "- Interoperability: easier support for cache coherence and virtual memory\n",
    "- Simplified locality monitoring: data locality of PEIs can be identified simply by the cache control logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ee5160",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5 - Barriers to adoption of PIM\n",
    "\n",
    "1. Functionality of and applications of software for PIM\n",
    "2. Ease of programming\n",
    "3. System support: coherence & virtual memory\n",
    "4. Runtime and compilation systems for adaptive scheduling, data mapping, access/sharing control\n",
    "5. Infrastructures to assess benefits and feasibility\n",
    "\n",
    "### Challenge 1: Code Mapping\n",
    "\n",
    "Which operations should be executed in-memory vs. CPU?\n",
    "\n",
    "### Challenge 2: Maintain memory coherence for hybrid CPU-PIM Apps\n",
    "\n",
    "We need to provide a system to provide a memory coherence (i.e. the data is always accessible at its most updated copy). \n",
    "\n",
    "#### CoNDA: Efficient Cache Coherence Support for Near-Data Accelerators\n",
    "\n",
    "Specialized accelerators: GPU, FPGA, ASICs\n",
    "\n",
    "The coherence is between NDAs and CPUs. It is impractical to use traditional coherence protocols. \n",
    "\n",
    "![conda](images/conda.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
