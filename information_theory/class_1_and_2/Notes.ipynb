{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66886dfd",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "## Class 1 - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e351974",
   "metadata": {},
   "source": [
    "### Important concepts\n",
    "\n",
    "- source\n",
    "- alphabet\n",
    "- quantity of information in a message (expressed as a measure of entropy, the quantifying unit is called a ***shannon***)\n",
    "- source encoder\n",
    "- channel encoder\n",
    "- channel and how to model one\n",
    "- channel decoder\n",
    "- source decoder\n",
    "\n",
    "> message source -> source encoder -> channel encoder -> channel -> channel decoder -> source decoder -> message receiver\n",
    "\n",
    "**Note**: A channel is an analog device but the translation between digital and analog is not done at the channel encoder. The channel encoder is a digital device. The waveform translation is done at the channel level.\n",
    "\n",
    "> Only sequences of digits will be considered in the class\n",
    "\n",
    "### Redundancy\n",
    "\n",
    "A digital message (a binary string of 0 and 1) is usually appended with new values to allow lossless compression.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d9635",
   "metadata": {},
   "source": [
    "### Quantifying information (Self-Information)\n",
    "\n",
    "> Let $A$ be an event with non-zero probability $P(A)$. \n",
    ">\n",
    "> The greater the uncertainty of $A$, the alrger hte information $h(A)$ provide by the realization of $A$. This can be expressed as follow:\n",
    "$$h(a)=f(\\frac{1}{P(A)})$$\n",
    "\n",
    "$f$ must satisfy three properties:\n",
    "\n",
    "1. **$f$ is an increasing function over $\\mathbb{R}_+$**\n",
    "\n",
    "\\begin{align}\n",
    "P(A)\\text{ increasing}&\\Rightarrow h(A)\\text{ decreasing}\\\\\n",
    "P(A)\\text{ decreasing}&\\Rightarrow h(A)\\text{ increasing}\\\\\n",
    "h(A)&=f(\\frac{1}{P(A)})\\text{ with $f$ increasing}\\\\\n",
    "P(A)\\text{ increasing}&\\Rightarrow\\frac{1}{P(A)}\\text{ decreasing}\\Rightarrow f(\\frac{1}{P(A)})\\text{ decreasing}\n",
    "\\end{align}\n",
    "\n",
    "2. **information provided by 1 sure event is zero: $lim_{p\\rightarrow1}f(p)=0$**\n",
    "\n",
    "\\begin{align}\n",
    "P(A)\\rightarrow1&\\Rightarrow \\frac{1}{P(A)}\\rightarrow 1\n",
    "\\end{align}\n",
    "\n",
    "3. **information provided by 2 independent events: $f(p_1 + p_2) = f(p_1) + f(p_2)$**\n",
    "\n",
    "\\begin{align}\n",
    "h(A, B) &= f(\\frac{P(A, B)}) = f(\\frac{1}{P(A)}.\\frac{1}{P(B)})\\\\\n",
    "&\\overset{\\text{WE WANT}}{=} f(\\frac{1}{P(A)} + f(\\frac{1}{P(B)})\\\\\n",
    "&=h(A) + h(B)\n",
    "\\end{align}\n",
    "\n",
    "*We want the function of the product of two variables to be equal to the sum of the function of the two variables separately.*\n",
    "\n",
    "> This leads to use the **logarithmic function** for $f$\n",
    "\n",
    "$$h(A) = -log(P(A))$$\n",
    "\n",
    "$$-ln_b(P(A))=-\\frac{ln_{b'}(P(A))}{ln(b)}ln(b')$$\n",
    "\n",
    "Log2 is the Shannon information (binary unit), Loge is the logon (natural unit), and Log10 is the Hartley (decimal unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b30bd9",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "h(A, B)&=log_2(P(A, B))\\\\\n",
    "&=log_2(P(A)P(B|A))\\\\\n",
    "&=-log_2(P(A)) - log_2(P(B|A))\\\\\n",
    "&=h(A) + h(B|A) \\text{ information conveyed by joint A and B}\\\\\n",
    "&=h(B) + h(A|B) \\text{ information conveyed by joint A and B}\n",
    "\\end{align}\n",
    "\n",
    "> h(B|A): information content of B not provided by A\n",
    "\n",
    "Two events and we want to know the quantity of information provided by two events: quantity of information provided by one + quantity of information provided by the second one given the first one.\n",
    "\n",
    "\\begin{align}\n",
    "h(A,B)&\\overset{indep.}{=} -log_2(P(A).P(B))\\\\\n",
    "&= -log_2(P(A)) - log_2(P(B))\\\\\n",
    "&= h(A) + h(B)\\\\\n",
    "&\\Rightarrow h(A|B) = h(A) \\text{ if indep.}\\\\\n",
    "&\\Rightarrow h(B|A) = h(b) \\text{ if indep.}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65252e41",
   "metadata": {},
   "source": [
    "## Class 2\n",
    "\n",
    "### Recap\n",
    "\n",
    "\\begin{align}\n",
    "h(A) &= -log_2P(A)\\text{ shannon}\\\\\n",
    "h(A, B) &= -log_2P(A, B)\\text{ shannon}\\\\\n",
    "h(A, B) &= h(A) + h(B|A)\\text{ shannon}\\\\\n",
    "h(A, B) &= h(A) + h(B)\\text{ if A and B indep.}\\\\\n",
    "h(A|B) &= h(A)\\text{ if A and B indep.}\\\\\n",
    "h(B|A) &= h(B)\\text{ if A and B indep.}\n",
    "\\end{align}\n",
    "\n",
    "### Information quantity\n",
    "\n",
    "$$i(A, B) \\overset{\\Delta}{=} h(B) - h(B|A) \\overset{\\Delta}{=} h(A) - h(A|B)$$\n",
    "\n",
    "Of note: \n",
    "\n",
    "\\begin{align}\n",
    "h(A, B) &= h(A) + h(B|A)\\\\\n",
    "h(B|A) &= h(A, B) - h(A)\\\\\n",
    "&= h(A) + h(B) - h(A, B)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecb649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
