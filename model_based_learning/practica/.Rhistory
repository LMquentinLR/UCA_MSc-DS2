for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
print(means)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
lda
lda = class_mean_2(wine[,c(2,4)], y)
lda
aggregate(wine[,c(2,4)], by=list(y), FUN=mean())
aggregate(wine[,c(2,4)], by=list(y), FUN=mean
)
cov(wine[,c(2,4)])
cov(wine[,c(2,4)], method="pearson")
cov(wine[,c(2,4)], method="kendall")
cov(wine[,c(2,4)], method="spearman")
install.packages('IRkernel')
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, params)
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(nrow(xstar), K)
for (k in 1:K){
Prob[,k] = t(params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma))
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
IRkernel::installspec(user = FALSE)
sudo IRkernel::installspec(user = FALSE)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = t(params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma))
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
IRkernel::installspec(user = FALSE)
sudo
IRkernel::installspec(user = FALSE)
IRkernel::installspec()
IRkernel::installspec()
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
wine
wine[, -1]
nrows(wine[, -1])
nrow(wine[, -1])
nrow(wine[, 0])
wine[, 0]
wine$Type
wine$Type[sample(1:nrow(wine$Type)), ]
sample(1:nrow(wine$Type))
wine$Type
nrow(wine$Type)
ncol(wine$Type)
length(wine$Type)
wine$Type[sample(1:length(wine$Type)), ]
wine$Type[,sample(1:length(wine$Type))]
as.matrix(wine$Type)[sample(1:length(wine$Type)), ]
as.matrix(wine)[sample(1:length(wine$Type)), ]
as.matrix(wine)[sample(1:length(wine$Type)), ]$Type
wine[sample(1:length(wine$Type)), ]
wine[sample(1:length(wine$Type)), ]$Type
wine[sample(1:length(wine$Type))]$Type
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
z = dataset$Type
z
length(z)
int(1/3*length(z))
integer(1/3*length(z))
1/3*length(z)
round(1/3*length(z))
X[0:59,]
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split]
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split,]
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
y_test
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
predictions$classification
y_test
predictions$classification == y_test
count(predictions$classification == y_test)
list(predictions$classification == y_test)
list(predictions$classification == y_test)==TRUE
list(predictions$classification == y_test)
sum(list(predictions$classification == y_test))
sum(predictions$classification == y_test)
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:" nb_match/length(y_test))
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
training_set = sample(nrow(X), 130)
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
X_test
length(X_test)
length(X_train)
nrow(X_train)
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
training_set = sample(nrow(X), 130)
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
training_set = sample(nrow(X), round(2/3*length(y)))
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
training_set = sample(nrow(X), round(2/3*length(y)))
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
library(pgmm)
source("useful_functions.R")
data(wine)
source("./useful_functions.R")
setwd("~/Dev/UCA_MSc-DS2/model_based_learning/practica")
setwd("~/Dev/UCA_MSc-DS2/model_based_learning/practica")
source("./useful_functions.R")
source("useful_functions.R")
source("practicum_4_useful_funcs.R")
data(wine)
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
plot(X,col=y)
unique(y)
length(unique(y))
vector(3, length=3)
vector(3)
help(matrix)
a = 1
a += 1
mean(data)
mean(X)
X
help(mean)
mean(X[0])
mean(X[,0])
mean(X[,1])
X[,1]
X[,0]
X
X[,1]
X[,2]
X
vector(mean(X[,1]), mean(X[,2]))
list(mean(X[,1]), mean(X[,2]))
as.matrix(list(mean(X[,1]), mean(X[,2])))
cov(X)
as.matrix(cov(X))
initialization <- function(data) {
mus = as.matrix(list(mean(data[,1]), mean(data[,2])))
sigmas = list(cov(data), cov(data))
props = as.matrix(list(1/3, 1/3, 1/3))
return(list(mus, sigmas, props))
}
initialization(X)
m, s, p = initialization(X)
(m, s, p) = initialization(X)
l = initialization(X)
l[0]
l[1]
l[1][1]
l[1, 1]
l[1]
l[2]
l[1, [1]]
l[1, [1,]]
l[1, 1]
l[1]
class(l[1])
l[1][,1]
l[1][1]
l[1][2]
l[1][0]
l[1][1,1]
props = as.matrix(vector(1/3, 1/3, 1/3))
initialization <- function(data) {
mus = as.matrix(vector(mean(data[,1]), mean(data[,2])))
sigmas = vector(cov(data), cov(data))
props = as.matrix(vector(1/3, 1/3, 1/3))
return(list(mus, sigmas, props))
}
initialization(X)
initialization <- function(data) {
mus = as.matrix(list(mean(data[,1]), mean(data[,2])))
sigmas = list(cov(data), cov(data))
props = as.matrix(list(1/3, 1/3, 1/3))
return(list(mus, sigmas, props))
}
initialization(X)
initialization(X)[[1]]
initialization(X)[[1, 1]]
initialization(X)[[1]][1]
initialization(X)[[1]][1] +1
initialization(X)[[1]][1][1] + 1
initialization(X)[[1]][1][0] + 1
initialization(X)[[1]][1][0]
initialization(X)[[1]][1]
initialization(X)[[1]][1,0]
initialization(X)[[1]][1,1]
initialization(X)[[1]][1]
initialization <- function(data) {
mus = as.matrix(list(mean(data[,1]), mean(data[,2])))
sigmas = list(cov(data), cov(data))
props = as.matrix(list(1/3, 1/3, 1/3))
return(list(mus = mus, sigmas = sigmas, props = props))
}
initialization(X)$mus
initialization(X)$mus[1]
initialization(X)$mus + 1
help(apply)
apply(X, 0, function(x) x + 1)
apply(X, 1, function(x) x + 1)
apply(X, 2, function(x) x + 1)
apply(X, 3, function(x) x + 1)
apply(X, function(x) x + 1)
apply(X, FUN=function(x) x + 1)
apply(X, MARGIN=1, FUN=function(x) x + 1)
help matrix
help(matrix)
matrix(0, 10, 3)
matrix(0, 10, 3)[10, 1]
initialization(X)
