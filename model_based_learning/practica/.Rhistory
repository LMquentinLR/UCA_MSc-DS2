prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, c(p, p, K))
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(prop = prop, mu = mu, Sigma = Sigma)
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(mu = means, Sigma = covariance)
}
lda = lda.learn(X, y)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(p, p, K)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(prop = prop, mu = mu, Sigma = Sigma)
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(mu = means, Sigma = covariance)
}
lda = lda.learn(X, y)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(mu = means, Sigma = covariance)
}
lda = lda.learn(X, y)
lda = lda.learn(X, y)
lda
lda = class_mean_2(wine[,c(2,4)], y)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
lda
lda = class_mean_2(wine[,c(2,4)], y)
lda
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
class_mean_2 <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
print(means)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
lda
lda = class_mean_2(wine[,c(2,4)], y)
lda
aggregate(wine[,c(2,4)], by=list(y), FUN=mean())
aggregate(wine[,c(2,4)], by=list(y), FUN=mean
)
cov(wine[,c(2,4)])
cov(wine[,c(2,4)], method="pearson")
cov(wine[,c(2,4)], method="kendall")
cov(wine[,c(2,4)], method="spearman")
install.packages('IRkernel')
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, params)
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(nrow(xstar), K)
for (k in 1:K){
Prob[,k] = t(params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma))
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
IRkernel::installspec(user = FALSE)
sudo IRkernel::installspec(user = FALSE)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = t(params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma))
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = apply(Prob, 1, FUN = function(x){x/sum(x)})
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
IRkernel::installspec(user = FALSE)
sudo
IRkernel::installspec(user = FALSE)
IRkernel::installspec()
IRkernel::installspec()
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
wine
wine[, -1]
nrows(wine[, -1])
nrow(wine[, -1])
nrow(wine[, 0])
wine[, 0]
wine$Type
wine$Type[sample(1:nrow(wine$Type)), ]
sample(1:nrow(wine$Type))
wine$Type
nrow(wine$Type)
ncol(wine$Type)
length(wine$Type)
wine$Type[sample(1:length(wine$Type)), ]
wine$Type[,sample(1:length(wine$Type))]
as.matrix(wine$Type)[sample(1:length(wine$Type)), ]
as.matrix(wine)[sample(1:length(wine$Type)), ]
as.matrix(wine)[sample(1:length(wine$Type)), ]$Type
wine[sample(1:length(wine$Type)), ]
wine[sample(1:length(wine$Type)), ]$Type
wine[sample(1:length(wine$Type))]$Type
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
z = dataset$Type
z
length(z)
int(1/3*length(z))
integer(1/3*length(z))
1/3*length(z)
round(1/3*length(z))
X[0:59,]
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split]
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split,]
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
y_test
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
predictions$classification
y_test
predictions$classification == y_test
count(predictions$classification == y_test)
list(predictions$classification == y_test)
list(predictions$classification == y_test)==TRUE
list(predictions$classification == y_test)
sum(list(predictions$classification == y_test))
sum(predictions$classification == y_test)
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:" nb_match/length(y_test))
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
training_set = sample(nrow(X), 130)
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
X_test
length(X_test)
length(X_train)
nrow(X_train)
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
training_set = sample(nrow(X), 130)
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
training_set = sample(nrow(X), round(2/3*length(y)))
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
# we'll use this package to load the data
#install.packages("pgmm")
#install.packages('mvtnorm')
library(pgmm); library(mvtnorm)
data(wine)
head(wine)
# ?wine
X = as.matrix(wine[,c(2,4)])
dim(X)
colnames(X)
y = as.numeric(wine$Type!=1)+1
plot(X, col = y+1, pch=19)
lda.learn <- function(X, z) {
n = nrow(X); p=ncol(X)
K = max(z)
prop = rep(NA, K)
mu = matrix(NA, K, p)
Sigma = matrix(0, p, p)
for (k in 1:K){
n_k     = sum(z==k)
prop[k] = n_k/n
mu[k,]  = colSums(X[z==k,])/n_k
Sigma   = Sigma + n_k / n * cov(X[z == k,])
}
return(list(prop = prop, mu = mu, Sigma = Sigma))
}
ldaDF.learn <- function(X_dataframe, y) {
means <- aggregate(X_dataframe, by=list(y), FUN=mean)
covariance <-  cov(X_dataframe)
return(list(mu = means, Sigma = covariance))
}
lda = lda.learn(X, y)
#lda = ldaDF.learn(wine[,c(2,4)], y)
lda
lda.predict <- function(xstar, params){
K = length(params$prop)
Prob = matrix(NA, nrow(xstar), K)
for (k in 1:K){
Prob[,k] = params$prop[k] * dmvnorm(xstar, params$mu[k,], params$Sigma)
}
Prob = t(apply(Prob, 1, FUN = function(x){x/sum(x)}))
zstar = apply(Prob, 1, FUN = which.max)
return(list(probabilities = Prob, classification = zstar))
}
predictions = lda.predict(X, lda)
predictions
dataset = wine[sample(1:length(wine$Type)),]
X = dataset[, -1] #excluding the first column: label
y = dataset$Type
split = round(2/3*length(y))
X_train = X[0:split,]
X_test = X[split:length(y),]
y_train = y[0:split]
y_test = y[split:length(y)]
training_set = sample(nrow(X), round(2/3*length(y)))
split = round(2/3*length(y))
X_train = X[training_set,]
X_test = X[-training_set,]
y_train = y[training_set]
y_test = y[-training_set]
model = lda.learn(X_train, y_train)
model
predictions = lda.predict(X_test, model)
predictions
nb_match = sum(predictions$classification != y_test)
cat("\n: Classification table:\n")
table(y_test, predictions$classification)
cat("\nError Rate:", nb_match/length(y_test))
library(pgmm)
setwd("~/Dev/UCA_MSc-DS2/model_based_learning/practica")
source("practicum_4_useful_funcs.R")
data(wine)
X = as.matrix(wine[,c(2,4)])
y = wine[,1]
plot(X,col=y)
initialization <- function(data) {
mus = as.matrix(list(mean(data[,1]), mean(data[,2])))
sigmas = list(cov(data), cov(data))
props = as.matrix(list(1/3, 1/3, 1/3))
return(list(mus = mus, sigmas = sigmas, props = props))
}
E_step <- function(data, params) {
N = nrow(X); K=3
gammas = matrix(0, n, k)
for (n in 1:N){
for (k in 1:K){
params
gammas[n, k] =
}
}
}
