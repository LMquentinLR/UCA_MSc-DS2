mostMatchingDistribution <- function(clusters, x, prop, mu, sigma){
######
#### Given a set of points, find the most matching distribution for them.
#### to be used for a test set.
######
# Computes the likelihood of belonging to a distribution
likelihoods = logGamma(dim(x)[1], clusters, x, prop, mu, sigma)
distributions = findCluster(likelihoods)
# return statement
return(list("likelihoods" = likelihoods, "distributions" = distributions))
}
findCluster <- function(likelihoods) {
######
#### Given an array of likelihood with each column representing the
#### likelihood of belonging to a specific cluster, finds the most
#### matching cluster.
######
# Computes the distribution to which each point belongs
distributions = (likelihoods == apply(likelihoods, 1, max))
distributions = apply(distributions, 1, which)
# Return statement
return(distributions)
}
expectationMaximization <- function(x, clusters, maxIterations=50) {
######
#### Implementation of an Expectation Maximization algorithm for multivariate
#### data, here the wine dataset provided by the pgmm library.
######
# Variable declarations and initializations
n = dim(x)[1]
K = dim(x)[2]
prop = rep(1/clusters, clusters)
mu = rmvnorm(clusters, colMeans(x), cov(x))
sigma = lapply(1:clusters, function(i) cov(x))
# Algorithm loop
for (loop in 1:maxIterations){
#### Expectation step
# Computes the new gamma values
gam = logGamma(n, clusters, x, prop, mu, sigma)
# Normalizes the gamma rows
gam = gam - apply(gam, 1, logSumExp)
gam = exp(gam)
#### Maximization step
for (cl in 1:clusters) {
nk = sum(gam[,cl])
# Parameter updates
prop[cl] = nk/n
mu[cl,] <- Reduce("+", lapply(1:n, function(i) gam[i,cl]*x[i,]))/nk
sigma[[cl]] <- Reduce(
"+",
lapply(1:n,function(i) gam[i,cl]*(x[i,]-mu[cl,])%*%t(x[i,]-mu[cl,]))
)/nk
}
}
# Computes the llh results for the given data
llh_results = mostMatchingDistribution(clusters, x, prop, mu, sigma)
# return statement
ret = list(
"n_wines" = n,
"n_clusters" = clusters,
"cl_gamma" = gam,
"cl_proportions" = prop,
"cl_means" = mu,
"cl_sigma" = sigma,
"distributions" = llh_results$distributions,
"logLikelihoods" = llh_results$likelihoods,
"sum_of_llh" = sum(llh_results$likelihoods)
)
return(ret)
}
generateCentroid <- function(x) {
######
#### Generates a random centroid from a given dataset.
######
# Variable declarations and initializations
K = dim(x)[2]
centroid = c()
# Randomly draws a value for each dimension of the dataset
centroid = rmvnorm(1, colMeans(x), cov(x))
colnames(centroid) <- NULL
# return statement
return(centroid)
}
euclidianDistance <- function(x, centroid){
######
#### Computes the euclidian distance between a datapoint and a centroid.
######
# Function declaration
dist = function(x, y) (x-y)^2
# Computes the distances
distances = t(apply(x, 1,
function(i) sqrt(sum(mapply(dist, i, centroid))))
)
# Return statement
return(distances)
}
findClosestCentroids <- function(x, centroids, clusters){
######
#### Computes the closest centroids for a given set of datapoints.
######
# Variable declarations and initializations
n = dim(x)[1]
K = dim(x)[2]
# Computes the distances
distances = c()
for (cl in 1:clusters){
cluster_dist = euclidianDistance(x, centroids[cl,])
distances = append(distances, cluster_dist)
}
distances = array(distances, dim=c(n, clusters))
# Finds the closest centroids
closest = (distances == apply(distances, 1, min))
# Return statement
return(list("distances"=distances, "closest"=closest))
}
kMeans <- function(x, clusters, maxIterations=50) {
######
#### Implementation of a K-Means algorithm for multivariate data, here
#### the wine dataset provided by the pgmm library.
######
# Variable declarations and initializations
n = dim(x)[1]
K = dim(x)[2]
centroids = t(lapply(1:clusters, function(i) generateCentroid(x)))
centroids = matrix(unlist(centroids), ncol=K, byrow=TRUE)
# Computes the euclidian distances between every point and each centroid
for (loop in 1:maxIterations){
# Computes the closest centroids
closest_centroids = findClosestCentroids(x, centroids, clusters)
# Computes the new closest centroids
centroids = c()
for (cl in 1:clusters) {
centroids = rbind(centroids, colMeans(x[closest_centroids$closest[,cl],]))
}
# Erases the matrix's columns and rows names inherited from the dataset
colnames(centroids) <- NULL
rownames(centroids) <- NULL
}
#return statement
ret = list(
"centroids" = centroids,
"distances" = closest_centroids$distances,
"closest_centroids" = apply(closest_centroids$closest, 1, which)
)
return(ret)
}
akaikeIC <- function(logLikelihood, clusters, K) {
######
#### Implementation of the the Akaike Information Criterion such that:
#### AIC = LLH - eta(M)
#### i.e. the final log-likelihood of a model minus the number of free
#### scalar parameters in the model (nb of proportions (-1 as there are
#### only cluster-1 degrees of freedom) + nb of means + nb of sigmas)
######
penalization = (clusters-1) + clusters*K + clusters*((K*(K+1))/2)
return(logLikelihood - penalization)
}
computeAIC <- function(x, max_cluster=max_n_clusters, print_steps=TRUE){
######
#### Computes the AIC of an EM algorithm implementation given a dataset
######
# Variable declaration
akaike_results = c()
# Loops through each cluster parameter to compute the corresponding AIC
for (j in min_n_clusters:max_cluster){
res = expectationMaximization(x, clusters=j)
akaike_value = akaikeIC(res$sum_of_llh, j, dim(x)[2])
akaike_results = append(akaike_results, akaike_value)
if (print_steps) {
print(paste("Total log-likelihood with ", j, " clusters: ",
round(akaike_value, 3)))
}
}
# Prints the result
print(paste("The best result is achieved with ",
which.max(akaike_results)+2,
" clusters (AIC)."))
}
bayesianIC <- function(logLikelihood, clusters, n, K) {
######
#### Implementation of the Bayesian Information Criterion such that:
#### BIC = LLH - 1/2*eta(M)*log(n)
#### i.e. the final log-likelihood of a model minus the half of the number
#### of free scalar parameters in the model (nb of proportions (-1 as there
#### are only cluster-1 degrees of freedom) + nb of means + nb of sigmas)
#### then multiplied by the log of the number of datapoints
######
# Variable declaration
penalization = (clusters-1) + clusters*K + clusters*((K*(K+1))/2)
return(logLikelihood - 1/2 * penalization * log(n))
}
computeBIC <- function(x, max_cluster=max_n_clusters, print_steps=TRUE){
######
#### Computes the BIC of an EM algorithm implementation given a dataset
######
# Variable declaration
bayesian_results = c()
# Loops through each cluster parameter to compute the corresponding AIC
for (j in min_n_clusters:max_cluster){
res = expectationMaximization(x, clusters=j)
bayesian_value = bayesianIC(res$sum_of_llh, j, dim(x)[1], dim(x)[2])
bayesian_results = append(bayesian_results, bayesian_value)
if (print_steps) {
print(paste("Total log-likelihood with ", j, " clusters: ",
round(bayesian_value, 3)))
}
}
# Prints the result
print(paste("The best result is achieved with ",
which.max(bayesian_results)+2,
" clusters (BIC)."))
}
doubleCrossValidation <- function(x_train, x_test, folds=10, max_cluster=max_n_clusters) {
######
#### Implements a double cross-validation with the resulting log-likelihood
#### being the selection criteria.
######
# Variable declaration
n_train = dim(x_train)[1]
fold_indexes = split(c(1:n_train),
ceiling(seq_along(c(1:n_train))/(n_train/10)))
mean_cluster_criteria = c()
# iterates over the cluster range
for (cl in min_n_clusters:max_cluster){
# Performs the first step of the double cross-validation: iteration over the
# folds of the training set
llhs = c()
best_model = NULL
for (kFold in fold_indexes){
x_train_train = x_train[-kFold,]
x_train_val = x_train[kFold,]
resultsEM = expectationMaximization(x_train_train, cl)
matching_distribution = mostMatchingDistribution(
resultsEM$n_clusters,
x_train_val,
resultsEM$cl_proportions,
resultsEM$cl_means,
resultsEM$cl_sigma
)
# Records the resulting log-likelihood
sum_of_llhs = sum(matching_distribution$likelihoods)
llhs = append(llhs, sum_of_llhs)
if (is.null(best_model) || sum_of_llhs == min(llhs)){
best_model <- resultsEM
}
}
# Computes the likelihood on the test set given the best achieved model
matching_distribution = mostMatchingDistribution(
best_model$n_clusters,
x_test,
best_model$cl_proportions,
best_model$cl_means,
best_model$cl_sigma
)
llhs = append(llhs, sum(matching_distribution$likelihoods))
# Records the mean llhs achieved with the cluster
print(paste("Mean log-likelihood achieved with ", cl, " clusters: ",
round(mean(llhs),4)))
mean_cluster_criteria = append(mean_cluster_criteria, mean(llhs))
}
# Finds which cluster had the best performance
best_cluster = which.max(mean_cluster_criteria)
print(paste("The best result is achieved with ",
best_cluster+2,
" clusters (double CV)."))
# return statement
return(mean_cluster_criteria)
}
n_clusters = 3
resultsEM = expectationMaximization(X, n_clusters)
resultsKM = kMeans(X, n_clusters)
plot(X, col=resultsEM$distributions+1, pch=19)
points(resultsEM$cl_means, col=1, pch=10)
plot(X, col=resultsKM$closest_centroids + 1, pch=19)
points(resultsKM$centroids, col=1, pch=10)
resultsEM_train = expectationMaximization(X_train, n_clusters)
resultsKM_train = kMeans(X_train, n_clusters)
test_resultsEM = mostMatchingDistribution(
n_clusters,
X_test,
resultsEM_train$cl_proportions,
resultsEM_train$cl_means,
resultsEM_train$cl_sigma
)
test_resultsKM = findClosestCentroids(
X_test,
resultsKM_train$centroids,
n_clusters
)
test_resultsKM$closest = apply(test_resultsKM$closest, 1, which)
plot(X_test, col=test_resultsEM$distributions+1, pch=19)
points(resultsEM_train$cl_means, col=1, pch=10)
plot(X_test, col=test_resultsKM$closest+1, pch=19)
points(resultsKM_train$centroids, col=1, pch=10)
classError(resultsEM$distributions, y)
classError(resultsKM$closest_centroid, y)
classError(test_resultsEM$distributions, y_test)
classError(test_resultsKM$closest, y_test)
adjustedRandIndex(resultsEM$distributions, y)
adjustedRandIndex(resultsKM$closest_centroid, y)
adjustedRandIndex(test_resultsEM$distributions, y_test)
adjustedRandIndex(test_resultsKM$closest, y_test)
computeAIC(X)
computeBIC(X)
cv_logLikelihoods = doubleCrossValidation(X_train, X_test)
Xhd = as.matrix(wine[,c(2:4)])
computeBIC(Xhd)
# we reuse the randomized indexes computed in 1.4
randomized_Xhd = Xhd[randomized_indexes,]
randomized_yhd = y[randomized_indexes]
Xhd_train = randomized_Xhd[1:training_split,]
Xhd_test = randomized_Xhd[training_split:n,]
yhd_train = randomized_yhd[1:training_split]
yhd_test = randomized_yhd[training_split:n]
resultsEMhd_train = expectationMaximization(Xhd_train, 3)
test_resultsEMhd = mostMatchingDistribution(
3,
Xhd_test,
resultsEMhd_train$cl_proportions,
resultsEMhd_train$cl_means,
resultsEMhd_train$cl_sigma
)
plot(Xhd_test[,c(1,3)], col=test_resultsEMhd$distributions+1, pch=19); points(resultsEMhd_train$cl_means[,c(1,3)], col=1, pch=10)
classError(test_resultsEMhd$distributions, y_test)
adjustedRandIndex(test_resultsEM$distributions, y_test)
# We extract the features from the original dataset
wine_features = subset(wine, select=-c(Type))
# We produce 5 5-permutations by random sampling
five_perms = c()
for (p in 1:5) {
five_perms = rbind(five_perms, sample(colnames(wine_features), 5))
}
# We produce the set of 4-permutations
four_perms = NULL
for (p in 1:nrow(five_perms)) {
all <- expand.grid(p1 = five_perms[p,],
p2 = five_perms[p,],
p3 = five_perms[p,],
p4 = five_perms[p,],
stringsAsFactors = FALSE)
perms <- all[apply(all, 1, function(x) {length(unique(x)) == 4}),]
if (is.null(four_perms)) {
four_perms = perms
} else {
four_perms = rbind(four_perms, perms)
}
}
four_perms = as.matrix(four_perms)
# We produce the set of 3-permutations
three_perms = NULL
for (p in 1:nrow(four_perms)) {
all <- expand.grid(p1 = four_perms[p,],
p2 = four_perms[p,],
p3 = four_perms[p,],
stringsAsFactors = FALSE)
perms <- all[apply(all, 1, function(x) {length(unique(x)) == 3}),]
if (is.null(three_perms)) {
three_perms = perms
} else {
three_perms = rbind(three_perms, perms)
}
}
three_perms = as.matrix(three_perms)
five_perms = five_perms[!duplicated(apply(five_perms, 1, sort)),]
four_perms = four_perms[!duplicated(apply(four_perms, 1, sort)),]
three_perms = three_perms[!duplicated(apply(three_perms, 1, sort)),]
for (perm in 1:nrow(five_perms)) {
cat("\nSelected features: ", paste(five_perms[perm,], collapse=", "), "\n")
computeAIC(as.matrix(wine_features[five_perms[perm,]]), print_steps=FALSE)
}
for (perm in 1:nrow(five_perms)) {
cat("\nSelected features: ", paste(five_perms[perm,], collapse=", "), "\n")
computeBIC(as.matrix(wine_features[five_perms[perm,]]), print_steps=FALSE)
}
for (perm in sample(1:nrow(four_perms), 20)) {
cat("\nSelected features: ", paste(four_perms[perm,], collapse=", "), "\n")
computeAIC(as.matrix(wine_features[four_perms[perm,]]), print_steps=FALSE)
}
for (perm in sample(1:nrow(three_perms), 20)) {
cat("\nSelected features: ", paste(three_perms[perm,], collapse=", "), "\n")
computeAIC(as.matrix(wine_features[three_perms[perm,]]), print_steps=FALSE)
}
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2, byrow=TRUE)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
p = 1/10
binomial_draws = rbinom(n, 1, p)
binomial_draws
?which
which(binomial_draws==1)
data[which(binomial_draws==1),2]=NA
data
which(binomial_draws==1)
data[which(binomial_draws==1),2]=NA
data
plot(data)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
data[missing_idx.mcar] = NA
data
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
data[missing_idx.mcar, 2] = NA
data
mean(data)
?mean
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
X = data[missing_idx.mcar, 2]
# We use the empirical mean and sigma
mu_init = mean(data)
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
X = data
X[missing_idx.mcar, 2] = NA
# We use the empirical mean and sigma
mu_init = apply(data, 1, mean)
mu_init
# We use the empirical mean and sigma
mu_init = apply(data, 0, mean)
# We use the empirical mean and sigma
mu_init = apply(data, 2, mean)
mu_init
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
X = data
X[missing_idx.mcar, 2] = NA
# We use the empirical mean and sigma
mu_init = apply(X, 2, rm.na=TRUE)
library(mvtnorm)
mu = matrix(c(5, -1), nrow=1, ncol=2)
sigma = matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol=2)
mu
sigma
n = 100
data = rmvnorm(n, mu, sigma)
#p = 1/10
#binomial_draws = rbinom(n, 1, p)
#data[which(binomial_draws==1),2]=NA
missing_idx.mcar <- sample.int(n, 0.3*n)
X = data
X[missing_idx.mcar, 2] = NA
# We use the empirical mean and sigma
mu_init = apply(X, 2, mean, rm.na=TRUE)
mu_init
# We use the empirical mean and sigma
mu_init = apply(X, 2, mean, na.rm=TRUE)
mu_init
# We use the empirical mean and sigma
mu_init = apply(X, 2, mean, na.rm=TRUE)
sigma_init = cov(X, use"complete.obs")
# We use the empirical mean and sigma
mu_init = apply(X, 2, mean, na.rm=TRUE)
sigma_init = cov(X, use="complete.obs")
sigma_init
