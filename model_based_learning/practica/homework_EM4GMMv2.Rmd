---
title: "From Scratch Implementation of the Expectation-Maximization Algorithm for Multivariate Gaussian Mixture Models"
author: |
  | Elias Boughosn
  | Quentin Le Roux
output:
  html_document:
    df_print: paged
---
  
The following R-markdown implements an Expectation-Maximization algorithm from scratch in the case of a multivariate Gaussian Mixture Model.

The pgmm package's Wine dataset is used as a test case example.

### Implementation steps

1) **Implementation of the EM algorithm with a basic bivariate example**

  - Implement the EM algorithm from scratch for a GMM on the variables 2 and 4 of the Wine dataset.
  - Cluster the data and compare the results with a K-Means algorithm.
  - Assess the quality of the clustering (the function classError and/or adjustedRandIndex from the mclust package will be used).

2) **Performing a model selection**

  - Find a relevant number of clusters using three criterion methods: AIC, BIC, and cross-validated likelihood.
  
***Note***: *A double cross-validation will be used*.

3) **Towards higher dimensional spaces**

  - Model more than two variables (from the Wine dataset). 
  - Comment on the resulting clusters, their number, etc.

<hr>

***Notes on known issues***: *A list of known issues is available at the end of the document/file*.

<hr>

# 1 - Preliminary steps: library and dataset imports

#### 1.1. Library imports

```{r  echo=TRUE, results='hide', message=FALSE}

# install.packages("pgmm")
# install.packages('mvtnorm')
# install.packages('mclust')
# install.packages('ggplot2')

library(pgmm); library(mvtnorm); library(mclust); library(ggplot2)

```

### 1.2. Dataset import

Once the packages are imported, the dataset is loaded in the R environment.
```{r}

data(wine)
head(wine)

```

The variables 2 and 4 of the Wine dataset are extracted -- they will form our features for the first part of the implementation. The labels of each entry are also extracted (variable 1 of the Wine dataset).

```{r}

X = as.matrix(wine[,c(2,4)])
y = wine$Type

```

#### 1.3. Dataset check

We check the shape and content of the features we selected.

```{r}

print(paste("Feature dimensions: ", paste(dim(X), collapse= ", ")))
print(paste("Feature names: ", paste(colnames(X), collapse=", ")))

```

<u>Plot of the bivariate feature data:</u>

```{r}

plot(X, col=y+1, pch=19)

```

#### 1.4. Train/Test Split

Beyond clustering the whole dataset, we are also interested in the performance of our algorithm. 

To properly assess the resulting clusterings that will be performed below, we split the dataset between training and testing sets. 

To this effect, we choose a $0.7$ split for training and $0.3$ split for testing. This is informed by it being the standard split in the AI field for small datasets. The split is done via randomizing the dataset order and splitting at the $0.7/0.3$ threshold.

```{r}

# We randomize the dataset via its index
randomized_indexes = sample(1:nrow(X))
randomized_X = X[randomized_indexes,]
randomized_y = y[randomized_indexes]


n = dim(X)[1]
training_split = round(0.7*n,0)

X_train = randomized_X[1:training_split,]
X_test = randomized_X[(training_split+1):n,]
y_train = randomized_y[1:training_split]
y_test = randomized_y[(training_split+1):n]

```

# 2 - Function declarations

The following section contains the declarations of all the functions used in this markdown. You will find the from-scratch implementations of the Expectation-Maximization and K-Means algorithms here. The EM algorithm can be declared with random initialization or KM initialization.

Functions are declared in alphabetical order in each subsection.

#### 2.1. Generic functions

```{r}

euclidianDistance <- function(x, centroid){
  ######
  #### Computes the euclidian distance between a datapoint and a centroid.
  ######
  
  # Declares necessary functions for computing the euclidian distance
  dist = function(a, b) (a-b)^2
  norm = function(a)    sqrt(sum(mapply(dist, a, centroid)))
  
  # Computes the distances
  distances = t(apply(x, 1, norm))
  
  # Return statement
  return(distances)
}

findClosestCentroids <- function(x, centroids, clusters){
  ######
  #### Computes the centroid closest to each datapoints in an input array.
  ######
  
  # Variable initialization
  n = dim(x)[1]
  distances = c()
  
  # Computes the euclidian distances
  for (cl in 1:clusters){
    cluster_dist = euclidianDistance(x, centroids[cl,])
    distances = append(distances, cluster_dist)
  }
  distances = array(distances, dim=c(n, clusters))
  
  # Finds the closest centroids
  closest = (distances == apply(distances, 1, min))
  
  # Return statement
  return(list("distances"=distances, "closest"=closest))
    
}

findClusters <- function(likelihoods) {
  ######
  #### Given an array of likelihood values (each column representing the 
  #### likelihood of belonging to a specific cluster, each row representing 
  #### a datapoint in an underlying dataset), finds the most matching cluster.
  ######
  
  # Finds the most likely distribution to which each point belongs
  clustering = apply((likelihoods == apply(likelihoods, 1, max)), 1, which)
  
  # Return statement
  return(clustering)
}

generateCentroid <- function(x) {
  ######
  #### Generates a random centroid for a given dataset, given a normal
  #### multivariate distribution with the mean and variance equal to the
  #### empirical mu and Sigma of the dataset.
  ######
  
  # Variable initialization
  centroid = c()
  
  # Randomly draws a value for each dimension of the dataset
  centroid = rmvnorm(1, colMeans(x), cov(x))
  colnames(centroid) <- NULL
  
  # return statement
  return(centroid)
}

logLikelihood <- function(n, clusters, x, prop, mu, sigma) {
  ######
  #### Computes the gamma values of a dataset as part of an EM algorithm.
  ######
  
  # Variable initialization
  lgn = matrix(nrow=n, ncol=clusters)
  
  # Computes the gammas per clusters
  for (cl in 1:clusters){
    log_proba = matrix(dmvnorm(x, mu[cl,], sigma[[cl]], log=TRUE))
    lgn[,cl] = log(prop[cl]) + log_proba
  }
  
  # return statement
  return(lgn)
}

logSumExp <- function (x) {
  ######
  #### Computes the log-sum-exponential of a vector/list of variables.
  ######
  
  max(x) + log(sum(exp(x - max(x))))
}

mostMatchingDistribution <- function(clusters, x, prop, mu, sigma){
  ######
  #### Given a set of points, find the most matching distributions for them.
  #### To be used for a test set.
  ######
  
  # Computes the likelihood of belonging to a distribution
  likelihoods = logLikelihood(dim(x)[1], clusters, x, prop, mu, sigma)
  clustering = findClusters(likelihoods)
  
  # return statement
  return(list("likelihoods" = likelihoods, "clustering" = clustering))
}

```

#### 2.2. K-Means Algorithm (from scratch)

```{r}

kMeans <- function(x, clusters, maxIterations=200, printMessage=T) {
  ######
  #### Implementation of a K-Means algorithm.
  ######
  
  if(printMessage){cat("Processing KM-Means with", clusters, "cluster(s).")}
  
  # Variable initialization: dimensions of the input data
  n = dim(x)[1]
  K = dim(x)[2]
  # Variable initialization: centroids
  centroids = t(lapply(1:clusters, function(i) generateCentroid(x)))
  centroids = matrix(unlist(centroids), ncol=K, byrow=TRUE)
  # Variables initialization: vector to record step values
  history_centroids = c(centroids)
  
  ###########################
  ###### KM loop BEGIN ###### 
  ###########################
  for (loop in 1:maxIterations){
    
    # Computes the current closest centroids for each datapoint
    closest_centroids = findClosestCentroids(x, centroids, clusters)
   
    # Updates the closest centroids
    centroids = c()
    for (cl in 1:clusters) {
      # checks on the length of the cluster split as colMeans crashes if
      # x[closest_centroids$closest[,cl],]  is a single datapoint
      if (length(x[closest_centroids$closest[,cl],])==2) {
        column_means = x[closest_centroids$closest[,cl],] 
      } else {
        column_means = colMeans(x[closest_centroids$closest[,cl],])
      }
      centroids = rbind(centroids, column_means)
    }
    
    # Erases the matrix's columns and rows names inherited from the dataset
    colnames(centroids) <- NULL
    rownames(centroids) <- NULL
    
    # Records the loop's update
    history_centroids = append(history_centroids, centroids)
  }
  ###########################
  ####### KM loop END ####### 
  ###########################
  
  #return statement
  ret = list(
    "centroids" = centroids,
    "distances" = closest_centroids$distances,
    "closest_centroids" = apply(closest_centroids$closest, 1, which),
    "centroids_history" = history_centroids
   )
  return(ret)
}

```

#### 2.3. Expectation-Maximization Algorithm (from scratch)

```{r}

expectationMaximization <- function(
  x, clusters, 
  maxIterations=200, kmInit = F, printMessage=T
) {
  ######
  #### Implementation of a multivariate EM algorithm.
  ######
  
  if (printMessage) {
    if (kmInit) {
      cat("Processing EM with", clusters, "cluster(s), KM initialization.")
    } else {
      cat("Processing EM with", clusters, "cluster(s), random initialization.")
    }
  }
  
  # Variables initialization: dimensions of the input dataset
  n = dim(x)[1]
  K = dim(x)[2]
  # Variables initialization: starting values for the EM parameters (means
  # and sigmas are generated randomly with a multivariate normal distribution)
  prop = rep(1/clusters, clusters)
  if (kmInit) {
    mu = kMeans(x, clusters, maxIterations, printMessage=F)$centroids
  } else {
    mu = rmvnorm(clusters, colMeans(x), cov(x))
  }
  sigma = lapply(1:clusters, function(i) cov(x))
  # Variables initialization: comparators, vectors for recording 
  #                           step values, etc.
  history_LLH = c()
  history_prop = c()
  history_mu = c()
  history_sigma = c()
  previous_LLH = -Inf
  counter = 0
  loop_counter = 0
  
  ###########################
  ###### EM loop BEGIN ###### 
  ###########################
  for (loop in 1:maxIterations){
    
    ################################################
    #### E(xpectation)-Step (with logExp trick) ####
    ################################################
    
    loop_counter = loop_counter + 1
    
    # Computes log-likelihood of the setup at the start of the loop
    LLH = logLikelihood(n, clusters, x, prop, mu, sigma)
    sum_LLH = sum(apply(LLH, 1, logSumExp))
    
    # Checks the LLH with the last recorded LLH for early stopping
    # Criterion: no update for 5 loops (with llh rounded to 4 decimals)
    if (round(sum_LLH,4) <= round(previous_LLH,4)) {
      counter = counter + 1
      if (counter >= 5) {
        if (printMessage) {cat("\nEarly stopping at loop:", loop)}
        break
      }
    } else {counter = 0; previous_LLH = sum_LLH}
    
    # Records the current LLH (plotting purposes)
    history_LLH = append(history_LLH, sum_LLH)
    
    # Computes the gamma values per datapoints and clusters with
    # the logExp trick
    gam = exp(LLH - apply(LLH, 1, logSumExp))
    
    ################################################
    ############## M(aximization)-Step #############
    ################################################
    
    # Computes the update parameters of the EM algorithm 
    # for the current loop for each cluster
    for (cl in 1:clusters) {
      
      # Computes the sum of gammas and updates the cluster's proportions
      nk = sum(gam[,cl])
      prop[cl] = nk/n
      # Updates the mean parameters
      mean_compute = function(i) gam[i,cl] * x[i,]
      mu[cl,] = Reduce("+", lapply(1:n, mean_compute))/nk
      # Updates the covariance matrix parameters
      m = mu[cl,]
      sigma_compute = function(i) gam[i,cl] * (x[i,]-m) %*% t(x[i,]-m)
      sigma[[cl]] = Reduce("+", lapply(1:n, sigma_compute))/nk
      
    }
    
    # Records the loop's updates
    history_prop = append(history_prop, prop)
    history_mu = append(history_mu, mu)
    history_sigma = append(history_sigma, sigma)
    
  }
  ###########################
  ####### EM loop END ####### 
  ###########################
  
  # Computes the log-likelihood results for the given dataset
  llh_results = mostMatchingDistribution(clusters, x, prop, mu, sigma)
  llh_sum = sum(apply(llh_results$likelihoods, 1, logSumExp))
  if (printMessage) {cat("\nEnd log-likelihood: ", llh_sum, "\n")}
  
  # Formatting the record variables
  history_prop = array(history_prop, c(loop_counter, clusters))
  history_mu = array(history_mu, c(clusters, K, loop_counter))
  history_sigma = array(history_sigma, c(clusters, K, K, loop_counter))
  
  # return statement
  ret = list(
    "N" = n,
    "n_clusters" = clusters,
    "prop" = prop,
    "means" = mu,
    "sigma" = sigma,
    "clustering" = llh_results$clustering,
    "llh_per_points" = llh_results$likelihoods,
    "llh_sum" = llh_sum,
    "prop_history" = history_prop,
    "means_history" = history_mu,
    "sigma_history" = history_sigma,
    "llh_history" = history_LLH
    )
  return(ret)
}

```

#### 2.4. Akaike Information Criterion

```{r}

akaikeIC <- function(llh, clusters, K) {
  ######
  #### Implementation of the the Akaike Information Criterion such that:
  #### AIC = log-likelihood - eta(M)
  #### i.e. the final log-likelihood of a model minus the number of free
  #### scalar parameters in the model (nb of proportions (-1 as there are
  #### only cluster-1 degrees of freedom) + nb of means + nb of sigmas).
  ######
  llh - (clusters-1) + clusters*K + clusters*((K*(K+1))/2)
}

computeAIC <- function(x, max_cluster=max_clusters, print_steps=TRUE){
  ######
  #### Computes the AIC of an EM algorithm implementation.
  ######
  
  # Variable initialization
  akaike_results = c()
  
  # Loops through a cluster parameter range to compute the AIC
  for (cl in min_clusters:max_cluster){
    EM = expectationMaximization(x, clusters=cl, printMessage=F)
    akaike = akaikeIC(EM$llh_sum, cl, dim(x)[2])
    akaike_results = append(akaike_results, akaike)
    if (print_steps) {
      print(paste("Total LLH with ", cl, " clusters: ", round(akaike, 3)))
    }
  }
  
  # Prints the result
  print(paste("The best AIC result is achieved with ", 
              which.max(akaike_results)+2, 
              " clusters."))
}

```

#### 2.5. Bayesian Information Criterion

```{r}

bayesianIC <- function(llh, clusters, n, K) {
  ######
  #### Implementation of the Bayesian Information Criterion such that:
  #### BIC = LLH - 1/2*eta(M)*log(n)
  #### i.e. the final log-likelihood of a model minus the half of the number
  #### of free scalar parameters in the model (nb of proportions (-1 as there
  #### are only cluster-1 degrees of freedom) + nb of means + nb of sigmas)
  #### then multiplied by the log of the number of datapoints.
  ######
  
  # Variable declaration
  llh - 1/2 * ((clusters-1) + clusters*K + clusters*((K*(K+1))/2)) * log(n)
}

computeBIC <- function(x, max_cluster=max_clusters, print_steps=TRUE){
  ######
  #### Computes the BIC of an EM algorithm implementation.
  ######
  
  # Variable declaration
  bayesian_results = c()
  
  # Loops through each cluster parameter to compute the corresponding AIC
  for (cl in min_clusters:max_cluster){
    EM = expectationMaximization(x, clusters=cl, printMessage=F)
    bayesian = bayesianIC(EM$llh_sum, cl, dim(x)[1], dim(x)[2])
    bayesian_results = append(bayesian_results, bayesian)
    if (print_steps) {
      print(paste("Total LLH with ", cl, " clusters: ", round(bayesian, 3)))
    }
  }

  # Prints the result
  print(paste("The best BIC result is achieved with ", 
              which.max(bayesian_results)+2, 
              " clusters."))
}

```

#### 2.6. (Cross-)Validated Likelihood

```{r}

doubleCrossValidation <- function(x_train, x_test, folds=10, max_cl=max_clusters) {
  ######
  #### Implements a double cross-validation with the resulting log-likelihood
  #### being the selection criteria.
  ######
  
  # Variable initialization
  n_train = dim(x_train)[1]
  foldAllocation = ceiling(seq_along(c(1:n_train))/(n_train/10))
  fold_indexes = split(c(1:n_train), foldAllocation)
  mean_cluster_criteria = c()
  
  # Iterates over the cluster range
  for (cl in min_clusters:max_cl){
    
    # Performs the first step of the double cross-validation: iteration
    # over the folds of the training set
    llhs = c()
    best_model = NULL
    
    # iterates over the k-folds
    for (kFold in fold_indexes){
      
      # Stores the training dataset depending on which fold is validation
      x_train_train = x_train[-kFold,]
      x_train_val = x_train[kFold,]
      
      # Computes the EM (on the training set)
      EM = expectationMaximization(x_train_train, cl, printMessage=F)
      
      # Retrieves the resulting llhs/distributions on the validation set
      dists = mostMatchingDistribution(
        EM$n_clusters, x_train_val, EM$prop, EM$means, EM$sigma
      )
      
      # Records the resulting log-likelihood
      sum_of_llhs = sum(apply(dists$likelihoods, 1, logSumExp))
      llhs = append(llhs, sum_of_llhs)
      
      # Updates the best models computed so far if needed
      if (is.null(best_model) || sum_of_llhs == min(llhs)){
        best_model <- EM
      }
    }
    
    # Computes the likelihood on the test set given the model
    # with the highest performance on the validation set
    dists = mostMatchingDistribution(
        best_model$n_clusters, 
        x_test,
        best_model$prop, 
        best_model$means, 
        best_model$sigma
        )
    
    # Records the resulting log-likelihood
    llhs = append(llhs, sum(apply(dists$likelihoods, 1, logSumExp)))
    
    # Records the mean llhs achieved with the cluster parameter
    print(paste("Mean log-likelihood achieved with ", cl, " clusters: ",
                round(mean(llhs), 4)))
    mean_cluster_criteria = append(mean_cluster_criteria, mean(llhs))
  }
  
  # Finds which cluster had the best performance
  best_cluster = which.max(mean_cluster_criteria)
  print(paste("The best result is achieved with ", 
            best_cluster+2, 
            " clusters (double CV)."))
  
  # return statement
  return(mean_cluster_criteria)
  
}

```

#### 2.7. ggplot2 plotting

```{r}

plotData <- function(
  x, clustering, mean_clusters,
  t="K-Means", xl="Alcohol", yl="Fixed Acidity"
) {
  ######
  #### Plots a cloud of point with clustering ellipses.
  #####
  
  # Formats the input data as a dataframe
  df = data.frame(x)
  mean_clusters = data.frame(mean_clusters)
  names(mean_clusters) = names(df)
  
  # Formats the clustering labels as factors for coloring
  colors = as.factor(clustering)
  
  # Declares the plot
  p = ggplot(df, aes_string(names(df)[1], names(df)[2], color=colors)) + 
    geom_point() +
    stat_ellipse(geom="polygon", aes(fill=colors), alpha=0.05) + 
    guides(fill = "none") + 
    labs(color="Wine Type", 
         title=paste("Clustering obtained via", t), 
         x=xl,y=yl) + 
    geom_point(data=mean_clusters, color="black")
  
  # return statement 
  return(p)
}

```

# 3 - Comparing Expectation-Maximization and K-Means Algorithms

#### 3.1.1. Computing and plotting the results (Full Dataset case)

We compute the EM (with and without KM initialization) and KM algorithms on the whole dataset. We set 3, which we know as the number of wine types in the dataset, as the number of clusters.

```{r}

n_clusters = 3
min_clusters = length(unique(y))
max_clusters = 10

```

```{r}

resultsKM = kMeans(X, n_clusters)

```

```{r}

resultsEMwithoutKM = expectationMaximization(X, n_clusters)

```

```{r}

resultsEMwithKM = expectationMaximization(X, n_clusters, kmInit=T)

```

We display the final clustering obtained via the single KM:

```{r}

# non-ggplot implementation without ellipses
# plot(X, col=resultsKM$closest_centroids + 1, pch=19)
# points(resultsKM$centroids, col=1, pch=10)
# title(main="Clustering obtained via K-Means")

plotData(X, resultsKM$closest_centroids, resultsKM$centroids)

```

We display the final clustering obtained via the EM algorithm (with and without KM initialization):

```{r}

# non-ggplot implementation without ellipses
# plot(X, col=resultsEMwithoutKM$clustering+1, pch=19)
# points(resultsEMwithoutKM$means, col=1, pch=10)
# title(main="Clustering obtained via EM (without K-Means init.)")

plotData(X, resultsEMwithoutKM$clustering, resultsEMwithoutKM$means,
         t="EM (without K-Means init.)")

```

```{r}

# non-ggplot implementation without ellipses
# plot(X, col=resultsEMwithKM$clustering+1, pch=19)
# points(resultsEMwithKM$means, col=1, pch=10)
# title(main="Clustering obtained via EM (with K-Means init.)")

plotData(X, resultsEMwithKM$clustering, resultsEMwithKM$means, 
         t="EM (with K-Means init.)")

```

<u>Observations:</u>

In general, we observe that the EM with random initialization performs differently from its KM-initialized counterpart, or the simple KM. The KM initialization strongly informs the convergence of the EM algorithm towards a local optimum. 

At first glance, and given our knowledge of the Wine dataset, the EM with random initialization seem to perform better overall -- to be verified in the latter subsections.

#### 3.1.2. Computing and plotting the results (Train-Test Split)

We compute the EM and KM algorithms on the training data. We set 3, which we know as the number of wine types in the dataset, as the number of clusters.

```{r}

resultsKM_train = kMeans(X_train, n_clusters)

```

```{r}

resultsEM_train_withoutKM = expectationMaximization(X_train, n_clusters)

```

```{r}

resultsEM_train_withKM = expectationMaximization(X_train, n_clusters, kmInit=T)

```

We perform the clustering on the test set using the results obtained with the algorithms on the train set.

```{r}

# KM on test set
test_resultsKM = findClosestCentroids(
  X_test, 
  resultsKM_train$centroids,
  n_clusters
  )
test_resultsKM$closest = apply(test_resultsKM$closest, 1, which)

# EM with random init. on test set
test_resultsEM_withoutKM = mostMatchingDistribution(
  n_clusters, 
  X_test, 
  resultsEM_train_withoutKM$prop, 
  resultsEM_train_withoutKM$means, 
  resultsEM_train_withoutKM$sigma
  )

# EM with KM init. on test set
test_resultsEM_withKM = mostMatchingDistribution(
  n_clusters, 
  X_test, 
  resultsEM_train_withKM$prop, 
  resultsEM_train_withKM$means, 
  resultsEM_train_withKM$sigma
  )

```

We display the final clustering obtained on the test set via the single KM:

```{r}

# non-ggplot implementation without ellipses
# plot(X_test, col=test_resultsKM$closest+1, pch=19)
# points(resultsKM_train$centroids, col=1, pch=10)

plotData(X_test, test_resultsKM$closest, resultsKM_train$centroids, 
         t="K-Means, test set")

```

We display the final clustering obtained on the test set via the EM algorithm (with and without KM initialization):

```{r}

# non-ggplot implementation without ellipses
# plot(X_test, col=test_resultsEM_withoutKM$clustering+1, pch=19)
# points(resultsEM_train_withoutKM$means, col=1, pch=10)
# title(main="Test set clustering obtained via EM (without K-Means init.)")

plotData(X_test, test_resultsEM_withoutKM$clustering, 
         resultsEM_train_withoutKM$means, 
         t="EM, test set, (without K-Means init.)")

```

```{r}

# non-ggplot implementation without ellipses
# plot(X_test, col=test_resultsEM_withKM$clustering+1, pch=19)
# points(resultsEM_train_withKM$means, col=1, pch=10)
# title(main="Test set clustering obtained via EM (with K-Means init.)")

plotData(X_test, test_resultsEM_withKM$clustering, 
         resultsEM_train_withKM$means, 
         t="EM, test set, (with K-Means init.)")

```

<u>Observations:</u>

As with using the full dataset, we observe that the EM with random initialization performs slightly differently from its KM-initialized counterpart, or the simple KM. We can still state, however, that the EM with the KM initialization still moves closer to its random-initialized version (**Note**: in some occasions, the two solutions converges to a very similar state). 

As such, the KM initialization still seem to generally inform the convergence of the EM algorithm towards a local optimum, albeit in a lesser manner.

A implementation-wise observation is that, given the small amount of data points, the ggplot2 display of the clustering might fail to produce the 3 expected ellipses as a clustering might end up with two few elements. 

#### 3.2.1 Classification Error when clustering the whole dataset

Relying on the **mclust** library, we compute the [classError](https://mclust-org.github.io/mclust/reference/classError.html) to assess the quality of our clustering. The **Classification Error** corresponds to the rate of a given classification relative to the known classes, and the location of misclassified data points, i.e. "*the lower the better*."

<u>K-Means:</u>

```{r}

classError(resultsKM$closest_centroid, y)

```

<u>Expectation-Maximization:</u>

For the case with a random initialization:

```{r}

classError(resultsEMwithoutKM$clustering, y)

```

For the case with a k-means initialization:

```{r}

classError(resultsEMwithKM$clustering, y)

```

#### 3.2.2 Classification Error when clustering the test set (after training)


<u>K-Means:</u>

```{r}

classError(test_resultsKM$closest, y_test)

```

<u>Expectation-Maximization:</u>

For the case with a random initialization:

```{r}

classError(test_resultsEM_withoutKM$clustering, y_test)

```

For the case with a k-means initialization:

```{r}

classError(test_resultsEM_withKM$clustering, y_test)

```

#### 3.3.1. Adjusted Rand Index when clustering the whole dataset

Relying on the **mclust** library, we compute the [Adjusted Rand Index](https://mclust-org.github.io/mclust/reference/adjustedRandIndex.html) to assess the quality of our clustering. The **Adjusted Rand Index** compares two partitions of values with a resulting score between 0 and 1, with zero indicating randomness between the two and 1 indicating a perfect match, i.e. "*the higher the better*."

<u>K-Means:</u>

```{r}

adjustedRandIndex(resultsKM$closest_centroid, y)

```

<u>Expectation-Maximization:</u>

For the case with a random initialization:

```{r}

adjustedRandIndex(resultsEMwithoutKM$clustering, y)

```

For the case with a k-means initialization:

```{r}

adjustedRandIndex(resultsEMwithKM$clustering, y)

```

#### 3.3.2. Adjusted Rand Index when clustering the test set (after training)

<u>K-Means:</u>

```{r}

adjustedRandIndex(test_resultsKM$closest, y_test)

```

<u>Expectation-Maximization:</u>

For the case with a random initialization:

```{r}

adjustedRandIndex(test_resultsEM_withoutKM$clustering, y_test)

```

For the case with a k-means initialization:

```{r}

adjustedRandIndex(test_resultsEM_withKM$clustering, y_test)

```

#### 3.4. Comments and observations

In general, we observe that the EM algorithm performs better than the KM algorithm. Furthermore we observe that the random initialization performs better in general compared to a KM initialization (in the present bivariate case). 

As the KM provides somewhat already locally-optimized centroids, it is fair to suppose that the EM algorithm with KM initialization does not update the mixture parameters as much in the same amount of iterations as its random initialization counterpart (The EM algorithm still improves on the performance of the KM alone).

These observations are underlined by our computed metrics (class error and adjusted rand index) where the EM algorithm obtains better results (lower class error and higher adjusted rand index) compared to the KM algorithm.

<u>Example of metrics obtained on October 29th:</u>

| algorithm | class error | adjusted rand index |
| --- | --- | --- |
| EM (full dataset, random init.) | 0.29 | 0.33 |
| EM (full dataset, KM init.) | 0.48 | 0.16 |
| EM (test dataset, random init.) | **0.25** | **0.37** |
| EM (test dataset, KM init.) | 0.46 | 0.18 |
| KM (full dataset) | 0.49 | 0.15 |
| KM (test dataset) | 0.48 | 0.18 |

# 4 - Performing Model Selection with the EM algorithm

To perform model selection, we decide to increment the number of clusters from 3 to 10 included and check each resulting EM model's performance with three different criteria: Akaike Information Criterion, Bayesian Information Criterion, and the Double Cross-Validation heuristic. These three approaches will inform our selection approach.

Given our previous results, we decide to use the EM algorithm with random initialization.

#### 4.1. Using the Akaike Information Criterion

```{r}

computeAIC(X)

```

#### 4.2. Using the Bayesian Information Criterion

```{r}

computeBIC(X)

```

#### 4.3. Using Cross-Validated Likelihood

We implement a double cross-validation with 10 folds and a maximum cluster parameter of 10.


```{r}

cv_logLikelihoods = doubleCrossValidation(X_train, X_test) 

```

#### 4.4. Comments and observations

We find that the Akaike Information Criterion is unstable compared to the Bayesian Information Criterion and the Cross-Validated Likelihood. Furthermore, the AIC consistently prefers models with a higher number of clusters.

Meanwhile the BIC and double cross-validation generally yield 3 as the most optimal number of clusters. As expected, the BIC penalizes a number of clusters that is higher than necessary compared to the AIC. This results in the BIC yielding a result closer to the ground truth.

# 5 - Testing with higher-dimensional spaces

We operate in two steps. We first hand-pick a select few features to observe whether we obtain any change compared to the previous bivariate case. Then we sample feature sets at random and iterate over them with the EM algorithm.

#### 5.1. Hand-selected features

As part of our first, hand-picked case, we selected the first 3 features of the dataset. They correspond to selecting the variables 2, 3, and 4 of the Wine dataset.

```{r}

Xhd = as.matrix(wine[,c(2:4)])

```

<u>Computing the BIC on the whole dataset:</u>

We compute the best cluster number on the whole dataset using the BIC.

```{r}

computeBIC(Xhd)

```

<u>Computing the Class Error and Adjusted Rand Index in the train/test case:</u>

As we find with the BIC that the best result is again obtained with a cluster number of 3 clusters, we try to apply the EM algorithm on a train split of the 3-feature dataset. Then, we will check the resulting clustering on the test split.

```{r}

# we reuse the randomized indexes computed in 1.4
randomized_Xhd = Xhd[randomized_indexes,]
randomized_yhd = y[randomized_indexes]
Xhd_train = randomized_Xhd[1:training_split,]
Xhd_test = randomized_Xhd[(training_split+1):n,]
yhd_train = randomized_yhd[1:training_split]
yhd_test = randomized_yhd[(training_split+1):n]

resultsEMhd_train = expectationMaximization(Xhd_train, 3)

test_resultsEMhd = mostMatchingDistribution(
  3, 
  Xhd_test, 
  resultsEMhd_train$prop, 
  resultsEMhd_train$means, 
  resultsEMhd_train$sigma
  )

```

Once done with the EM iteration, we visualize the resulting clustering on the test set by focusing on the two features we previously used in the bivariate case:

```{r}

# non-ggplot implementation without ellipses
# plot(Xhd_test[,c(1,3)], col=test_resultsEMhd$clustering+1, pch=19)
# points(resultsEMhd_train$means[,c(1,3)], col=1, pch=10)

plotData(Xhd_test[,c(1,3)], test_resultsEMhd$clustering, 
         resultsEMhd_train$means[,c(1,3)], 
         t="EM, test set, (without K-Means init.)")

```

We then compute the class error:

```{r}

classError(test_resultsEMhd$clustering, yhd_test)

```

As well as the adjusted rand index:

```{r}

adjustedRandIndex(test_resultsEMhd$clustering, yhd_test)

```

<u>Comments and observations:</u>

Going through multiple trials of the present EM algorithm with 3 features, we find similar results as with the bivariate case, that is: the most efficient number of clusters is 3. 

This result is expected as we know there are 3 types of wine in the dataset. Having a proof through the application of the EM algorithm is reassuring.

#### 5.2.1. Randomly selected features

As previously mentioned, we also want to test a number of different higher-dimensional sets of features. 

To do so, we randomly generate five 10-element permutations and five 5-element permutations from the dataset (out of the 27 available feature columns). From the latter set of 5-element permutations, we build the corresponding sets of 3-element, and 4-element sub-permutations.

As such, the resulting number of generated permutations is:

- 5 10-elem. permutations, 5 5-elem. permutations, 600 4-elem. permutations, 14400 3-elem. permutations.

Given this high number of permutations, we only randomly select 20 permutations out of the 4- and 3-element permutations sets respectively. 

As such, we will run the EM algorithm on 50 different sets of features in dimensions 10, 5, 4, and 3.

```{r}

# We extract the features from the original dataset
wine_features = subset(wine, select=-c(Type))

# We produce 5 10-element permutations by random sampling
ten_perms = c()
for (p in 1:5) {
  ten_perms = rbind(ten_perms, sample(colnames(wine_features), 10))
}

# We produce 5 5-element permutations by random sampling
five_perms = c()
for (p in 1:5) {
  five_perms = rbind(five_perms, sample(colnames(wine_features), 5))
}

# We produce the set of 4-element permutations
four_perms = NULL
for (p in 1:nrow(five_perms)) {
  all <- expand.grid(p1 = five_perms[p,], 
                     p2 = five_perms[p,], 
                     p3 = five_perms[p,], 
                     p4 = five_perms[p,],
                     stringsAsFactors = FALSE)
  perms <- all[apply(all, 1, function(x) {length(unique(x)) == 4}),]
  if (is.null(four_perms)) {
    four_perms = perms
  } else {
    four_perms = rbind(four_perms, perms)
  }
}
four_perms = as.matrix(four_perms)

# We produce the set of 3-element permutations
three_perms = NULL
for (p in 1:nrow(four_perms)) {
  all <- expand.grid(p1 = four_perms[p,], 
                     p2 = four_perms[p,], 
                     p3 = four_perms[p,], 
                     stringsAsFactors = FALSE)
  perms <- all[apply(all, 1, function(x) {length(unique(x)) == 3}),]
  if (is.null(three_perms)) {
    three_perms = perms
  } else {
    three_perms = rbind(three_perms, perms)
  }
}
three_perms = as.matrix(three_perms)

```

We remove the duplicated permutations if need be:

```{r}

ten_perms = ten_perms[!duplicated(apply(ten_perms, 2, sort)),]
five_perms = five_perms[!duplicated(apply(five_perms, 2, sort)),]
four_perms = four_perms[!duplicated(apply(four_perms, 1, sort)),]
three_perms = three_perms[!duplicated(apply(three_perms, 1, sort)),]

```

#### 5.2.2. Computing the AIC and BIC on the 10-element permutations

We compute the best number of clusters via the use of the AIC and BIC with the following features of the wine dataset.

```{r}

for (perm in 1:nrow(ten_perms)) {
  cat("\nSelected features: ", paste(ten_perms[perm,], collapse=", "), "\n")
  computeAIC(as.matrix(wine_features[ten_perms[perm,]]), print_steps=FALSE)
}

```

```{r}

for (perm in 1:nrow(five_perms)) {
  cat("\nSelected features: ", paste(ten_perms[perm,], collapse=", "), "\n")
  computeBIC(as.matrix(wine_features[ten_perms[perm,]]), print_steps=FALSE)
}

```

<u>Comments:</u>

As previously seen, we see that the AIC yields higher and different cluster numbers per given set of permutated features. However, it seems that the BIC also starts yielding a higher number of clusters as a result.

We might want to hypothesize that the EM algorithm is less efficient in higher dimensions for the Wine dataset. Indeed, we are trying to split a population of 3 types using an increasingly more complex space.

#### 5.2.3. Computing the AIC and BIC on the 5-element permutations

We compute the best number of clusters via the use of the AIC and BIC with the following features of the wine dataset.

```{r}

for (perm in 1:nrow(five_perms)) {
  cat("\nSelected features: ", paste(five_perms[perm,], collapse=", "), "\n")
  computeAIC(as.matrix(wine_features[five_perms[perm,]]), print_steps=FALSE)
}

```

```{r}

for (perm in 1:nrow(five_perms)) {
  cat("\nSelected features: ", paste(five_perms[perm,], collapse=", "), "\n")
  computeBIC(as.matrix(wine_features[five_perms[perm,]]), print_steps=FALSE)
}

```

<u>Comments:</u>

As previously seen, we see that the AIC yields higher and different clusters per given sets of permutated features. As was the case for the 10-element permutations, we observe that depending on the permutations, the BIC also yields a higher number of clusters. 

However, we tend to find on average a lower number of clusters than with 10-element permutations.

Here, we might repeat our hypothesis: that the EM algorithm is indeed less efficient in higher dimensions as we are trying to split a population of 3 types using an increasingly more complex space. There seems to be a sweet spot in the case of the Wine dataset where a lower number of features results in a more efficient separation of the data into its ground truth clusters.

Of note, this lack of performance at a higher dimension seems to also be highly dependent on the selected features.

#### 5.2.4. Computing the BIC on the 4-element permutations

```{r}

for (perm in sample(1:nrow(four_perms), 20)) {
  cat("\nSelected features: ", paste(four_perms[perm,], collapse=", "), "\n")
  computeBIC(as.matrix(wine_features[four_perms[perm,]]), print_steps=FALSE)
}

```

<u>Comments:</u>

Compared to the 10-element and 5-element permutations, and only using the BIC, we find that a larger number of the runs yield 3 as the best number of clusters. This reinforce our hypothesis.

#### 5.2.5. Computing the BIC on the 3-element permutations

```{r}

for (perm in sample(1:nrow(three_perms), 20)) {
  cat("\nSelected features: ", paste(three_perms[perm,], collapse=", "), "\n")
  computeBIC(as.matrix(wine_features[three_perms[perm,]]), print_steps=FALSE)
}

```

<u>Comments:</u>

In most runs, we mostly yield 3 as the best number of clusters.

#### 5.2.6. Overall observations

Overall, it seems that the higher the feature dimension we iterate over with the EM algorithm, the higher the resulting number of clusters we find (even when using the BIC criterion which should penalize more complex model).

Though at this stage it remains an hypothesis, we can suppose that the EM algorithm, which usually finds a local optimum, might struggle in a higher dimensional space with the Wine dataset as we know the ground truth to be 3 clusters. 

As such, in the case of the Wine dataset, the EM algorithm may yield non-optimum results when iterated over high-dimensional data.

As such, and given these experimental results, we might want to inform any feature heuristic by keeping our feature selection to a low number (i.e. as close to the ground truth as possible). 

<hr>

# 6 - Known issues with the current implementation

### 6.1. Assymetric matrix error

This error may occur on ``computeBIC`` or ``expectationMaximization`` calls for instance. The functions may crash due to the randomized aspect of the algorithm process resulting in an asymmetric matrix (This happens when a cluster is computed to only contain a single datapoint).

#### Error message
  
> (max(abs(sigma - t(sigma))) > sqrt(.Machine$double.eps)) stop("sigma must be a symmetric matrix") : missing value where TRUE/FALSE needed

### 6.2. EM random initialization fail:

This error occurs on ``expectationMaximization``. Sometimes the random initialization of the ``expectationMaximization`` function fails.

#### Error message 

> Error in FUN(X[[i]], ...) : 'dimnames' applied to non-array

### 6.3. Crash when editing the code while running a chunk

Knitting or running a chunk may crash with the error below in the RStudio environment if the user edits the Rmd document (even without saving, even if only the mardown areas are updated). It is advised not to interact with the editing environment while running any code.

#### Error message

> Error in FUN(X[[i]], ...) : object[[i]] not found

### 6.4. Overwritten methods by conflicting packages

This warning may occur on the ``computeAIC`` and ``computeBIC`` calls for instance This is caused when multiple packages have the same named function.

#### Warning message

> Registered S3 methods overwritten by 'htmltools': method from print.html tools:rstudio, print.shiny.tag tools:rstudio, print.shiny.tag.list tools:rstudio^