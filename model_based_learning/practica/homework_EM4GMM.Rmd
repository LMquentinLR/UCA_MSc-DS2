---
title: "Tutorial Expectation-Maximization for Gaussian Mixture Models"
output:
  html_document:
    df_print: paged
---
  
The goal is to apply EM for multivariate GMMs on the Wine dataset, available in the `pgmm` package.

### Instructions

1) **Implementing the EM**

- Implement from scratch the EM algorithm for a GMM on the variables 2 and 4 of the wine data set.
- Cluster the data and compare your results with k-means.
- To assess the quality of the clustering, you may use the function classError and/or adjustedRandIndex from the Mclust package.


2) **Model selection**

- Try to find a relevant number of clusters using the three methods seen in class: AIC, BIC, and (cross-)validated likelihood.

3) **Towards higher dimensional spaces**

- Try to model more than just two variables of the same data set. Do you find the same clusters, the same number of clusters.

<hr>

# 1 - Library and dataset imports

#### 1.1. Library imports

```{r  echo=TRUE, results='hide', message=FALSE}
# we'll use this package to load the data
#install.packages("pgmm"); install.packages('mvtnorm'); install.packages('mclust')
library(pgmm); library(mvtnorm); library(mclust)
```

### 1.2. Dataset import

Once the package are loaded, we can load the data in the environment.
```{r}
data(wine)
head(wine)
```

We extract the variables 2 and 4 of the wine dataset, plus the labels of each entry.

```{r}
X = as.matrix(wine[,c(2,4)])
y = wine$Type
```

#### 1.3. Dataset check

```{r}
print(paste("Feature dimensions: ", paste(dim(X), collapse= ", ")))
print(paste("feature names: ", paste(colnames(X), collapse=", ")))
```

<u>Plots of the data (178 2-dimensional data points):</u>

```{r}
plot(X, col = y+1, pch=19)
```

#### 1.4. Train/Test Split

We split the dataset, 2/3 for training and 1/3 for testing.

```{r}

n = dim(X)[1]
min_n_clusters = 2 #length(unique(y))
max_n_clusters = 10

training_split = round(2/3*n,0)

randomized_indexes = sample(1:nrow(X))
randomized_X = X[randomized_indexes,]
randomized_y = y[randomized_indexes]

X_train = randomized_X[1:training_split,]
X_test = randomized_X[training_split:n,]
y_train = randomized_y[1:training_split]
y_test = randomized_y[training_split:n]

```


# 2 - Function declarations

#### 2.1. Generic functions

This code is reproduced from the class.

```{r}

log_gaussian_density <- function (x, mean, sigma) {
  ######
  #### Computes the log-density probability of a variable estimated to follow
  #### a gaussian distribution.
  ######
  distval <- mahalanobis(x, center = mean, cov = sigma)
  logdet <- determinant(sigma,logarithm = TRUE)$modulus
  loglikelihood <- -(ncol(x) * log(2 * pi) + logdet + distval)/2
  # return statement
  return(loglikelihood)
}

logSumExp <- function (x) {
  ######
  #### Computes the log-sum-exponential of a vector, list of variables.
  ######
  y = max(x)
  y + log(sum(exp(x - y)))
}

logNorm <- function (x) {
  ######
  #### Log-normalizes a vector, list of variables.
  ######
  logratio = log(x) - logsumexp(log(x))
  exp(logratio)
}

```


#### 2.2. Expectation-Maximization algorithm 

```{r}

logGamma <- function(n, clusters, x, prop, mu, sigma) {
  ######
  #### Precomputes the log-value of gamma as part of the EM algorithm.
  ######
  
  # Computes the gammas per clusters
    lgn = matrix(nrow=n, ncol=clusters)
    for (cl in 1:clusters){
      lgn[,cl] = log(prop[cl]) + matrix(dmvnorm(x, mu[cl,], sigma[[cl]], log=TRUE))
    }
    
    # return statement
    return(lgn)
}

mostMatchingDistribution <- function(clusters, x, prop, mu, sigma){
  ######
  #### Given a set of points, find the most matching distribution for them.
  #### to be used for a test set.
  ######
  
  # Computes the likelihood of belonging to a distribution
  likelihoods = logGamma(dim(x)[1], clusters, x, prop, mu, sigma)
  distributions = findCluster(likelihoods)
  
  # return statement
  return(list("likelihoods" = likelihoods, "distributions" = distributions))
}

findCluster <- function(likelihoods) {
  ######
  #### Given an array of likelihood with each column representing the 
  #### likelihood of belonging to a specific cluster, finds the most
  #### matching cluster.
  ######
  
  # Computes the distribution to which each point belongs
  distributions = (likelihoods == apply(likelihoods, 1, max))
  distributions = apply(distributions, 1, which)
  
  # Return statement
  return(distributions)
}

expectationMaximization <- function(x, clusters, maxIterations=50) {
  ######
  #### Implementation of an Expectation Maximization algorithm for multivariate
  #### data, here the wine dataset provided by the pgmm library.
  ######
  
  # Variable declarations and initializations
  n = dim(x)[1]
  K = dim(x)[2]
  prop = rep(1/clusters, clusters)
  mu = rmvnorm(clusters, colMeans(x), cov(x))#mean=apply(x, K, mean), sigma=diag(K))
  sigma = lapply(1:clusters, function(i) diag(K))

  # Algorithm loop
  for (loop in 1:maxIterations){
    
    #### Expectation step
    # Computes the new gamma values
    gam = logGamma(n, clusters, x, prop, mu, sigma)
    # Normalizes the gamma rows
    gam = gam - apply(gam, 1, logSumExp)
    gam = exp(gam)
    
    #### Maximization step
    for (cl in 1:clusters) {
      nk = sum(gam[,cl])
      # Parameter updates
      prop[cl] = nk/n
      mu[cl,] = Reduce("+", lapply(1:n, function(i) gam[i,cl]*x[i,]))/nk
      sigma[[cl]] = Reduce(
        "+",
        lapply(1:n,function(i) gam[i,cl]*(x[i,]-mu[cl,])%*%t(x[i,]-mu[cl,]))
        )/nk
    }
  }
  
  # Computes the llh results for the given data
  llh_results = mostMatchingDistribution(clusters, x, prop, mu, sigma)
  
  # return statement
  ret = list(
    "n_wines" = n,
    "n_clusters" = clusters,
    "cl_gamma" = gam,
    "cl_proportions" = prop,
    "cl_means" = mu,
    "cl_sigma" = sigma,
    "distributions" = llh_results$distributions,
    "logLikelihoods" = llh_results$likelihoods,
    "sum_of_llh" = sum(llh_results$likelihoods)
    )
  return(ret)
}
```


#### 2.3. K-Means Algorithm

```{r}

generateCentroid <- function(x) {
  ######
  #### Generates a random centroid from a given dataset.
  ######
  
  # Variable declarations and initializations
  K = dim(x)[2]
  centroid = c()
  
  # Randomly draws a value for each dimension of the dataset
  centroid = rmvnorm(1, colMeans(x), cov(x))
  colnames(centroid) <- NULL
  
  # return statement
  return(centroid)
}

euclidianDistance <- function(x, centroid){
  ######
  #### Computes the euclidian distance between a datapoint and a centroid.
  ######
  
  # Function declaration
  dist = function(x, y) (x-y)^2
  
  # Computes the distances
  distances = t(apply(x, 1, 
                      function(i) sqrt(sum(mapply(dist, i, centroid))))
                )
  
  # Return statement
  return(distances)
}

findClosestCentroids <- function(x, centroids, clusters){
  ######
  #### Computes the closest centroids for a given set of datapoints.
  ######
  
  # Variable declarations and initializations
  n = dim(x)[1]
  K = dim(x)[2]
  
  # Computes the distances
  distances = c()
  for (cl in 1:clusters){
    cluster_dist = euclidianDistance(x, centroids[cl,])
    distances = append(distances, cluster_dist)
  }
  distances = array(distances, dim=c(n, clusters))
  
  # Finds the closest centroids
  closest = (distances == apply(distances, 1, min))
  
  # Return statement
  return(list("distances"=distances, "closest"=closest))
    
}

kMeans <- function(x, clusters, maxIterations=50) {
  ######
  #### Implementation of a K-Means algorithm for multivariate data, here 
  #### the wine dataset provided by the pgmm library.
  ######
  
  # Variable declarations and initializations
  n = dim(x)[1]
  K = dim(x)[2]
  centroids = t(lapply(1:clusters, function(i) generateCentroid(x)))
  centroids = matrix(unlist(centroids), ncol=K, byrow=TRUE)
  
  # Computes the euclidian distances between every point and each centroid
  for (loop in 1:maxIterations){
    
    # Computes the closest centroids
    closest_centroids = findClosestCentroids(x, centroids, clusters)
   
    # Computes the new closest centroids
    centroids = c()
    for (cl in 1:clusters) {
      centroids = rbind(centroids, colMeans(x[closest_centroids$closest[,cl],]))
    }
    
    # Erases the matrix's columns and rows names inherited from the dataset
    colnames(centroids) <- NULL
    rownames(centroids) <- NULL
  }
  
  #return statement
  ret = list(
    "centroids" = centroids,
    "distances" = closest_centroids$distances,
    "closest_centroids" = apply(closest_centroids$closest, 1, which)
   )
  return(ret)
}

```


#### 2.4. Akaike Information Criterion

```{r}

akaikeIC <- function(logLikelihood, clusters, K) {
  ######
  #### Implementation of the the Akaike Information Criterion such that:
  #### AIC = LLH - eta(M)
  #### i.e. the final log-likelihood of a model minus the number of free
  #### scalar parameters in the model (nb of proportions (-1 as there are
  #### only cluster-1 degrees of freedom) + nb of means + nb of sigmas)
  ######
  penalization = (clusters-1) + clusters*K + clusters*((K*(K+1))/2)
  return(logLikelihood - penalization)
}

computeAIC <- function(x, max_cluster=max_n_clusters, print_steps=TRUE){
  ######
  #### Computes the AIC of an EM algorithm implementation given a dataset
  ######
  
  # Variable declaration
  akaike_results = c()
  
  # Loops through each cluster parameter to compute the corresponding AIC
  for (j in min_n_clusters:max_cluster){
    res = expectationMaximization(x, clusters=j, maxIterations = 50)
    akaike_value = akaikeIC(res$sum_of_llh, j, dim(x)[2])
    akaike_results = append(akaike_results, akaike_value)
    if (print_steps) {
      print(paste("Total log-likelihood with ", j, " clusters: ",
                  round(akaike_value, 3)))
    }
  }
  
  # Prints the result
  print(paste("The best result is achieved with ", 
              which.max(akaike_results)+2, 
              " clusters."))
}

```


#### 2.5. Bayesian Information Criterion

```{r}

bayesianIC <- function(logLikelihood, clusters, n, K) {
  ######
  #### Implementation of the Bayesian Information Criterion such that:
  #### BIC = LLH - 1/2*eta(M)*log(n)
  #### i.e. the final log-likelihood of a model minus the half of the number
  #### of free scalar parameters in the model (nb of proportions (-1 as there
  #### are only cluster-1 degrees of freedom) + nb of means + nb of sigmas)
  #### then multiplied by the log of the number of datapoints
  ######
  
  # Variable declaration
  penalization = (clusters-1) + clusters*K + clusters*((K*(K+1))/2)
  return(logLikelihood - 1/2 * penalization * log(n))
}

computeBIC <- function(x, max_cluster=max_n_clusters, print_steps=TRUE){
  ######
  #### Computes the BIC of an EM algorithm implementation given a dataset
  ######
  
  # Variable declaration
  bayesian_results = c()
  
  # Loops through each cluster parameter to compute the corresponding AIC
  for (j in min_n_clusters:max_cluster){
    res = expectationMaximization(x, clusters=j, maxIterations = 50)
    bayesian_value = bayesianIC(res$sum_of_llh, j, dim(x)[1], dim(x)[2])
    bayesian_results = append(bayesian_results, bayesian_value)
    if (print_steps) {
      print(paste("Total log-likelihood with ", j, " clusters: ", 
                  round(bayesian_value, 3)))
    }
  }

  # Prints the result
  print(paste("The best result is achieved with ", 
              which.max(bayesian_results)+2, 
              " clusters."))
}

```


#### 2.6. (Cross-)Validated Likelihood

```{r}

doubleCrossValidation <- function(x_train, x_test, folds=10, max_cluster=max_n_clusters) {
  ######
  #### Implements a double cross-validation with the resulting log-likelihood
  #### being the selection criteria.
  ######
  
  # Variable declaration
  n_train = dim(x_train)[1]
  fold_indexes = split(c(1:n_train), 
                       ceiling(seq_along(c(1:n_train))/(n_train/10)))
  mean_cluster_criteria = c()
  
  # iterates over the cluster range
  for (cl in min_n_clusters:max_cluster){
    # Performs the first step of the double cross-validation: iteration over the
    # folds of the training set
    llhs = c()
    best_model = NULL
    for (kFold in fold_indexes){
      x_train_train = x_train[-kFold,]
      x_train_val = x_train[kFold,]
      resultsEM = expectationMaximization(x_train_train, cl)
      matching_distribution = mostMatchingDistribution(
        resultsEM$n_clusters, 
        x_train_val, 
        resultsEM$cl_proportions, 
        resultsEM$cl_means, 
        resultsEM$cl_sigma
        )
      # Records the resulting log-likelihood
      sum_of_llhs = sum(matching_distribution$likelihoods)
      llhs = append(llhs, sum_of_llhs)
      if (is.null(best_model) || sum_of_llhs == min(llhs)){
        best_model <- resultsEM
      }
    }
    
    # Computes the likelihood on the test set given the best achieved model
    matching_distribution = mostMatchingDistribution(
        best_model$n_clusters, 
        x_test,
        best_model$cl_proportions, 
        best_model$cl_means, 
        best_model$cl_sigma
        )
    llhs = append(llhs, sum(matching_distribution$likelihoods))
    
    # Records the mean llhs achieved with the cluster
    print(paste("Mean log-likelihood achieved with ", cl, " clusters: ",
                round(mean(llhs),4)))
    mean_cluster_criteria = append(mean_cluster_criteria, mean(llhs))
  }
  
  # Finds which cluster had the best performance
  best_cluster = which.max(mean_cluster_criteria)
  print(paste("The best result is achieved with ", 
            best_cluster+2, 
            " clusters."))
  # return statement
  return(mean_cluster_criteria)
  
}

```


# 3 - Comparing EM and K-Means

#### 3.1.1. Computing the results (Full Dataset)

We compute our results with the EM and K-Means algorithms on the full dataset.

```{r}
n_clusters = 3

resultsEM = expectationMaximization(X, n_clusters)
resultsKM = kMeans(X, n_clusters)
```

We display the final clustering for the EM algorithm:

```{r}
plot(X, col=resultsEM$distributions+1, pch=19)
points(resultsEM$cl_means, col=1, pch=10)
```

We display the final clustering for the K-Means algorithm:

```{r}
plot(X, col=resultsKM$closest_centroids + 1, pch=19)
points(resultsKM$centroids, col=1, pch=10)
```


#### 3.1.2. Computing the results (Train-Test Split)

We compute our results with the EM and K-Means algorithms on the training set.

```{r}
resultsEM_train = expectationMaximization(X_train, n_clusters)
resultsKM_train = kMeans(X_train, n_clusters)
```

Then we test our clustering results on the test dataset.

```{r}
test_resultsEM = mostMatchingDistribution(
  n_clusters, 
  X_test, 
  resultsEM_train$cl_proportions, 
  resultsEM_train$cl_means, 
  resultsEM_train$cl_sigma
  )

test_resultsKM = findClosestCentroids(
  X_test, 
  resultsKM_train$centroids,
  n_clusters
  )

test_resultsKM$closest = apply(test_resultsKM$closest, 1, which)
```

We display the final clustering for the EM algorithm:

```{r}
plot(X_test, col=test_resultsEM$distributions+1, pch=19)
points(resultsEM_train$cl_means, col=1, pch=10)
```

We display the final clustering for the K-Means algorithm:

```{r}
plot(X_test, col=test_resultsKM$closest+1, pch=19)
points(resultsKM_train$centroids, col=1, pch=10)
```

Relying on the **mclust** library, we compute the classError and adjustedRandIndex to assess the quality of our clustering.

#### 3.2.1 Classification Error when clustering the whole dataset

Relying on the **mclust** library, we compute the [classError](https://mclust-org.github.io/mclust/reference/classError.html) to assess the quality of our clustering. The **Classification Error** corresponds to the rate of a given classification relative to the known classes, and the location of misclassified data points. I.e. Lower the better.

<u>Expectation-Maximization:</u>

```{r}
classError(resultsEM$distributions, y)
```

<u>K-Means:</u>

```{r}
classError(resultsKM$closest_centroid, y)
```


#### 3.2.2 Classification Error when clustering the test set (after training)

<u>Expectation-Maximization:</u>

```{r}
classError(test_resultsEM$distributions, y_test)
```

<u>K-Means:</u>

```{r}
classError(test_resultsKM$closest, y_test)
```


#### 3.3.1. Adjusted Rand Index when clustering the whole dataset

Relying on the **mclust** library, we compute the [Adjusted Rand Index](https://mclust-org.github.io/mclust/reference/adjustedRandIndex.html) to assess the quality of our clustering. The **Adjusted Rand Index** compares two partitions of values with a resulting score between 0 and 1, with zero indicating randomness between the two and 1 indicating a perfect match. I.e. Higher the better.

<u>Expectation-Maximization:</u>

```{r}
adjustedRandIndex(resultsEM$distributions, y)
```

<u>K-Means:</u>

```{r}
adjustedRandIndex(resultsKM$closest_centroid, y)
```


#### 3.3.2. Adjusted Rand Index when clustering the test set (after training)

<u>Expectation-Maximization:</u>

```{r}
adjustedRandIndex(test_resultsEM$distributions, y_test)
```

<u>K-Means:</u>

```{r}
adjustedRandIndex(test_resultsKM$closest, y_test)
```


#### 3.4. Comments and observations

When re-knitting/running the R markdown, it happens that, *in general*, the EM algorithm performs better than the K-Means (rarely, the reverse happens, which seems to be tied to the initialization of the centroids/parameters).

Sometimes, the K-Means ends up splitting the data points vertically rather than horizontally (this case results in the K-Means performing better than the EM algorithm.)

# 4 - Performing Model Selection with the EM algorithm

Starting with a cluster number of 2, we increase the number and check with different heuristics and criteria to select our best EM model.

#### 4.1. Using the Akaike Information Criterion

```{r}

computeAIC(X)

```


#### 4.2. Using the Bayesian Information Criterion

```{r}

computeBIC(X)

```


#### 4.3. Using Cross-Validated Likelihood

We implement a double cross-validation with 10 folds and a maximum cluster parameter of 10.


```{r}
cv_logLikelihoods = doubleCrossValidation(X_train, X_test) 
```


#### 4.4. Comments and observations

We find that the Akaike Information Criterion is unstable compared to the Bayesian Information Criterion and the Cross-Validated Likelihood. While the latter two consistently yield 2 as the best number of clusters, the AIC tends to favor a higher, and inconsistent, number of cluster. During our experiment, the AIC produced 9 as the best cluster number for instance.

# 5 - Testing with higher-dimensional spaces

#### 5.1. Hand-selected features

#### 5.2.1. Randomly selected features

To test a number of different higher-dimension sets of features, we generate five 5-elements permutations allowed by the dataset (out of 27 feature columns). From this set of 5-permutations, we build the corresponding sets of 3-, and 4- sub-permutations.

As such, our final selection of permutations would look something like:

- (a,b,c,d,e), (a,b,c,d), (a,b,c,e), (a,c,d,e), ..., (b,c,d), (c,d,e)

```{r}

# We extract the features from the original dataset
wine_features = subset(wine, select=-c(Type))

# We produce 5 5-permutations by random sampling
five_perms = c()
for (p in 1:5) {
  five_perms = rbind(five_perms, sample(colnames(wine_features), 5))
}

# We produce the set of 4-permutations
four_perms = NULL
for (p in 1:nrow(five_perms)) {
  all <- expand.grid(p1 = five_perms[p,], 
                     p2 = five_perms[p,], 
                     p3 = five_perms[p,], 
                     p4 = five_perms[p,],
                     stringsAsFactors = FALSE)
  perms <- all[apply(all, 1, function(x) {length(unique(x)) == 4}),]
  if (is.null(four_perms)) {
    four_perms = perms
  } else {
    four_perms = rbind(four_perms, perms)
  }
}
four_perms = as.matrix(four_perms)

# We produce the set of 3-permutations
three_perms = NULL
for (p in 1:nrow(four_perms)) {
  all <- expand.grid(p1 = four_perms[p,], 
                     p2 = four_perms[p,], 
                     p3 = four_perms[p,], 
                     stringsAsFactors = FALSE)
  perms <- all[apply(all, 1, function(x) {length(unique(x)) == 3}),]
  if (is.null(three_perms)) {
    three_perms = perms
  } else {
    three_perms = rbind(three_perms, perms)
  }
}
three_perms = as.matrix(three_perms)
```

We remove the duplicated permutations:

```{r}

five_perms = five_perms[!duplicated(apply(five_perms, 1, sort)),]
four_perms = four_perms[!duplicated(apply(four_perms, 1, sort)),]
three_perms = three_perms[!duplicated(apply(three_perms, 1, sort)),]

```

#### 5.2.2. Computing the BIC for the 5-permutations

```{r}
#for (perm in 1:nrow(five_perms)) {
#  computeBIC(wine_features[perm,], print_steps = FALSE)
#}
```

#### 5.2.3. Computing the BIC for the 4-permutations

#### 5.2.4. Computing the BIC for the 3-permutations

