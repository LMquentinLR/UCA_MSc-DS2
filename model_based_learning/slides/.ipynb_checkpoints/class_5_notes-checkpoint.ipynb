{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2917219",
   "metadata": {},
   "source": [
    "# Model-Based Statistical Learning\n",
    "\n",
    "***class 5***\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 1 - Gaussian Mixture Model (GMM)\n",
    "\n",
    "### Review\n",
    "\n",
    "$$P(x,\\theta) = \\sum_{k=1}^K\\pi_k.\\phi(x; \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "The GMM is probably the most popular mixture model for two main reasons:\n",
    "\n",
    "1. **wrong reason**: it is easy to implement and its computation is the simplest\n",
    "2. **right reason**: even though it is a simple model, it is flexible enough to fit/approximate a large number of cases.\n",
    "\n",
    "The GMM may fit data that does not appear *linear* (e.g. a cloud of point in the shape of a circle)\n",
    "\n",
    "The issue is finding the right $K$:\n",
    "\n",
    "> When varying $K$, we can fit to any data distribution. The limit will be to choose $K$ appropriately. \n",
    "\n",
    "### Model inference\n",
    "\n",
    "The model inference in the GMM case is not easy due to the specific form of the log-likelihood:\n",
    "\n",
    "\\begin{align}\n",
    "log\\mathcal{L}(x;\\theta) &= log(\\prod_{i=1}^N\\sum_{k=1}^K\\pi_k.\\phi(x_i; \\mu_k, \\Sigma_k))\\\\\n",
    "&= \\sum_{i=1}^Nlog(\\sum_{k=1}^K\\pi_k.\\phi(x_i; \\mu_k, \\Sigma_k))\\\\\n",
    "\\end{align}\n",
    "\n",
    "This is not easily solved because of the sum of log of sum operation.\n",
    "\n",
    "### The Expectation-Maximization (EM) algorithm\n",
    "\n",
    "The EM algorithm is still the most efficient optimizer than any other as at today. It is the **classical solution**.\n",
    "\n",
    "<u>First driving idea:</u>\n",
    "\n",
    "We first revisit the model by introducing a **latent variable** $z \\in [0, 1]^K$ to encode the class memberships:\n",
    "\n",
    "$$z_{ik}=1 \\text{ if } x_i \\text{ belongs to the cluster } k \\text{, $0$ otherwise}$$\n",
    "\n",
    "\\begin{align}\n",
    "z|\\pi &\\sim Multinomial(1;\\pi) \\text{, i.e $p(z=k) = \\pi_k$}\\\\\n",
    "x|z=k &\\sim \\mathcal{N}(x;\\mu_k, \\Sigma_k)\n",
    "\\end{align}\n",
    "\n",
    "If we integrate over $z$, we obtain the mixture of gaussian $P(x,\\theta) = \\sum_{k=1}^K\\pi_k.\\phi(x; \\mu_k, \\Sigma_k)$. This allows us to write the likelihood of the couple $(x, z)$ (called the **complete likelihood**) as:\n",
    "\n",
    "\\begin{align}\n",
    "log\\mathcal{L}(x, z; \\theta) &= \\sum^n_{i=1}\\big[ log(p(z_i|x_i;\\theta)) + log(p(x_i;\\theta))\\big]\\\\\n",
    "&= log\\mathcal{L}(x, \\theta) + \\sum^n_{i=1}log(p(z_i|x_i;\\theta))\\\\\n",
    "log\\mathcal{L}(x;\\theta) &= log\\mathcal{L}(x; z; \\theta) - \\sum^n_{i=1}log(p(z_i|x_i;\\theta))\\\\\n",
    "\\end{align}\n",
    "\n",
    "**Note:** $log\\mathcal{L}(x; z; \\theta)$ is a **lower-bound** of $log\\mathcal{L}(x;\\theta)$. However we need to know $z$, which we are looking for. But, if we know $z$, we can maximize the lower bound instead of the log-likelihood.\n",
    "\n",
    "**Note:** The EM Algorithm works for any mixture model, and any model with a latent variable.\n",
    "\n",
    "<u>Second driving idea:</u>\n",
    "\n",
    "The spirit of the EM algorithm is to alternate between two steps:\n",
    "\n",
    "1. **<u>Expectation</u> step:** Knowing a certain value of $\\theta$ called $\\theta^*$, we compute the expectation of the lower bound $log\\mathcal{L}(x; z; \\theta)$: $$\\mathbb{E}[\\mathcal{l}(x; z| \\theta)|\\theta^*]=Q(\\theta|\\theta^*)$$\n",
    "\n",
    "Note: $Q$ is a function of $\\theta$ that depends on a previous value $\\theta^*$.\n",
    "\n",
    "2. **<u>Maximization</u> step:** $Q(\\theta|\\theta^*)$ is optimized over $\\theta$ to obtain a new value/estimate of $\\theta^*$\n",
    "\n",
    "<hr>\n",
    "\n",
    "> <u>Theorem (Dempster, Laird, Rubin (theorem proposition), 1979; Wu (correct proof), 1981):</u> \n",
    ">\n",
    "> **The series of parameters $(\\theta^*)_q$ generated by the EM algorithm converges towards a <u>local</u> maximum of the log-likelihood $log\\mathcal{L}(x;\\theta)$.**\n",
    "\n",
    "<hr>\n",
    "\n",
    "<u>Graphical representation:</u>\n",
    "\n",
    "There is a **dependence to the initialization**. As such, a number of random $\\theta^0$ initializations is used as starting points. The best solution is kept as it leads to the maximum likelihood (local). In practice, we also stop the algorithm when a plateau of the likelihood is detected:\n",
    "\n",
    "![EMconvergence](images/EMcoverge.png)\n",
    "\n",
    "The central quantity to compute in the E-step is:\n",
    "\n",
    "\\begin{align}\n",
    "Q(\\theta|\\theta^*) &= \\mathbb{E}[l(x, z|\\theta^*)]\\\\\n",
    "l(x, z|theta) &= \\sum^n_{i=1} \\sum^K_{k=1} z_{ik} log(\\pi_k\\phi(x; \\mu_k \\Sigma_k))\\\\\n",
    "\\end{align}\n",
    "and therefore $Q(\\theta|\\theta^*) = \\sum_i \\sum_k E(z_ik|\\theta^*) log(\\pi_k\\phi(x_i, \\mu_k,\\Sigma_k))$\n",
    "So the E step for the GMM reduces to the computation of :\n",
    "\n",
    "$$\\gamma_{ik} = \\mathbb{E}[z_ik|\\theta^*] \\overset{Bayes}{\\propto} P(z_{ik} =1|\\theta^*)p(x_i|z_{ik}=1,\\theta^*)\\pi_k^*.\\phi(x_i;\\mu_k^*,\\Sigma_k^*)$$\n",
    "\n",
    "- **E-step**: \n",
    "\n",
    "> $\\gamma_{ik}\\propto \\pi^*_k.\\phi(x_i;\\mu_k^*;\\Sigma_k^*) \\forall i, \\forall k$\n",
    "\n",
    "- **M-step**: \n",
    "\n",
    "> Maximize over $\\pi_k$, $\\mu_k$, $\\Sigma_k$, the function $Q(\\theta|\\theta^*) = \\sum_i\\sum_k\\gamma_{ik}log(\\pi_k.\\phi(x_i;\\mu_k,\\Sigma_k))$ where $\\phi(x_i;\\mu_k,\\Sigma_k) = \\frac{1}{|\\Sigma_k|^{1/2}2\\pi^d}exp(-\\frac{1}{2}(x_i-\\mu_k)^T\\Sigma^{-1}_k(x_i-\\mu_k))$\n",
    "where $d$ is the dimensionality of $x_i\\in \\mathbb{R}^d$.\n",
    "\n",
    "The update equations for $\\pi_k$, $\\mu_k$, and $\\Sigma_k$ canbe attained by simply taking the partial derivatives of $Q(\\theta|\\theta^*$) regarding $\\pi_k$, $\\mu_k$, and $\\Sigma_k$ respectivelly and equalling to 0.\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\mu_k}Q(\\theta|\\theta^*)= 0 \\Leftrightarrow \\mu_k^* = \\frac{1}{n_k}\\sum^n\\gamma_{ik}x_i \\text{  where  } n_k = \\sum^n\\gamma_{ik}$$\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\Sigma_k}Q(\\theta|\\theta^*)= 0 \\Leftrightarrow \\Sigma_k^* = \\frac{1}{n_k}\\sum^n\\gamma_{ik}(x_i-\\mu_k^*)^T(x_i-\\mu_k^*)$$\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\pi_k}Q(\\theta|\\theta^*) \\overset{\\text{under the constraint $\\Sigma_k\\pi_k=1$}}{=} 0 \\Leftrightarrow \\pi_k = \\frac{n_k}{n}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7869813",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11effd9",
   "metadata": {},
   "source": [
    "<u>Computation of the derivative $\\frac{\\delta}{\\delta\\mu_k}Q(\\theta|\\theta^*)$:</u>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\mu_k}Q(\\theta|\\theta^*) &= \\frac{\\delta}{\\delta\\mu_k}\\mathbb{E}[l(x, z|\\theta^*)]\\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\sum_i \\sum_k E(z_ik|\\theta^*) log(\\Pi_k\\phi(x_i, \\mu_k,\\Sigma_k))\\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\sum_i\\sum_k\\gamma_{ik}log(\\Pi_k.\\phi(x_i;\\mu_k,\\Sigma_k))\\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\sum_i\\sum_k\\gamma_{ik}log\\big(\\frac{\\Pi_k}{|\\Sigma_k|^{1/2}(2\\pi)^{d/2}}\\exp(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k))\\big) \\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\sum_i\\sum_k\\gamma_{ik}\\big(log\\big(\\frac{\\Pi_k}{|\\Sigma_k|^{1/2}(2\\pi)^{d/2}}\\big)-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\sum_i\\sum_k\\gamma_{ik}\\big(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\quad\\text{(The first element does not depend on $\\mu_k$)}\\\\\n",
    " &= \\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\mu_k}\\big(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\quad\\text{(All $k$ different from $k^*$ are considered constants)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We set $X=(x-\\mu_k)$ and $a = \\Sigma_k$. As such, we know:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\mu_k}\\big(X^T(aX)\\big) &= \\frac{\\delta}{\\delta\\mu_k}\\big((aX)^TX\\big)\\\\\n",
    " &= \\frac{\\delta}{\\delta\\mu_k}\\big(a^TX^TX\\big)\\\\\n",
    " &= a^T\\frac{\\delta}{\\delta\\mu_k}\\big(X^TX\\big)\\\\\n",
    " &= a^T2X\\\\\n",
    " &= 2(\\Sigma_k^{-1})^T(x-\\mu_k)\\\\\n",
    " &= 2(\\Sigma_k^T)^{-1}(x-\\mu_k)\\\\\n",
    " &= 2\\Sigma_k^{-1}(x-\\mu_k)\\quad\\text{Given $\\Sigma_k$ is a square matrix}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Given this results, we retrieve\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\mu_k}Q(\\theta|\\theta^*) &= \\sum_i\\gamma_{ik}\\big(-\\frac{2}{2}\\Sigma_k^{-1}(x-\\mu_k)\\big)\\\\\n",
    "\\end{align}\n",
    "\n",
    "We are looking for:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\mu_k}Q(\\theta|\\theta^*) &= 0 \\\\\n",
    "\\sum_i\\gamma_{ik}\\big(-\\Sigma_k^{-1}(x-\\mu_k)\\big) &= 0\\\\\n",
    "\\sum_i\\gamma_{ik}(x-\\mu_k) &= 0\\\\\n",
    "\\Leftrightarrow \\mu_k^* &= \\frac{\\sum_i\\gamma_{ik}x_i}{\\sum_i\\gamma_{ik}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "As such we find:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k^* &= \\frac{\\sum_i\\gamma_{ik}x_i}{n_k} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e3723a",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<u>Computation of the derivative $\\frac{\\delta}{\\delta\\Sigma_k}Q(\\theta|\\theta^*)$:</u>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\Sigma_k}Q(\\theta|\\theta^*) &= \\frac{\\delta}{\\delta\\Sigma_k}\\mathbb{E}[l(x, z|\\theta^*)]\\\\\n",
    " &= \\frac{\\delta}{\\delta\\Sigma_k}\\sum_i \\sum_k E(z_ik|\\theta^*) log(\\Pi_k\\phi(x_i, \\mu_k,\\Sigma_k))\\\\\n",
    " &= \\frac{\\delta}{\\delta\\Sigma_k}\\sum_i\\sum_k\\gamma_{ik}log(\\Pi_k.\\phi(x_i;\\mu_k,\\Sigma_k))\\\\\n",
    " &= \\frac{\\delta}{\\delta\\Sigma_k}\\sum_i\\sum_k\\gamma_{ik}log\\big(\\frac{\\Pi_k}{|\\Sigma_k|^{1/2}(2\\pi)^{d/2}}\\exp(-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k))\\big) \\\\\n",
    " &= \\frac{\\delta}{\\delta\\Sigma_k}\\sum_i\\sum_k\\gamma_{ik}\\big(log\\big(\\frac{\\Pi_k}{|\\Sigma_k|^{1/2}(2\\pi)^{d/2}}\\big)-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\\\\n",
    " &= \\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\Sigma_k}\\big(log\\big(\\frac{\\Pi_k}{|\\Sigma_k|^{1/2}(2\\pi)^{d/2}}\\big)-\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\quad\\text{(All $k$ different from $k^*$ are considered constants)}\\\\\n",
    " &= \\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\Sigma_k}\\big(log(\\Pi_k)-log(|\\Sigma_k|^{1/2})-log((2\\pi)^{d/2})\\big) - \\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\Sigma_k}\\big(\\frac{1}{2}(x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\\\\n",
    " &= -\\frac{1}{2}\\big(\\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\Sigma_k}log(|\\Sigma_k|) + \\sum_i\\gamma_{ik}\\frac{\\delta}{\\delta\\Sigma_k}\\big((x-\\mu_k)^T\\Sigma_k^{-1}(x-\\mu_k)\\big)\\big)\\quad\\text{(We remove the elements that don't depend on $\\Sigma_k$)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "We know that:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\Sigma_k}log(|\\Sigma_k|) &= ((\\Sigma_k)^{-1})^T\\\\\n",
    "\\frac{\\delta}{\\delta X}a^TX^{-1}a &= -(X^{-1})^Taa^T(X^{-1})^T\n",
    "\\end{align}\n",
    "\n",
    "As such:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\delta}{\\delta\\Sigma_k}Q(\\theta|\\theta^*) &= -\\frac{1}{2}\\big(\\sum_i\\gamma_{ik} ((\\Sigma_k)^{-1})^T - \\sum_i\\gamma_{ik}((\\Sigma_k)^{-1})^T(x-\\mu_k)(x-\\mu_k)^T((\\Sigma_k)^{-1})^T\\big) \\\\\n",
    " &= -\\frac{1}{2}((\\Sigma_k)^{-1})^T\\big(\\sum_i\\gamma_{ik} - \\sum_i\\gamma_{ik}(x-\\mu_k)(x-\\mu_k)^T((\\Sigma_k)^{-1})^T\\big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "We are looking for:\n",
    "\n",
    "\\begin{align}\n",
    " \\frac{\\delta}{\\delta\\Sigma_k}Q(\\theta|\\theta^*) &= 0 \\\\\n",
    " -\\frac{1}{2}((\\Sigma_k)^{-1})^T\\big(\\sum_i\\gamma_{ik} - \\sum_i\\gamma_{ik}(x-\\mu_k)(x-\\mu_k)^T((\\Sigma_k)^{-1})^T\\big) &= 0 \\\\\n",
    " \\sum_i\\gamma_{ik} &= \\sum_i\\gamma_{ik}(x-\\mu_k)(x-\\mu_k)^T((\\Sigma_k)^{-1})^T\\\\\n",
    " 1 &= \\frac{\\sum_i\\gamma_{ik}(x-\\mu_k)^T(x-\\mu_k)\\Sigma_k^{-1}}{\\sum_i\\gamma_{ik}}\\\\\n",
    " \\Sigma_k &= \\frac{\\sum_i\\gamma_{ik}(x-\\mu_k)^T(x-\\mu_k)}{\\sum_i\\gamma_{ik}}\\quad\\text{(As $\\Sigma_k^T=\\Sigma_k$)}\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "As such, we find:\n",
    "\n",
    "\\begin{align}\n",
    " \\Sigma_k &= \\frac{\\sum_i\\gamma_{ik}(x-\\mu_k)^T(x-\\mu_k)}{n_k} \\\\\n",
    "\\end{align}\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90620cfc",
   "metadata": {},
   "source": [
    "## 2 - Model Selection, how to choose $K$?\n",
    "\n",
    "We need a quantity that mesure the adequacy of the model to the data. This quantity is the **likelihood**. \n",
    "\n",
    "### Model selection theory\n",
    "\n",
    "There is a model selection criteria that penalizes the likelihood with a quantity that favors models with a low number of groups.\n",
    "\n",
    "$$MSCriteria = log\\mathcal{L}(x;\\hat{\\theta}) - pen(K)$$\n",
    "\n",
    "The work of model selection is to find the right $pen(K)$.\n",
    "\n",
    "### Popular criteria\n",
    "\n",
    "- **AIC**: $log(\\mathcal{L}(x;\\hat{\\theta})) - \\eta(M)$\n",
    "\n",
    "- **BIC**: $log(\\mathcal{L}(x;\\hat{\\theta})) - \\frac{1}{2}\\eta(M)log(n)$\n",
    "\n",
    "where $\\eta(M)$ is the number of free scalar parameters in the model $M$.\n",
    "\n",
    "In practice, $\\mu(GMM)$ is easy to compute.  $$\\mu(GMM\\text{ with $K$ groups})=\\text{ nb of }\\pi_k + \\text{ nb of }\\mu_k + \\text{ nb of }\\Sigma_k$$. Knowing the $\\pi_k$ must sum to 1, it implies that there are: $$(K-1) + kd + K\\frac{d(d+1)}{2}$$ free parameters.\n",
    "\n",
    "As such:\n",
    "\n",
    "\\begin{align}\n",
    "BIC(GMM)&=log(\\mathbb{L}(x;\\hat{\\theta}) - \\frac{(K-1)+K*d + K*\\frac{d*(d+1)}{2}}{2} log(n)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8f1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
