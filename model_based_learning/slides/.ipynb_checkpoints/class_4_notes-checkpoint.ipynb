{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d76e656",
   "metadata": {},
   "source": [
    "# Model-Based Statistical Learning\n",
    "\n",
    "***class 4***\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Page 8\n",
    "\n",
    "$$c\\approx Cat(\\pi)$$\n",
    "\n",
    "with $\\pi$ the class proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c78c6",
   "metadata": {},
   "source": [
    "## Page 16\n",
    "\n",
    "### The Expectation-Maximization Algorithm\n",
    "\n",
    "The EM algorithm has the advantage over maximum likelihood (when we don't know the classes) that it is easier to optimize. In ML, we have the sum of a log of a sum and that's very complex:\n",
    "\n",
    "$$l(\\theta) = \\overset{N}{\\underset{n=1}{\\sum}}log(p_\\theta(x_n)) = \\overset{N}{\\underset{n=1}{\\sum}} log (\\overset{K}{\\underset{k=1}{\\sum}}\\pi_k\\mathcal{N}(x_n|\\mu_k,\\Sigma_k))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ab2b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "709a2153",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed7dea4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c2cb40b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1ffda21",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
